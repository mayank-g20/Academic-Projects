{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "This module explains how to apply neural networks to the Kaggle OTTO Group Challenge and get the log loss score on the test data lower than 0.49 which was earlier achieved and described by applying Gradient Boosting. For information on the data set, please follow the link https://github.com/mayank-g20/Academic-Projects/blob/master/Gradient%20Boosting.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayank/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/mayank/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/mayank/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.optimizers import Adam,SGD,Adagrad\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (Kaggle OTTO  Group challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41244, 94)\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(\"/Users/mayank/Documents/College Documents/Q3 Courses/Machine Learning/Course Project/supervized_classification_data/train_sample.csv\")\n",
    "#X.drop('id', axis=1, inplace=True)\n",
    "print(X.shape)\n",
    "np.random.seed(1)\n",
    "# Shuffle data. It is originally oredered by class\n",
    "X = X.reindex(np.random.permutation(X.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17009</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20763</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17705</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "17009       0       0       0       1       0       0       0       0       0   \n",
       "543         1       0       0       1       0       0       0       1       2   \n",
       "20763       2       0       1       0       0       0       0       0       0   \n",
       "17705       1       0       0       2       0       0       2       0       0   \n",
       "24340       0       0       0       0       0       0       0       1       0   \n",
       "\n",
       "       feat_10   ...    feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
       "17009        0   ...          0        0        0        1        0        0   \n",
       "543          0   ...          0        1        0        1        0        0   \n",
       "20763        0   ...          0        0        0        0        0        1   \n",
       "17705        0   ...          1        1        0        1        4        1   \n",
       "24340        0   ...          0        1        0        0        1        0   \n",
       "\n",
       "       feat_91  feat_92  feat_93  target  \n",
       "17009        0        0        0       1  \n",
       "543          0        2        0       0  \n",
       "20763        0        0        0       1  \n",
       "17705        1        3        1       3  \n",
       "24340        0        1        1       7  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a multiclass classification problem. \n",
    "\n",
    "Keras loss function for this problem is *'categorical_crossentropy'* (multiclass logloss). \n",
    "\n",
    "##### Also note that for multi-class identification problems, it is advisable to use log loss as an evaluation parameter as compared to the accuracy values.\n",
    "\n",
    "There is important note in the manual:  <br>\n",
    "when using the *'categorical_crossentropy'* loss, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except for 1 at the index corresponding to the class of the sample). \n",
    "\n",
    "In order to convert integer targets into categorical targets, you can use Keras utility *'to_categorical'*:\n",
    "\n",
    "`*from keras.utils import to_categorical*`\n",
    "`*categorical_labels = to_categorical(int_labels, num_classes=None)*`\n",
    "\n",
    "The target variable initially is in string format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17009    1\n",
       "543      0\n",
       "20763    1\n",
       "17705    3\n",
       "24340    7\n",
       "24643    6\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.target[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target data is in integer format, we will convert them to ctegorical targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract target\n",
    "# Encode it to make it manageable by ML algo, convert to np.int32\n",
    "y = X.target.values\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y).astype(np.int32)\n",
    "num_classes = len(encoder.classes_)\n",
    "# convert class vectors to binary class matrices\n",
    "y = to_categorical(y, num_classes)\n",
    "\n",
    "# Remove target from train\n",
    "X.drop('target', axis=1, inplace=True)\n",
    "\n",
    "y[:6,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was noticed during competition that logarithmic transfrom improves performance of most of ML algrithms. Transform the data and convert them to numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = X.shape[1]\n",
    "# log transform to decrease large values affect\n",
    "X = np.log(1+X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The baseline model\n",
    "\n",
    "Start tunning sequential network with two hidden layers and dropout layers after each of them. \n",
    "\n",
    "The following function creates such network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout explanation for the below use case:\n",
    "+ Dropout is a regularization technique for neural network models.\n",
    "\n",
    "+ Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "+ As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features providing some specialization. Neighboring neurons become to rely on this specialization, which if taken too far can result in a fragile model too specialized to the training data. This reliant on context for a neuron during training is referred to complex co-adaptations.\n",
    "\n",
    "+ You can imagine that if neurons are randomly dropped out of the network during training, that other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n",
    "\n",
    "+ The effect is that the network becomes less sensitive to the specific weights of neurons. This in turn results in a network that is capable of better generalization and is less likely to overfit the training data.\n",
    "\n",
    "Ref: https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 178.35 410.00\" width=\"178pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 174.349,-406 174.349,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4449345208 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4449345208</title>\n",
       "<polygon fill=\"none\" points=\"0,-365.5 0,-401.5 170.349,-401.5 170.349,-365.5 0,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-379.3\">Dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4449344872 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4449344872</title>\n",
       "<polygon fill=\"none\" points=\"31.4932,-292.5 31.4932,-328.5 138.855,-328.5 138.855,-292.5 31.4932,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-306.3\">Dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 4449345208&#45;&gt;4449344872 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4449345208-&gt;4449344872</title>\n",
       "<path d=\"M85.1743,-365.313C85.1743,-357.289 85.1743,-347.547 85.1743,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-338.529 85.1743,-328.529 81.6744,-338.529 88.6744,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4888413576 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4888413576</title>\n",
       "<polygon fill=\"none\" points=\"19.8174,-219.5 19.8174,-255.5 150.531,-255.5 150.531,-219.5 19.8174,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-233.3\">Dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 4449344872&#45;&gt;4888413576 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4449344872-&gt;4888413576</title>\n",
       "<path d=\"M85.1743,-292.313C85.1743,-284.289 85.1743,-274.547 85.1743,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-265.529 85.1743,-255.529 81.6744,-265.529 88.6744,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4888979496 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4888979496</title>\n",
       "<polygon fill=\"none\" points=\"31.4932,-146.5 31.4932,-182.5 138.855,-182.5 138.855,-146.5 31.4932,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-160.3\">Dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 4888413576&#45;&gt;4888979496 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4888413576-&gt;4888979496</title>\n",
       "<path d=\"M85.1743,-219.313C85.1743,-211.289 85.1743,-201.547 85.1743,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-192.529 85.1743,-182.529 81.6744,-192.529 88.6744,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4888777448 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4888777448</title>\n",
       "<polygon fill=\"none\" points=\"19.8174,-73.5 19.8174,-109.5 150.531,-109.5 150.531,-73.5 19.8174,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-87.3\">Dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 4888979496&#45;&gt;4888777448 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>4888979496-&gt;4888777448</title>\n",
       "<path d=\"M85.1743,-146.313C85.1743,-138.289 85.1743,-128.547 85.1743,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-119.529 85.1743,-109.529 81.6744,-119.529 88.6744,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4957056640 -->\n",
       "<g class=\"node\" id=\"node6\"><title>4957056640</title>\n",
       "<polygon fill=\"none\" points=\"36.5415,-0.5 36.5415,-36.5 133.807,-36.5 133.807,-0.5 36.5415,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-14.3\">Output: Dense</text>\n",
       "</g>\n",
       "<!-- 4888777448&#45;&gt;4957056640 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>4888777448-&gt;4957056640</title>\n",
       "<path d=\"M85.1743,-73.3129C85.1743,-65.2895 85.1743,-55.5475 85.1743,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-46.5288 85.1743,-36.5288 81.6744,-46.5289 88.6744,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getModel(dropout=0.1, neurons1=500, neurons2=250,\n",
    "             learningRate=0.04):\n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu', input_dim=num_features,\n",
    "                    name='Dense_1'))\n",
    "    model.add(Dropout(dropout,name='Dropout_1'))\n",
    "    model.add(Dense(neurons2, activation='relu',name='Dense_2'))\n",
    "    model.add(Dropout(dropout,name='Dropout_2'))\n",
    "    model.add(Dense(num_classes, activation='softmax',name='Output'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adagrad(lr=learningRate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model1 = getModel()\n",
    "\n",
    "SVG(model_to_dot(model1).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a better way to visualize this and it is mentioned in the chunk below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "ann_viz(model1, title=\"Network Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network was created by *'getModel()'* with default parameters. Method *Fit* has parameter  \n",
    "\n",
    "*validation_split*: float (0. < x < 1): this is fraction of the data to use as hold-out validation data.  \n",
    "\n",
    "Set *validation_split = 0.2* i.e. 20% of data will be used for validation. <br>\n",
    "Fit the model and draw validation loss plot. <br>\n",
    "Since we want to show validation loss behaviour after it reaches minimum, we do not use *EarlyStopping* callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 1.3020 - acc: 0.6718 - val_loss: 0.6110 - val_acc: 0.7688\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.6052 - acc: 0.7681 - val_loss: 0.5838 - val_acc: 0.7739\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.5539 - acc: 0.7851 - val_loss: 0.5658 - val_acc: 0.7814\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.5216 - acc: 0.7963 - val_loss: 0.5863 - val_acc: 0.7674\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.4915 - acc: 0.8075 - val_loss: 0.5460 - val_acc: 0.7858\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.4658 - acc: 0.8181 - val_loss: 0.5509 - val_acc: 0.7864\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.4385 - acc: 0.8273 - val_loss: 0.5554 - val_acc: 0.7894\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.4139 - acc: 0.8372 - val_loss: 0.5721 - val_acc: 0.7783\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.3942 - acc: 0.8457 - val_loss: 0.5685 - val_acc: 0.7812\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.3733 - acc: 0.8513 - val_loss: 0.5628 - val_acc: 0.7868\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.3478 - acc: 0.8617 - val_loss: 0.5739 - val_acc: 0.7857\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.3315 - acc: 0.8705 - val_loss: 0.5867 - val_acc: 0.7851\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.3136 - acc: 0.8760 - val_loss: 0.5933 - val_acc: 0.7891\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.2939 - acc: 0.8868 - val_loss: 0.6077 - val_acc: 0.7896\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.2778 - acc: 0.8935 - val_loss: 0.6322 - val_acc: 0.7785\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.2686 - acc: 0.8950 - val_loss: 0.6256 - val_acc: 0.7897\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.2491 - acc: 0.9057 - val_loss: 0.6658 - val_acc: 0.7728\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.2345 - acc: 0.9113 - val_loss: 0.6692 - val_acc: 0.7765\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.2248 - acc: 0.9144 - val_loss: 0.6808 - val_acc: 0.7818\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.2128 - acc: 0.9190 - val_loss: 0.6988 - val_acc: 0.7738\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.2011 - acc: 0.9242 - val_loss: 0.7138 - val_acc: 0.7820\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.1946 - acc: 0.9275 - val_loss: 0.7251 - val_acc: 0.7773\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.1848 - acc: 0.9314 - val_loss: 0.7416 - val_acc: 0.7738\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.1732 - acc: 0.9364 - val_loss: 0.7569 - val_acc: 0.7780\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.1651 - acc: 0.9395 - val_loss: 0.7663 - val_acc: 0.7797\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.1570 - acc: 0.9427 - val_loss: 0.7780 - val_acc: 0.7824\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.1489 - acc: 0.9453 - val_loss: 0.7994 - val_acc: 0.7809\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.1470 - acc: 0.9465 - val_loss: 0.8093 - val_acc: 0.7795\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.1394 - acc: 0.9497 - val_loss: 0.8352 - val_acc: 0.7774\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.1320 - acc: 0.9530 - val_loss: 0.8489 - val_acc: 0.7704\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.1256 - acc: 0.9560 - val_loss: 0.8497 - val_acc: 0.7740\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.1236 - acc: 0.9574 - val_loss: 0.8880 - val_acc: 0.7698\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.1146 - acc: 0.9611 - val_loss: 0.8825 - val_acc: 0.7761\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.1137 - acc: 0.9605 - val_loss: 0.8864 - val_acc: 0.7782\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.1106 - acc: 0.9620 - val_loss: 0.9079 - val_acc: 0.7719\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.1043 - acc: 0.9639 - val_loss: 0.9216 - val_acc: 0.7761\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.1027 - acc: 0.9641 - val_loss: 0.9307 - val_acc: 0.7739\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.0959 - acc: 0.9682 - val_loss: 0.9450 - val_acc: 0.7767\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.0952 - acc: 0.9674 - val_loss: 0.9588 - val_acc: 0.7750\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.0917 - acc: 0.9688 - val_loss: 0.9619 - val_acc: 0.7757\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.0907 - acc: 0.9695 - val_loss: 0.9699 - val_acc: 0.7704\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.0857 - acc: 0.9717 - val_loss: 0.9797 - val_acc: 0.7740\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.0856 - acc: 0.9717 - val_loss: 1.0104 - val_acc: 0.7771\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.0846 - acc: 0.9722 - val_loss: 1.0045 - val_acc: 0.7748\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.0804 - acc: 0.9720 - val_loss: 1.0060 - val_acc: 0.7751\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.0784 - acc: 0.9732 - val_loss: 1.0300 - val_acc: 0.7728\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.0777 - acc: 0.9738 - val_loss: 1.0247 - val_acc: 0.7767\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.0758 - acc: 0.9748 - val_loss: 1.0389 - val_acc: 0.7751\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.0718 - acc: 0.9766 - val_loss: 1.0390 - val_acc: 0.7716\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.0718 - acc: 0.9765 - val_loss: 1.0455 - val_acc: 0.7750\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.0714 - acc: 0.9764 - val_loss: 1.0526 - val_acc: 0.7777\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.0670 - acc: 0.9781 - val_loss: 1.0715 - val_acc: 0.7740\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.0670 - acc: 0.9785 - val_loss: 1.0700 - val_acc: 0.7710\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.0666 - acc: 0.9781 - val_loss: 1.0780 - val_acc: 0.7771\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.0667 - acc: 0.9778 - val_loss: 1.0849 - val_acc: 0.7697\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.0605 - acc: 0.9802 - val_loss: 1.0871 - val_acc: 0.7693\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.0610 - acc: 0.9799 - val_loss: 1.0934 - val_acc: 0.7757\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.0599 - acc: 0.9809 - val_loss: 1.1067 - val_acc: 0.7749\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.0612 - acc: 0.9806 - val_loss: 1.1111 - val_acc: 0.7735\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.0575 - acc: 0.9822 - val_loss: 1.1131 - val_acc: 0.7731\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.0553 - acc: 0.9825 - val_loss: 1.1148 - val_acc: 0.7712\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.0564 - acc: 0.9820 - val_loss: 1.1303 - val_acc: 0.7709\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0546 - acc: 0.9824 - val_loss: 1.1243 - val_acc: 0.7674\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.0536 - acc: 0.9834 - val_loss: 1.1343 - val_acc: 0.7728\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.0537 - acc: 0.9829 - val_loss: 1.1582 - val_acc: 0.7731\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.0538 - acc: 0.9826 - val_loss: 1.1479 - val_acc: 0.7714\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.0534 - acc: 0.9832 - val_loss: 1.1561 - val_acc: 0.7659\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.0516 - acc: 0.9836 - val_loss: 1.1559 - val_acc: 0.7689\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.0488 - acc: 0.9852 - val_loss: 1.1606 - val_acc: 0.7717\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.0503 - acc: 0.9843 - val_loss: 1.1722 - val_acc: 0.7715\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.0498 - acc: 0.9839 - val_loss: 1.1708 - val_acc: 0.7731\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.0473 - acc: 0.9856 - val_loss: 1.1700 - val_acc: 0.7716\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.0499 - acc: 0.9841 - val_loss: 1.1778 - val_acc: 0.7725\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.0436 - acc: 0.9870 - val_loss: 1.1920 - val_acc: 0.7704\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.0491 - acc: 0.9840 - val_loss: 1.1799 - val_acc: 0.7726\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.0492 - acc: 0.9853 - val_loss: 1.1729 - val_acc: 0.7723\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.0440 - acc: 0.9865 - val_loss: 1.1834 - val_acc: 0.7757\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.0459 - acc: 0.9860 - val_loss: 1.1842 - val_acc: 0.7766\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.0432 - acc: 0.9871 - val_loss: 1.1942 - val_acc: 0.7743\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.0420 - acc: 0.9870 - val_loss: 1.2008 - val_acc: 0.7721\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.0415 - acc: 0.9869 - val_loss: 1.2095 - val_acc: 0.7733\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.0407 - acc: 0.9876 - val_loss: 1.2159 - val_acc: 0.7755\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.0425 - acc: 0.9868 - val_loss: 1.2072 - val_acc: 0.7735\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.0413 - acc: 0.9870 - val_loss: 1.2183 - val_acc: 0.7739\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.0411 - acc: 0.9874 - val_loss: 1.2292 - val_acc: 0.7743\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.0423 - acc: 0.9871 - val_loss: 1.2251 - val_acc: 0.7761\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.0386 - acc: 0.9880 - val_loss: 1.2248 - val_acc: 0.7749\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0384 - acc: 0.9877 - val_loss: 1.2270 - val_acc: 0.7777\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.0393 - acc: 0.9885 - val_loss: 1.2373 - val_acc: 0.7755\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.0358 - acc: 0.9891 - val_loss: 1.2437 - val_acc: 0.7708\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.0385 - acc: 0.9877 - val_loss: 1.2488 - val_acc: 0.7717\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.0390 - acc: 0.9878 - val_loss: 1.2430 - val_acc: 0.7737\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.0378 - acc: 0.9885 - val_loss: 1.2514 - val_acc: 0.7712\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.0359 - acc: 0.9886 - val_loss: 1.2677 - val_acc: 0.7723\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0367 - acc: 0.9881 - val_loss: 1.2468 - val_acc: 0.7751\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0368 - acc: 0.9890 - val_loss: 1.2565 - val_acc: 0.7737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      " - 1s - loss: 0.0368 - acc: 0.9882 - val_loss: 1.2668 - val_acc: 0.7728\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0362 - acc: 0.9888 - val_loss: 1.2723 - val_acc: 0.7717\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0368 - acc: 0.9889 - val_loss: 1.2734 - val_acc: 0.7759\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0361 - acc: 0.9895 - val_loss: 1.2722 - val_acc: 0.7716\n"
     ]
    }
   ],
   "source": [
    "net1 = model1.fit(X, y, epochs=100, batch_size=512, verbose=2,\n",
    "                  validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we will check for the best possible values of val_loss (validation data set loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEMCAYAAABtKgnyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_loss1 = net1.history[\"val_loss\"]\n",
    "plt.plot(valid_loss1, linewidth=3, label=\"The baseline\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"validation loss\")\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(0.48, 0.8)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the increase in number of epochs the validation error keeps on increasing even though the training loss is decreasing with the number of epochs, this is primarily hinting towards over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase dropout\n",
    "\n",
    "Add some drop out to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 1.3921 - acc: 0.6553 - val_loss: 0.6453 - val_acc: 0.7584\n",
      "Epoch 2/100\n",
      " - 1s - loss: 0.6777 - acc: 0.7437 - val_loss: 0.5949 - val_acc: 0.7720\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.6311 - acc: 0.7623 - val_loss: 0.5794 - val_acc: 0.7742\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.6004 - acc: 0.7691 - val_loss: 0.5770 - val_acc: 0.7795\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.5780 - acc: 0.7783 - val_loss: 0.5640 - val_acc: 0.7828\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.5582 - acc: 0.7851 - val_loss: 0.5604 - val_acc: 0.7829\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.5442 - acc: 0.7904 - val_loss: 0.5517 - val_acc: 0.7879\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.5324 - acc: 0.7921 - val_loss: 0.5526 - val_acc: 0.7852\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.5134 - acc: 0.8001 - val_loss: 0.5493 - val_acc: 0.7865\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.5023 - acc: 0.8062 - val_loss: 0.5407 - val_acc: 0.7885\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.4947 - acc: 0.8067 - val_loss: 0.5445 - val_acc: 0.7876\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.4822 - acc: 0.8123 - val_loss: 0.5442 - val_acc: 0.7889\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.4711 - acc: 0.8146 - val_loss: 0.5332 - val_acc: 0.7936\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.4633 - acc: 0.8173 - val_loss: 0.5330 - val_acc: 0.7982\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.4518 - acc: 0.8209 - val_loss: 0.5368 - val_acc: 0.7945\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.4460 - acc: 0.8242 - val_loss: 0.5339 - val_acc: 0.7971\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.4359 - acc: 0.8254 - val_loss: 0.5395 - val_acc: 0.7951\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.4282 - acc: 0.8301 - val_loss: 0.5414 - val_acc: 0.7940\n",
      "Epoch 19/100\n",
      " - 1s - loss: 0.4207 - acc: 0.8335 - val_loss: 0.5466 - val_acc: 0.7950\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.4147 - acc: 0.8346 - val_loss: 0.5490 - val_acc: 0.7934\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.4054 - acc: 0.8397 - val_loss: 0.5463 - val_acc: 0.7994\n",
      "Epoch 22/100\n",
      " - 1s - loss: 0.4002 - acc: 0.8392 - val_loss: 0.5443 - val_acc: 0.7984\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.3945 - acc: 0.8429 - val_loss: 0.5460 - val_acc: 0.7948\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.3876 - acc: 0.8477 - val_loss: 0.5457 - val_acc: 0.7979\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.3816 - acc: 0.8467 - val_loss: 0.5596 - val_acc: 0.7915\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.3785 - acc: 0.8496 - val_loss: 0.5539 - val_acc: 0.7989\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.3678 - acc: 0.8533 - val_loss: 0.5565 - val_acc: 0.7951\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.3613 - acc: 0.8539 - val_loss: 0.5614 - val_acc: 0.7969\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.3583 - acc: 0.8578 - val_loss: 0.5670 - val_acc: 0.7976\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.3519 - acc: 0.8606 - val_loss: 0.5644 - val_acc: 0.7971\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.3498 - acc: 0.8615 - val_loss: 0.5658 - val_acc: 0.7955\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.3441 - acc: 0.8651 - val_loss: 0.5722 - val_acc: 0.8001\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.3386 - acc: 0.8656 - val_loss: 0.5741 - val_acc: 0.7922\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.3343 - acc: 0.8665 - val_loss: 0.5739 - val_acc: 0.7951\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.3299 - acc: 0.8676 - val_loss: 0.5781 - val_acc: 0.7952\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.3243 - acc: 0.8710 - val_loss: 0.5815 - val_acc: 0.7966\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.3236 - acc: 0.8710 - val_loss: 0.5867 - val_acc: 0.7972\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.3167 - acc: 0.8729 - val_loss: 0.5894 - val_acc: 0.7969\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.3165 - acc: 0.8756 - val_loss: 0.5902 - val_acc: 0.7954\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.3106 - acc: 0.8768 - val_loss: 0.5957 - val_acc: 0.7977\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.3068 - acc: 0.8780 - val_loss: 0.5993 - val_acc: 0.7920\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.3053 - acc: 0.8770 - val_loss: 0.6062 - val_acc: 0.7980\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.3016 - acc: 0.8789 - val_loss: 0.6031 - val_acc: 0.7965\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.3014 - acc: 0.8780 - val_loss: 0.6080 - val_acc: 0.7988\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.2926 - acc: 0.8840 - val_loss: 0.6074 - val_acc: 0.7979\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.2941 - acc: 0.8840 - val_loss: 0.6167 - val_acc: 0.7967\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.2895 - acc: 0.8850 - val_loss: 0.6123 - val_acc: 0.7980\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.2889 - acc: 0.8848 - val_loss: 0.6127 - val_acc: 0.7939\n",
      "Epoch 49/100\n",
      " - 1s - loss: 0.2839 - acc: 0.8867 - val_loss: 0.6196 - val_acc: 0.7946\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.2817 - acc: 0.8877 - val_loss: 0.6218 - val_acc: 0.7961\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.2786 - acc: 0.8895 - val_loss: 0.6275 - val_acc: 0.7936\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.2751 - acc: 0.8908 - val_loss: 0.6290 - val_acc: 0.7967\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.2750 - acc: 0.8890 - val_loss: 0.6341 - val_acc: 0.7948\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.2737 - acc: 0.8898 - val_loss: 0.6403 - val_acc: 0.7949\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.2679 - acc: 0.8933 - val_loss: 0.6437 - val_acc: 0.7946\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.2670 - acc: 0.8927 - val_loss: 0.6467 - val_acc: 0.7927\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.2669 - acc: 0.8923 - val_loss: 0.6485 - val_acc: 0.7942\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.2645 - acc: 0.8949 - val_loss: 0.6469 - val_acc: 0.7962\n",
      "Epoch 59/100\n",
      " - 1s - loss: 0.2613 - acc: 0.8951 - val_loss: 0.6501 - val_acc: 0.7951\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.2585 - acc: 0.8987 - val_loss: 0.6563 - val_acc: 0.7931\n",
      "Epoch 61/100\n",
      " - 1s - loss: 0.2534 - acc: 0.9010 - val_loss: 0.6569 - val_acc: 0.7921\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.2522 - acc: 0.8982 - val_loss: 0.6653 - val_acc: 0.7916\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.2519 - acc: 0.9008 - val_loss: 0.6614 - val_acc: 0.7916\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.2516 - acc: 0.9015 - val_loss: 0.6666 - val_acc: 0.7917\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.2477 - acc: 0.9011 - val_loss: 0.6679 - val_acc: 0.7914\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.2445 - acc: 0.9029 - val_loss: 0.6706 - val_acc: 0.7902\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.2434 - acc: 0.9019 - val_loss: 0.6723 - val_acc: 0.7929\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.2463 - acc: 0.9020 - val_loss: 0.6794 - val_acc: 0.7932\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.2438 - acc: 0.9042 - val_loss: 0.6713 - val_acc: 0.7891\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.2416 - acc: 0.9068 - val_loss: 0.6746 - val_acc: 0.7904\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.2379 - acc: 0.9057 - val_loss: 0.6785 - val_acc: 0.7921\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.2360 - acc: 0.9063 - val_loss: 0.6838 - val_acc: 0.7915\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.2377 - acc: 0.9066 - val_loss: 0.6880 - val_acc: 0.7927\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.2362 - acc: 0.9067 - val_loss: 0.6855 - val_acc: 0.7893\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.2336 - acc: 0.9072 - val_loss: 0.6901 - val_acc: 0.7897\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.2275 - acc: 0.9097 - val_loss: 0.6982 - val_acc: 0.7925\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.2274 - acc: 0.9102 - val_loss: 0.6966 - val_acc: 0.7923\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.2241 - acc: 0.9124 - val_loss: 0.7058 - val_acc: 0.7906\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.2253 - acc: 0.9102 - val_loss: 0.7071 - val_acc: 0.7914\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.2238 - acc: 0.9122 - val_loss: 0.7073 - val_acc: 0.7882\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.2242 - acc: 0.9114 - val_loss: 0.7022 - val_acc: 0.7915\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.2235 - acc: 0.9128 - val_loss: 0.7089 - val_acc: 0.7903\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.2211 - acc: 0.9136 - val_loss: 0.7146 - val_acc: 0.7908\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.2198 - acc: 0.9129 - val_loss: 0.7165 - val_acc: 0.7931\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.2171 - acc: 0.9139 - val_loss: 0.7208 - val_acc: 0.7915\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.2122 - acc: 0.9164 - val_loss: 0.7279 - val_acc: 0.7919\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.2092 - acc: 0.9181 - val_loss: 0.7344 - val_acc: 0.7908\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.2180 - acc: 0.9134 - val_loss: 0.7264 - val_acc: 0.7949\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.2098 - acc: 0.9175 - val_loss: 0.7324 - val_acc: 0.7899\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.2114 - acc: 0.9169 - val_loss: 0.7317 - val_acc: 0.7888\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.2103 - acc: 0.9184 - val_loss: 0.7308 - val_acc: 0.7919\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.2119 - acc: 0.9160 - val_loss: 0.7349 - val_acc: 0.7905\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.2107 - acc: 0.9173 - val_loss: 0.7408 - val_acc: 0.7916\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.2088 - acc: 0.9168 - val_loss: 0.7370 - val_acc: 0.7892\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.2083 - acc: 0.9193 - val_loss: 0.7453 - val_acc: 0.7922\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.2063 - acc: 0.9199 - val_loss: 0.7392 - val_acc: 0.7922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      " - 1s - loss: 0.2044 - acc: 0.9208 - val_loss: 0.7446 - val_acc: 0.7896\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.2020 - acc: 0.9218 - val_loss: 0.7477 - val_acc: 0.7905\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.1984 - acc: 0.9221 - val_loss: 0.7538 - val_acc: 0.7940\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.2017 - acc: 0.9217 - val_loss: 0.7454 - val_acc: 0.7923\n"
     ]
    }
   ],
   "source": [
    "increasedDropout = 0.4\n",
    "model2 = getModel(increasedDropout)\n",
    "net2 = model2.fit(X, y, epochs=100, batch_size=512, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the results have improved fairly with using the drop out value of 0.4, further the best validation loss score is obtained at around 13th epoch with the val_loss around 0.5318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare validation loss for *net1* and *net2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEMCAYAAABkwamIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss 0.5329629761664878\n"
     ]
    }
   ],
   "source": [
    "valid_loss2 = net2.history[\"val_loss\"]\n",
    "plt.plot(valid_loss1, linewidth=3, label=\"The baseline\")\n",
    "plt.plot(valid_loss2, linewidth=3, label=\"Increased dropout\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"validation loss\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0.47, 0.8)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "print('best validation loss', min(valid_loss2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is better. <br>\n",
    "No more overfitting, and it feels like it is possible to increase number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase number of neurons\n",
    "\n",
    "Increase number of neurons in the first hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 2.1256 - acc: 0.6322 - val_loss: 0.6377 - val_acc: 0.7628\n",
      "Epoch 2/100\n",
      " - 2s - loss: 0.6482 - acc: 0.7537 - val_loss: 0.5852 - val_acc: 0.7782\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.5970 - acc: 0.7718 - val_loss: 0.5678 - val_acc: 0.7808\n",
      "Epoch 4/100\n",
      " - 2s - loss: 0.5684 - acc: 0.7820 - val_loss: 0.5690 - val_acc: 0.7788\n",
      "Epoch 5/100\n",
      " - 2s - loss: 0.5401 - acc: 0.7905 - val_loss: 0.5453 - val_acc: 0.7885\n",
      "Epoch 6/100\n",
      " - 2s - loss: 0.5225 - acc: 0.7957 - val_loss: 0.5441 - val_acc: 0.7910\n",
      "Epoch 7/100\n",
      " - 2s - loss: 0.5022 - acc: 0.8022 - val_loss: 0.5416 - val_acc: 0.7931\n",
      "Epoch 8/100\n",
      " - 2s - loss: 0.4871 - acc: 0.8082 - val_loss: 0.5458 - val_acc: 0.7897\n",
      "Epoch 9/100\n",
      " - 2s - loss: 0.4713 - acc: 0.8143 - val_loss: 0.5420 - val_acc: 0.7932\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.4548 - acc: 0.8182 - val_loss: 0.5307 - val_acc: 0.7963\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.4420 - acc: 0.8259 - val_loss: 0.5364 - val_acc: 0.7936\n",
      "Epoch 12/100\n",
      " - 1s - loss: 0.4295 - acc: 0.8282 - val_loss: 0.5363 - val_acc: 0.7942\n",
      "Epoch 13/100\n",
      " - 1s - loss: 0.4145 - acc: 0.8338 - val_loss: 0.5332 - val_acc: 0.7965\n",
      "Epoch 14/100\n",
      " - 1s - loss: 0.4048 - acc: 0.8382 - val_loss: 0.5359 - val_acc: 0.7994\n",
      "Epoch 15/100\n",
      " - 1s - loss: 0.3927 - acc: 0.8417 - val_loss: 0.5377 - val_acc: 0.7985\n",
      "Epoch 16/100\n",
      " - 1s - loss: 0.3824 - acc: 0.8441 - val_loss: 0.5386 - val_acc: 0.7984\n",
      "Epoch 17/100\n",
      " - 1s - loss: 0.3710 - acc: 0.8496 - val_loss: 0.5476 - val_acc: 0.7973\n",
      "Epoch 18/100\n",
      " - 1s - loss: 0.3585 - acc: 0.8565 - val_loss: 0.5532 - val_acc: 0.7974\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.3499 - acc: 0.8590 - val_loss: 0.5567 - val_acc: 0.7963\n",
      "Epoch 20/100\n",
      " - 1s - loss: 0.3418 - acc: 0.8615 - val_loss: 0.5595 - val_acc: 0.7983\n",
      "Epoch 21/100\n",
      " - 1s - loss: 0.3308 - acc: 0.8659 - val_loss: 0.5646 - val_acc: 0.7980\n",
      "Epoch 22/100\n",
      " - 2s - loss: 0.3223 - acc: 0.8717 - val_loss: 0.5601 - val_acc: 0.8009\n",
      "Epoch 23/100\n",
      " - 1s - loss: 0.3113 - acc: 0.8736 - val_loss: 0.5678 - val_acc: 0.7997\n",
      "Epoch 24/100\n",
      " - 1s - loss: 0.3031 - acc: 0.8780 - val_loss: 0.5785 - val_acc: 0.8006\n",
      "Epoch 25/100\n",
      " - 1s - loss: 0.2951 - acc: 0.8809 - val_loss: 0.5801 - val_acc: 0.8020\n",
      "Epoch 26/100\n",
      " - 1s - loss: 0.2875 - acc: 0.8840 - val_loss: 0.5869 - val_acc: 0.8017\n",
      "Epoch 27/100\n",
      " - 1s - loss: 0.2809 - acc: 0.8877 - val_loss: 0.5949 - val_acc: 0.7982\n",
      "Epoch 28/100\n",
      " - 1s - loss: 0.2706 - acc: 0.8909 - val_loss: 0.6029 - val_acc: 0.7996\n",
      "Epoch 29/100\n",
      " - 1s - loss: 0.2685 - acc: 0.8914 - val_loss: 0.6080 - val_acc: 0.8012\n",
      "Epoch 30/100\n",
      " - 1s - loss: 0.2640 - acc: 0.8921 - val_loss: 0.6078 - val_acc: 0.7967\n",
      "Epoch 31/100\n",
      " - 1s - loss: 0.2518 - acc: 0.8981 - val_loss: 0.6093 - val_acc: 0.7980\n",
      "Epoch 32/100\n",
      " - 1s - loss: 0.2470 - acc: 0.9004 - val_loss: 0.6167 - val_acc: 0.7989\n",
      "Epoch 33/100\n",
      " - 1s - loss: 0.2494 - acc: 0.8999 - val_loss: 0.6246 - val_acc: 0.7971\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.2382 - acc: 0.9046 - val_loss: 0.6278 - val_acc: 0.7974\n",
      "Epoch 35/100\n",
      " - 1s - loss: 0.2361 - acc: 0.9048 - val_loss: 0.6348 - val_acc: 0.7977\n",
      "Epoch 36/100\n",
      " - 1s - loss: 0.2266 - acc: 0.9087 - val_loss: 0.6514 - val_acc: 0.7990\n",
      "Epoch 37/100\n",
      " - 1s - loss: 0.2274 - acc: 0.9085 - val_loss: 0.6440 - val_acc: 0.8002\n",
      "Epoch 38/100\n",
      " - 1s - loss: 0.2238 - acc: 0.9099 - val_loss: 0.6516 - val_acc: 0.8011\n",
      "Epoch 39/100\n",
      " - 1s - loss: 0.2171 - acc: 0.9122 - val_loss: 0.6572 - val_acc: 0.7911\n",
      "Epoch 40/100\n",
      " - 1s - loss: 0.2110 - acc: 0.9167 - val_loss: 0.6690 - val_acc: 0.8005\n",
      "Epoch 41/100\n",
      " - 1s - loss: 0.2101 - acc: 0.9170 - val_loss: 0.6757 - val_acc: 0.7905\n",
      "Epoch 42/100\n",
      " - 1s - loss: 0.2048 - acc: 0.9193 - val_loss: 0.6802 - val_acc: 0.7982\n",
      "Epoch 43/100\n",
      " - 1s - loss: 0.2035 - acc: 0.9197 - val_loss: 0.6820 - val_acc: 0.7983\n",
      "Epoch 44/100\n",
      " - 1s - loss: 0.1999 - acc: 0.9208 - val_loss: 0.6796 - val_acc: 0.7977\n",
      "Epoch 45/100\n",
      " - 1s - loss: 0.1940 - acc: 0.9236 - val_loss: 0.6868 - val_acc: 0.7982\n",
      "Epoch 46/100\n",
      " - 1s - loss: 0.1876 - acc: 0.9270 - val_loss: 0.7071 - val_acc: 0.7983\n",
      "Epoch 47/100\n",
      " - 1s - loss: 0.1874 - acc: 0.9256 - val_loss: 0.6995 - val_acc: 0.8005\n",
      "Epoch 48/100\n",
      " - 1s - loss: 0.1865 - acc: 0.9254 - val_loss: 0.6962 - val_acc: 0.7978\n",
      "Epoch 49/100\n",
      " - 2s - loss: 0.1808 - acc: 0.9290 - val_loss: 0.7183 - val_acc: 0.7950\n",
      "Epoch 50/100\n",
      " - 1s - loss: 0.1763 - acc: 0.9306 - val_loss: 0.7150 - val_acc: 0.7974\n",
      "Epoch 51/100\n",
      " - 1s - loss: 0.1796 - acc: 0.9286 - val_loss: 0.7171 - val_acc: 0.7985\n",
      "Epoch 52/100\n",
      " - 1s - loss: 0.1723 - acc: 0.9316 - val_loss: 0.7306 - val_acc: 0.7979\n",
      "Epoch 53/100\n",
      " - 1s - loss: 0.1729 - acc: 0.9326 - val_loss: 0.7261 - val_acc: 0.7936\n",
      "Epoch 54/100\n",
      " - 1s - loss: 0.1697 - acc: 0.9346 - val_loss: 0.7443 - val_acc: 0.7960\n",
      "Epoch 55/100\n",
      " - 1s - loss: 0.1682 - acc: 0.9338 - val_loss: 0.7402 - val_acc: 0.7914\n",
      "Epoch 56/100\n",
      " - 1s - loss: 0.1645 - acc: 0.9348 - val_loss: 0.7429 - val_acc: 0.7942\n",
      "Epoch 57/100\n",
      " - 1s - loss: 0.1607 - acc: 0.9386 - val_loss: 0.7544 - val_acc: 0.7962\n",
      "Epoch 58/100\n",
      " - 2s - loss: 0.1621 - acc: 0.9382 - val_loss: 0.7542 - val_acc: 0.7934\n",
      "Epoch 59/100\n",
      " - 2s - loss: 0.1574 - acc: 0.9386 - val_loss: 0.7640 - val_acc: 0.7977\n",
      "Epoch 60/100\n",
      " - 2s - loss: 0.1603 - acc: 0.9373 - val_loss: 0.7520 - val_acc: 0.7978\n",
      "Epoch 61/100\n",
      " - 2s - loss: 0.1539 - acc: 0.9421 - val_loss: 0.7678 - val_acc: 0.7928\n",
      "Epoch 62/100\n",
      " - 1s - loss: 0.1493 - acc: 0.9430 - val_loss: 0.7802 - val_acc: 0.7957\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.1451 - acc: 0.9431 - val_loss: 0.7821 - val_acc: 0.7946\n",
      "Epoch 64/100\n",
      " - 1s - loss: 0.1444 - acc: 0.9457 - val_loss: 0.7858 - val_acc: 0.7951\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.1456 - acc: 0.9432 - val_loss: 0.7865 - val_acc: 0.7933\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.1422 - acc: 0.9455 - val_loss: 0.7961 - val_acc: 0.7923\n",
      "Epoch 67/100\n",
      " - 1s - loss: 0.1420 - acc: 0.9454 - val_loss: 0.7950 - val_acc: 0.7926\n",
      "Epoch 68/100\n",
      " - 1s - loss: 0.1414 - acc: 0.9454 - val_loss: 0.7938 - val_acc: 0.7912\n",
      "Epoch 69/100\n",
      " - 1s - loss: 0.1402 - acc: 0.9461 - val_loss: 0.7958 - val_acc: 0.7883\n",
      "Epoch 70/100\n",
      " - 1s - loss: 0.1382 - acc: 0.9464 - val_loss: 0.8135 - val_acc: 0.7943\n",
      "Epoch 71/100\n",
      " - 1s - loss: 0.1320 - acc: 0.9497 - val_loss: 0.8117 - val_acc: 0.7951\n",
      "Epoch 72/100\n",
      " - 1s - loss: 0.1335 - acc: 0.9494 - val_loss: 0.8108 - val_acc: 0.7922\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.1313 - acc: 0.9499 - val_loss: 0.8169 - val_acc: 0.7917\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.1283 - acc: 0.9501 - val_loss: 0.8198 - val_acc: 0.7882\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.1300 - acc: 0.9499 - val_loss: 0.8231 - val_acc: 0.7961\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.1232 - acc: 0.9537 - val_loss: 0.8325 - val_acc: 0.7948\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.1280 - acc: 0.9518 - val_loss: 0.8334 - val_acc: 0.7954\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.1242 - acc: 0.9534 - val_loss: 0.8303 - val_acc: 0.7967\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.1262 - acc: 0.9519 - val_loss: 0.8246 - val_acc: 0.7944\n",
      "Epoch 80/100\n",
      " - 1s - loss: 0.1220 - acc: 0.9539 - val_loss: 0.8326 - val_acc: 0.7962\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.1211 - acc: 0.9529 - val_loss: 0.8363 - val_acc: 0.7942\n",
      "Epoch 82/100\n",
      " - 1s - loss: 0.1183 - acc: 0.9549 - val_loss: 0.8487 - val_acc: 0.7942\n",
      "Epoch 83/100\n",
      " - 1s - loss: 0.1191 - acc: 0.9548 - val_loss: 0.8526 - val_acc: 0.7897\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.1160 - acc: 0.9563 - val_loss: 0.8587 - val_acc: 0.7934\n",
      "Epoch 85/100\n",
      " - 1s - loss: 0.1139 - acc: 0.9571 - val_loss: 0.8662 - val_acc: 0.7933\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.1163 - acc: 0.9549 - val_loss: 0.8647 - val_acc: 0.7923\n",
      "Epoch 87/100\n",
      " - 1s - loss: 0.1113 - acc: 0.9570 - val_loss: 0.8686 - val_acc: 0.7944\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.1149 - acc: 0.9570 - val_loss: 0.8753 - val_acc: 0.7910\n",
      "Epoch 89/100\n",
      " - 1s - loss: 0.1131 - acc: 0.9571 - val_loss: 0.8651 - val_acc: 0.7906\n",
      "Epoch 90/100\n",
      " - 1s - loss: 0.1080 - acc: 0.9610 - val_loss: 0.8700 - val_acc: 0.7912\n",
      "Epoch 91/100\n",
      " - 1s - loss: 0.1062 - acc: 0.9589 - val_loss: 0.8771 - val_acc: 0.7931\n",
      "Epoch 92/100\n",
      " - 1s - loss: 0.1077 - acc: 0.9598 - val_loss: 0.8883 - val_acc: 0.7951\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.1112 - acc: 0.9577 - val_loss: 0.8800 - val_acc: 0.7911\n",
      "Epoch 94/100\n",
      " - 1s - loss: 0.1089 - acc: 0.9581 - val_loss: 0.8766 - val_acc: 0.7917\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.1070 - acc: 0.9608 - val_loss: 0.8981 - val_acc: 0.7925\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.1070 - acc: 0.9607 - val_loss: 0.8946 - val_acc: 0.7914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      " - 1s - loss: 0.1030 - acc: 0.9630 - val_loss: 0.8922 - val_acc: 0.7889\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.1032 - acc: 0.9607 - val_loss: 0.8883 - val_acc: 0.7926\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.1054 - acc: 0.9615 - val_loss: 0.8950 - val_acc: 0.7916\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.1041 - acc: 0.9613 - val_loss: 0.9006 - val_acc: 0.7923\n"
     ]
    }
   ],
   "source": [
    "increasedNeurons1 = 1000\n",
    "model4 = getModel(increasedDropout, neurons1=increasedNeurons1)\n",
    "net4 = model4.fit(X, y, epochs=100, batch_size=512, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the increased number of neurons in the first layer we see that the validation loss value has reached to 0.5280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAEMCAYAAABkwamIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss 0.5306921957160273\n"
     ]
    }
   ],
   "source": [
    "valid_loss4 = net4.history[\"val_loss\"]\n",
    "plt.plot(valid_loss1, linewidth=3, label=\"The baseline\")\n",
    "#plt.plot(valid_loss3, linewidth=3, label=\"Increased dropout & epoch\")\n",
    "plt.plot(valid_loss4, linewidth=3, label=\"Increased neurons\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"validation loss\")\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0.47, 0.8)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "print('best validation loss', min(valid_loss4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decrease learning rate\n",
    "\n",
    "To prevent overfitting decrease the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 0.8451 - acc: 0.6974 - val_loss: 0.6203 - val_acc: 0.7654\n",
      "Epoch 2/300\n",
      " - 1s - loss: 0.6398 - acc: 0.7584 - val_loss: 0.5897 - val_acc: 0.7757\n",
      "Epoch 3/300\n",
      " - 1s - loss: 0.6038 - acc: 0.7708 - val_loss: 0.5735 - val_acc: 0.7790\n",
      "Epoch 4/300\n",
      " - 1s - loss: 0.5826 - acc: 0.7775 - val_loss: 0.5713 - val_acc: 0.7794\n",
      "Epoch 5/300\n",
      " - 1s - loss: 0.5641 - acc: 0.7874 - val_loss: 0.5570 - val_acc: 0.7851\n",
      "Epoch 6/300\n",
      " - 2s - loss: 0.5513 - acc: 0.7879 - val_loss: 0.5530 - val_acc: 0.7860\n",
      "Epoch 7/300\n",
      " - 1s - loss: 0.5393 - acc: 0.7918 - val_loss: 0.5484 - val_acc: 0.7896\n",
      "Epoch 8/300\n",
      " - 1s - loss: 0.5303 - acc: 0.7956 - val_loss: 0.5472 - val_acc: 0.7858\n",
      "Epoch 9/300\n",
      " - 1s - loss: 0.5223 - acc: 0.7988 - val_loss: 0.5421 - val_acc: 0.7922\n",
      "Epoch 10/300\n",
      " - 1s - loss: 0.5109 - acc: 0.8031 - val_loss: 0.5385 - val_acc: 0.7925\n",
      "Epoch 11/300\n",
      " - 1s - loss: 0.5040 - acc: 0.8068 - val_loss: 0.5379 - val_acc: 0.7911\n",
      "Epoch 12/300\n",
      " - 1s - loss: 0.4993 - acc: 0.8082 - val_loss: 0.5354 - val_acc: 0.7909\n",
      "Epoch 13/300\n",
      " - 1s - loss: 0.4895 - acc: 0.8089 - val_loss: 0.5314 - val_acc: 0.7943\n",
      "Epoch 14/300\n",
      " - 1s - loss: 0.4846 - acc: 0.8112 - val_loss: 0.5319 - val_acc: 0.7962\n",
      "Epoch 15/300\n",
      " - 1s - loss: 0.4778 - acc: 0.8147 - val_loss: 0.5310 - val_acc: 0.7903\n",
      "Epoch 16/300\n",
      " - 2s - loss: 0.4701 - acc: 0.8162 - val_loss: 0.5272 - val_acc: 0.7951\n",
      "Epoch 17/300\n",
      " - 2s - loss: 0.4654 - acc: 0.8192 - val_loss: 0.5293 - val_acc: 0.7929\n",
      "Epoch 18/300\n",
      " - 2s - loss: 0.4579 - acc: 0.8212 - val_loss: 0.5275 - val_acc: 0.7956\n",
      "Epoch 19/300\n",
      " - 2s - loss: 0.4569 - acc: 0.8232 - val_loss: 0.5260 - val_acc: 0.7951\n",
      "Epoch 20/300\n",
      " - 2s - loss: 0.4505 - acc: 0.8254 - val_loss: 0.5239 - val_acc: 0.7949\n",
      "Epoch 21/300\n",
      " - 2s - loss: 0.4427 - acc: 0.8282 - val_loss: 0.5231 - val_acc: 0.7979\n",
      "Epoch 22/300\n",
      " - 2s - loss: 0.4401 - acc: 0.8290 - val_loss: 0.5221 - val_acc: 0.7961\n",
      "Epoch 23/300\n",
      " - 2s - loss: 0.4327 - acc: 0.8309 - val_loss: 0.5217 - val_acc: 0.7984\n",
      "Epoch 24/300\n",
      " - 2s - loss: 0.4321 - acc: 0.8331 - val_loss: 0.5218 - val_acc: 0.7983\n",
      "Epoch 25/300\n",
      " - 2s - loss: 0.4259 - acc: 0.8345 - val_loss: 0.5217 - val_acc: 0.7954\n",
      "Epoch 26/300\n",
      " - 2s - loss: 0.4218 - acc: 0.8359 - val_loss: 0.5201 - val_acc: 0.8011\n",
      "Epoch 27/300\n",
      " - 1s - loss: 0.4166 - acc: 0.8401 - val_loss: 0.5215 - val_acc: 0.7972\n",
      "Epoch 28/300\n",
      " - 1s - loss: 0.4137 - acc: 0.8392 - val_loss: 0.5217 - val_acc: 0.7972\n",
      "Epoch 29/300\n",
      " - 1s - loss: 0.4096 - acc: 0.8426 - val_loss: 0.5215 - val_acc: 0.7983\n",
      "Epoch 30/300\n",
      " - 1s - loss: 0.4083 - acc: 0.8401 - val_loss: 0.5217 - val_acc: 0.7969\n",
      "Epoch 31/300\n",
      " - 2s - loss: 0.4003 - acc: 0.8453 - val_loss: 0.5196 - val_acc: 0.7980\n",
      "Epoch 32/300\n",
      " - 2s - loss: 0.4015 - acc: 0.8430 - val_loss: 0.5203 - val_acc: 0.7989\n",
      "Epoch 33/300\n",
      " - 1s - loss: 0.3986 - acc: 0.8444 - val_loss: 0.5247 - val_acc: 0.7952\n",
      "Epoch 34/300\n",
      " - 1s - loss: 0.3923 - acc: 0.8473 - val_loss: 0.5214 - val_acc: 0.7984\n",
      "Epoch 35/300\n",
      " - 1s - loss: 0.3929 - acc: 0.8468 - val_loss: 0.5226 - val_acc: 0.7985\n",
      "Epoch 36/300\n",
      " - 2s - loss: 0.3862 - acc: 0.8486 - val_loss: 0.5212 - val_acc: 0.7996\n",
      "Epoch 37/300\n",
      " - 2s - loss: 0.3839 - acc: 0.8496 - val_loss: 0.5228 - val_acc: 0.8012\n",
      "Epoch 38/300\n",
      " - 1s - loss: 0.3838 - acc: 0.8504 - val_loss: 0.5246 - val_acc: 0.7966\n",
      "Epoch 39/300\n",
      " - 1s - loss: 0.3773 - acc: 0.8533 - val_loss: 0.5274 - val_acc: 0.7966\n",
      "Epoch 40/300\n",
      " - 1s - loss: 0.3767 - acc: 0.8533 - val_loss: 0.5245 - val_acc: 0.7999\n",
      "Epoch 41/300\n",
      " - 1s - loss: 0.3721 - acc: 0.8530 - val_loss: 0.5295 - val_acc: 0.7946\n",
      "Epoch 42/300\n",
      " - 1s - loss: 0.3708 - acc: 0.8537 - val_loss: 0.5238 - val_acc: 0.8012\n",
      "Epoch 43/300\n",
      " - 2s - loss: 0.3702 - acc: 0.8562 - val_loss: 0.5280 - val_acc: 0.7969\n",
      "Epoch 44/300\n",
      " - 2s - loss: 0.3657 - acc: 0.8579 - val_loss: 0.5242 - val_acc: 0.8005\n",
      "Epoch 45/300\n",
      " - 2s - loss: 0.3619 - acc: 0.8583 - val_loss: 0.5246 - val_acc: 0.7997\n",
      "Epoch 46/300\n",
      " - 2s - loss: 0.3620 - acc: 0.8592 - val_loss: 0.5290 - val_acc: 0.8013\n",
      "Epoch 47/300\n",
      " - 2s - loss: 0.3606 - acc: 0.8593 - val_loss: 0.5254 - val_acc: 0.7995\n",
      "Epoch 48/300\n",
      " - 1s - loss: 0.3553 - acc: 0.8600 - val_loss: 0.5267 - val_acc: 0.7997\n",
      "Epoch 49/300\n",
      " - 1s - loss: 0.3530 - acc: 0.8610 - val_loss: 0.5294 - val_acc: 0.8002\n",
      "Epoch 50/300\n",
      " - 1s - loss: 0.3508 - acc: 0.8632 - val_loss: 0.5288 - val_acc: 0.8002\n",
      "Epoch 51/300\n",
      " - 1s - loss: 0.3477 - acc: 0.8641 - val_loss: 0.5279 - val_acc: 0.8007\n",
      "Epoch 52/300\n",
      " - 2s - loss: 0.3477 - acc: 0.8627 - val_loss: 0.5279 - val_acc: 0.8000\n",
      "Epoch 53/300\n",
      " - 2s - loss: 0.3419 - acc: 0.8659 - val_loss: 0.5294 - val_acc: 0.7988\n",
      "Epoch 54/300\n",
      " - 2s - loss: 0.3396 - acc: 0.8647 - val_loss: 0.5312 - val_acc: 0.8016\n",
      "Epoch 55/300\n",
      " - 2s - loss: 0.3410 - acc: 0.8675 - val_loss: 0.5304 - val_acc: 0.7984\n",
      "Epoch 56/300\n",
      " - 2s - loss: 0.3375 - acc: 0.8673 - val_loss: 0.5362 - val_acc: 0.7986\n",
      "Epoch 57/300\n",
      " - 1s - loss: 0.3354 - acc: 0.8691 - val_loss: 0.5335 - val_acc: 0.8024\n",
      "Epoch 58/300\n",
      " - 1s - loss: 0.3324 - acc: 0.8719 - val_loss: 0.5348 - val_acc: 0.7994\n",
      "Epoch 59/300\n",
      " - 1s - loss: 0.3303 - acc: 0.8707 - val_loss: 0.5335 - val_acc: 0.7996\n",
      "Epoch 60/300\n",
      " - 1s - loss: 0.3305 - acc: 0.8701 - val_loss: 0.5369 - val_acc: 0.7992\n",
      "Epoch 61/300\n",
      " - 1s - loss: 0.3303 - acc: 0.8714 - val_loss: 0.5354 - val_acc: 0.7988\n",
      "Epoch 62/300\n",
      " - 1s - loss: 0.3240 - acc: 0.8740 - val_loss: 0.5367 - val_acc: 0.8008\n",
      "Epoch 63/300\n",
      " - 1s - loss: 0.3253 - acc: 0.8728 - val_loss: 0.5375 - val_acc: 0.8001\n",
      "Epoch 64/300\n",
      " - 1s - loss: 0.3214 - acc: 0.8751 - val_loss: 0.5378 - val_acc: 0.7980\n",
      "Epoch 65/300\n",
      " - 1s - loss: 0.3197 - acc: 0.8766 - val_loss: 0.5394 - val_acc: 0.8002\n",
      "Epoch 66/300\n",
      " - 1s - loss: 0.3183 - acc: 0.8746 - val_loss: 0.5389 - val_acc: 0.7999\n",
      "Epoch 67/300\n",
      " - 1s - loss: 0.3142 - acc: 0.8761 - val_loss: 0.5428 - val_acc: 0.8016\n",
      "Epoch 68/300\n",
      " - 1s - loss: 0.3174 - acc: 0.8754 - val_loss: 0.5391 - val_acc: 0.8000\n",
      "Epoch 69/300\n",
      " - 1s - loss: 0.3165 - acc: 0.8769 - val_loss: 0.5384 - val_acc: 0.8003\n",
      "Epoch 70/300\n",
      " - 1s - loss: 0.3136 - acc: 0.8790 - val_loss: 0.5428 - val_acc: 0.8007\n",
      "Epoch 71/300\n",
      " - 1s - loss: 0.3114 - acc: 0.8759 - val_loss: 0.5424 - val_acc: 0.8005\n",
      "Epoch 72/300\n",
      " - 1s - loss: 0.3080 - acc: 0.8802 - val_loss: 0.5412 - val_acc: 0.8005\n",
      "Epoch 73/300\n",
      " - 1s - loss: 0.3052 - acc: 0.8790 - val_loss: 0.5433 - val_acc: 0.8005\n",
      "Epoch 74/300\n",
      " - 1s - loss: 0.3061 - acc: 0.8810 - val_loss: 0.5436 - val_acc: 0.8024\n",
      "Epoch 75/300\n",
      " - 1s - loss: 0.3024 - acc: 0.8833 - val_loss: 0.5462 - val_acc: 0.8007\n",
      "Epoch 76/300\n",
      " - 1s - loss: 0.3057 - acc: 0.8803 - val_loss: 0.5458 - val_acc: 0.8024\n",
      "Epoch 77/300\n",
      " - 1s - loss: 0.2988 - acc: 0.8834 - val_loss: 0.5463 - val_acc: 0.7994\n",
      "Epoch 78/300\n",
      " - 1s - loss: 0.3001 - acc: 0.8815 - val_loss: 0.5493 - val_acc: 0.7990\n",
      "Epoch 79/300\n",
      " - 1s - loss: 0.2996 - acc: 0.8823 - val_loss: 0.5465 - val_acc: 0.8013\n",
      "Epoch 80/300\n",
      " - 1s - loss: 0.2964 - acc: 0.8851 - val_loss: 0.5480 - val_acc: 0.8012\n",
      "Epoch 81/300\n",
      " - 2s - loss: 0.2946 - acc: 0.8853 - val_loss: 0.5480 - val_acc: 0.8020\n",
      "Epoch 82/300\n",
      " - 2s - loss: 0.2939 - acc: 0.8839 - val_loss: 0.5491 - val_acc: 0.8029\n",
      "Epoch 83/300\n",
      " - 2s - loss: 0.2918 - acc: 0.8866 - val_loss: 0.5509 - val_acc: 0.8023\n",
      "Epoch 84/300\n",
      " - 2s - loss: 0.2910 - acc: 0.8864 - val_loss: 0.5512 - val_acc: 0.8036\n",
      "Epoch 85/300\n",
      " - 1s - loss: 0.2882 - acc: 0.8883 - val_loss: 0.5546 - val_acc: 0.8040\n",
      "Epoch 86/300\n",
      " - 2s - loss: 0.2891 - acc: 0.8857 - val_loss: 0.5538 - val_acc: 0.8014\n",
      "Epoch 87/300\n",
      " - 1s - loss: 0.2861 - acc: 0.8889 - val_loss: 0.5548 - val_acc: 0.8029\n",
      "Epoch 88/300\n",
      " - 2s - loss: 0.2863 - acc: 0.8880 - val_loss: 0.5555 - val_acc: 0.8025\n",
      "Epoch 89/300\n",
      " - 2s - loss: 0.2834 - acc: 0.8907 - val_loss: 0.5553 - val_acc: 0.8020\n",
      "Epoch 90/300\n",
      " - 1s - loss: 0.2846 - acc: 0.8897 - val_loss: 0.5556 - val_acc: 0.8026\n",
      "Epoch 91/300\n",
      " - 1s - loss: 0.2854 - acc: 0.8885 - val_loss: 0.5562 - val_acc: 0.8016\n",
      "Epoch 92/300\n",
      " - 1s - loss: 0.2786 - acc: 0.8906 - val_loss: 0.5599 - val_acc: 0.8032\n",
      "Epoch 93/300\n",
      " - 2s - loss: 0.2789 - acc: 0.8923 - val_loss: 0.5603 - val_acc: 0.8018\n",
      "Epoch 94/300\n",
      " - 2s - loss: 0.2789 - acc: 0.8923 - val_loss: 0.5595 - val_acc: 0.8023\n",
      "Epoch 95/300\n",
      " - 2s - loss: 0.2784 - acc: 0.8920 - val_loss: 0.5590 - val_acc: 0.8024\n",
      "Epoch 96/300\n",
      " - 2s - loss: 0.2768 - acc: 0.8921 - val_loss: 0.5589 - val_acc: 0.8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300\n",
      " - 2s - loss: 0.2746 - acc: 0.8926 - val_loss: 0.5619 - val_acc: 0.8011\n",
      "Epoch 98/300\n",
      " - 2s - loss: 0.2751 - acc: 0.8928 - val_loss: 0.5630 - val_acc: 0.8029\n",
      "Epoch 99/300\n",
      " - 2s - loss: 0.2710 - acc: 0.8943 - val_loss: 0.5661 - val_acc: 0.8025\n",
      "Epoch 100/300\n",
      " - 2s - loss: 0.2724 - acc: 0.8913 - val_loss: 0.5651 - val_acc: 0.8028\n",
      "Epoch 101/300\n",
      " - 2s - loss: 0.2696 - acc: 0.8969 - val_loss: 0.5655 - val_acc: 0.8009\n",
      "Epoch 102/300\n",
      " - 2s - loss: 0.2679 - acc: 0.8960 - val_loss: 0.5662 - val_acc: 0.8005\n",
      "Epoch 103/300\n",
      " - 2s - loss: 0.2640 - acc: 0.8962 - val_loss: 0.5690 - val_acc: 0.8018\n",
      "Epoch 104/300\n",
      " - 2s - loss: 0.2673 - acc: 0.8961 - val_loss: 0.5675 - val_acc: 0.8002\n",
      "Epoch 105/300\n",
      " - 2s - loss: 0.2683 - acc: 0.8938 - val_loss: 0.5702 - val_acc: 0.8023\n",
      "Epoch 106/300\n",
      " - 2s - loss: 0.2660 - acc: 0.8960 - val_loss: 0.5679 - val_acc: 0.8014\n",
      "Epoch 107/300\n",
      " - 2s - loss: 0.2656 - acc: 0.8956 - val_loss: 0.5683 - val_acc: 0.8007\n",
      "Epoch 108/300\n",
      " - 2s - loss: 0.2646 - acc: 0.8972 - val_loss: 0.5740 - val_acc: 0.8024\n",
      "Epoch 109/300\n",
      " - 2s - loss: 0.2617 - acc: 0.8986 - val_loss: 0.5722 - val_acc: 0.8022\n",
      "Epoch 110/300\n",
      " - 1s - loss: 0.2610 - acc: 0.8990 - val_loss: 0.5711 - val_acc: 0.7999\n",
      "Epoch 111/300\n",
      " - 2s - loss: 0.2621 - acc: 0.8973 - val_loss: 0.5720 - val_acc: 0.8008\n",
      "Epoch 112/300\n",
      " - 2s - loss: 0.2625 - acc: 0.8973 - val_loss: 0.5723 - val_acc: 0.8011\n",
      "Epoch 113/300\n",
      " - 1s - loss: 0.2579 - acc: 0.8992 - val_loss: 0.5742 - val_acc: 0.8032\n",
      "Epoch 114/300\n",
      " - 1s - loss: 0.2558 - acc: 0.9020 - val_loss: 0.5772 - val_acc: 0.7997\n",
      "Epoch 115/300\n",
      " - 2s - loss: 0.2565 - acc: 0.9003 - val_loss: 0.5755 - val_acc: 0.7989\n",
      "Epoch 116/300\n",
      " - 2s - loss: 0.2550 - acc: 0.8997 - val_loss: 0.5773 - val_acc: 0.8014\n",
      "Epoch 117/300\n",
      " - 2s - loss: 0.2540 - acc: 0.9017 - val_loss: 0.5768 - val_acc: 0.8000\n",
      "Epoch 118/300\n",
      " - 2s - loss: 0.2578 - acc: 0.8991 - val_loss: 0.5780 - val_acc: 0.8014\n",
      "Epoch 119/300\n",
      " - 1s - loss: 0.2547 - acc: 0.9001 - val_loss: 0.5781 - val_acc: 0.8032\n",
      "Epoch 120/300\n",
      " - 1s - loss: 0.2514 - acc: 0.9024 - val_loss: 0.5799 - val_acc: 0.8045\n",
      "Epoch 121/300\n",
      " - 1s - loss: 0.2518 - acc: 0.9036 - val_loss: 0.5809 - val_acc: 0.8025\n",
      "Epoch 122/300\n",
      " - 1s - loss: 0.2494 - acc: 0.9030 - val_loss: 0.5797 - val_acc: 0.8012\n",
      "Epoch 123/300\n",
      " - 1s - loss: 0.2516 - acc: 0.9024 - val_loss: 0.5831 - val_acc: 0.8001\n",
      "Epoch 124/300\n",
      " - 1s - loss: 0.2519 - acc: 0.9023 - val_loss: 0.5827 - val_acc: 0.8014\n",
      "Epoch 125/300\n",
      " - 1s - loss: 0.2445 - acc: 0.9056 - val_loss: 0.5804 - val_acc: 0.8013\n",
      "Epoch 126/300\n",
      " - 1s - loss: 0.2464 - acc: 0.9055 - val_loss: 0.5827 - val_acc: 0.8005\n",
      "Epoch 127/300\n",
      " - 1s - loss: 0.2452 - acc: 0.9044 - val_loss: 0.5855 - val_acc: 0.8014\n",
      "Epoch 128/300\n",
      " - 1s - loss: 0.2454 - acc: 0.9061 - val_loss: 0.5852 - val_acc: 0.8016\n",
      "Epoch 129/300\n",
      " - 1s - loss: 0.2430 - acc: 0.9053 - val_loss: 0.5868 - val_acc: 0.7994\n",
      "Epoch 130/300\n",
      " - 2s - loss: 0.2428 - acc: 0.9060 - val_loss: 0.5907 - val_acc: 0.8003\n",
      "Epoch 131/300\n",
      " - 1s - loss: 0.2401 - acc: 0.9048 - val_loss: 0.5899 - val_acc: 0.8012\n",
      "Epoch 132/300\n",
      " - 1s - loss: 0.2413 - acc: 0.9061 - val_loss: 0.5881 - val_acc: 0.8023\n",
      "Epoch 133/300\n",
      " - 1s - loss: 0.2398 - acc: 0.9064 - val_loss: 0.5907 - val_acc: 0.7996\n",
      "Epoch 134/300\n",
      " - 1s - loss: 0.2400 - acc: 0.9075 - val_loss: 0.5917 - val_acc: 0.8002\n",
      "Epoch 135/300\n",
      " - 1s - loss: 0.2392 - acc: 0.9080 - val_loss: 0.5954 - val_acc: 0.8011\n",
      "Epoch 136/300\n",
      " - 1s - loss: 0.2404 - acc: 0.9072 - val_loss: 0.5922 - val_acc: 0.7997\n",
      "Epoch 137/300\n",
      " - 2s - loss: 0.2386 - acc: 0.9079 - val_loss: 0.5919 - val_acc: 0.8011\n",
      "Epoch 138/300\n",
      " - 2s - loss: 0.2364 - acc: 0.9090 - val_loss: 0.5937 - val_acc: 0.7994\n",
      "Epoch 139/300\n",
      " - 2s - loss: 0.2333 - acc: 0.9097 - val_loss: 0.5938 - val_acc: 0.8011\n",
      "Epoch 140/300\n",
      " - 2s - loss: 0.2366 - acc: 0.9070 - val_loss: 0.5926 - val_acc: 0.8012\n",
      "Epoch 141/300\n",
      " - 2s - loss: 0.2365 - acc: 0.9081 - val_loss: 0.5956 - val_acc: 0.8011\n",
      "Epoch 142/300\n",
      " - 2s - loss: 0.2336 - acc: 0.9092 - val_loss: 0.5952 - val_acc: 0.8009\n",
      "Epoch 143/300\n",
      " - 2s - loss: 0.2313 - acc: 0.9107 - val_loss: 0.5964 - val_acc: 0.7992\n",
      "Epoch 144/300\n",
      " - 2s - loss: 0.2325 - acc: 0.9089 - val_loss: 0.5943 - val_acc: 0.7996\n",
      "Epoch 145/300\n",
      " - 2s - loss: 0.2346 - acc: 0.9084 - val_loss: 0.5979 - val_acc: 0.7995\n",
      "Epoch 146/300\n",
      " - 2s - loss: 0.2296 - acc: 0.9117 - val_loss: 0.5988 - val_acc: 0.8014\n",
      "Epoch 147/300\n",
      " - 2s - loss: 0.2262 - acc: 0.9114 - val_loss: 0.6022 - val_acc: 0.8016\n",
      "Epoch 148/300\n",
      " - 2s - loss: 0.2314 - acc: 0.9098 - val_loss: 0.5980 - val_acc: 0.8011\n",
      "Epoch 149/300\n",
      " - 1s - loss: 0.2314 - acc: 0.9113 - val_loss: 0.6007 - val_acc: 0.8029\n",
      "Epoch 150/300\n",
      " - 1s - loss: 0.2290 - acc: 0.9111 - val_loss: 0.6002 - val_acc: 0.8005\n",
      "Epoch 151/300\n",
      " - 2s - loss: 0.2293 - acc: 0.9091 - val_loss: 0.6017 - val_acc: 0.8005\n",
      "Epoch 152/300\n",
      " - 1s - loss: 0.2269 - acc: 0.9117 - val_loss: 0.6024 - val_acc: 0.7988\n",
      "Epoch 153/300\n",
      " - 1s - loss: 0.2289 - acc: 0.9107 - val_loss: 0.6037 - val_acc: 0.8007\n",
      "Epoch 154/300\n",
      " - 2s - loss: 0.2255 - acc: 0.9124 - val_loss: 0.6020 - val_acc: 0.8011\n",
      "Epoch 155/300\n",
      " - 2s - loss: 0.2232 - acc: 0.9124 - val_loss: 0.6034 - val_acc: 0.8000\n",
      "Epoch 156/300\n",
      " - 1s - loss: 0.2283 - acc: 0.9110 - val_loss: 0.6038 - val_acc: 0.8002\n",
      "Epoch 157/300\n",
      " - 1s - loss: 0.2246 - acc: 0.9130 - val_loss: 0.6054 - val_acc: 0.7982\n",
      "Epoch 158/300\n",
      " - 1s - loss: 0.2189 - acc: 0.9162 - val_loss: 0.6070 - val_acc: 0.8002\n",
      "Epoch 159/300\n",
      " - 1s - loss: 0.2249 - acc: 0.9134 - val_loss: 0.6056 - val_acc: 0.7997\n",
      "Epoch 160/300\n",
      " - 2s - loss: 0.2260 - acc: 0.9132 - val_loss: 0.6046 - val_acc: 0.8008\n",
      "Epoch 161/300\n",
      " - 2s - loss: 0.2162 - acc: 0.9177 - val_loss: 0.6095 - val_acc: 0.7997\n",
      "Epoch 162/300\n",
      " - 2s - loss: 0.2199 - acc: 0.9141 - val_loss: 0.6099 - val_acc: 0.8008\n",
      "Epoch 163/300\n",
      " - 2s - loss: 0.2222 - acc: 0.9134 - val_loss: 0.6089 - val_acc: 0.7991\n",
      "Epoch 164/300\n",
      " - 2s - loss: 0.2183 - acc: 0.9157 - val_loss: 0.6104 - val_acc: 0.8014\n",
      "Epoch 165/300\n",
      " - 1s - loss: 0.2223 - acc: 0.9133 - val_loss: 0.6117 - val_acc: 0.7997\n",
      "Epoch 166/300\n",
      " - 1s - loss: 0.2159 - acc: 0.9157 - val_loss: 0.6112 - val_acc: 0.7997\n",
      "Epoch 167/300\n",
      " - 2s - loss: 0.2200 - acc: 0.9144 - val_loss: 0.6116 - val_acc: 0.7988\n",
      "Epoch 168/300\n",
      " - 2s - loss: 0.2174 - acc: 0.9165 - val_loss: 0.6116 - val_acc: 0.8007\n",
      "Epoch 169/300\n",
      " - 1s - loss: 0.2169 - acc: 0.9167 - val_loss: 0.6126 - val_acc: 0.8007\n",
      "Epoch 170/300\n",
      " - 1s - loss: 0.2183 - acc: 0.9166 - val_loss: 0.6137 - val_acc: 0.7996\n",
      "Epoch 171/300\n",
      " - 1s - loss: 0.2168 - acc: 0.9172 - val_loss: 0.6134 - val_acc: 0.7990\n",
      "Epoch 172/300\n",
      " - 1s - loss: 0.2169 - acc: 0.9171 - val_loss: 0.6140 - val_acc: 0.8006\n",
      "Epoch 173/300\n",
      " - 1s - loss: 0.2141 - acc: 0.9181 - val_loss: 0.6148 - val_acc: 0.8001\n",
      "Epoch 174/300\n",
      " - 1s - loss: 0.2155 - acc: 0.9170 - val_loss: 0.6156 - val_acc: 0.7984\n",
      "Epoch 175/300\n",
      " - 1s - loss: 0.2141 - acc: 0.9190 - val_loss: 0.6176 - val_acc: 0.7990\n",
      "Epoch 176/300\n",
      " - 1s - loss: 0.2127 - acc: 0.9166 - val_loss: 0.6181 - val_acc: 0.7991\n",
      "Epoch 177/300\n",
      " - 1s - loss: 0.2157 - acc: 0.9168 - val_loss: 0.6167 - val_acc: 0.7978\n",
      "Epoch 178/300\n",
      " - 1s - loss: 0.2129 - acc: 0.9188 - val_loss: 0.6210 - val_acc: 0.7992\n",
      "Epoch 179/300\n",
      " - 1s - loss: 0.2118 - acc: 0.9175 - val_loss: 0.6189 - val_acc: 0.7999\n",
      "Epoch 180/300\n",
      " - 1s - loss: 0.2083 - acc: 0.9185 - val_loss: 0.6189 - val_acc: 0.7985\n",
      "Epoch 181/300\n",
      " - 1s - loss: 0.2101 - acc: 0.9185 - val_loss: 0.6222 - val_acc: 0.8009\n",
      "Epoch 182/300\n",
      " - 1s - loss: 0.2100 - acc: 0.9184 - val_loss: 0.6186 - val_acc: 0.7980\n",
      "Epoch 183/300\n",
      " - 1s - loss: 0.2098 - acc: 0.9192 - val_loss: 0.6206 - val_acc: 0.8011\n",
      "Epoch 184/300\n",
      " - 1s - loss: 0.2078 - acc: 0.9206 - val_loss: 0.6198 - val_acc: 0.7989\n",
      "Epoch 185/300\n",
      " - 1s - loss: 0.2112 - acc: 0.9186 - val_loss: 0.6234 - val_acc: 0.7999\n",
      "Epoch 186/300\n",
      " - 1s - loss: 0.2062 - acc: 0.9219 - val_loss: 0.6226 - val_acc: 0.8013\n",
      "Epoch 187/300\n",
      " - 1s - loss: 0.2099 - acc: 0.9174 - val_loss: 0.6219 - val_acc: 0.8001\n",
      "Epoch 188/300\n",
      " - 1s - loss: 0.2042 - acc: 0.9206 - val_loss: 0.6237 - val_acc: 0.8002\n",
      "Epoch 189/300\n",
      " - 1s - loss: 0.2065 - acc: 0.9198 - val_loss: 0.6252 - val_acc: 0.8002\n",
      "Epoch 190/300\n",
      " - 2s - loss: 0.2052 - acc: 0.9213 - val_loss: 0.6264 - val_acc: 0.8013\n",
      "Epoch 191/300\n",
      " - 2s - loss: 0.2050 - acc: 0.9209 - val_loss: 0.6272 - val_acc: 0.8013\n",
      "Epoch 192/300\n",
      " - 2s - loss: 0.2031 - acc: 0.9227 - val_loss: 0.6281 - val_acc: 0.8007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/300\n",
      " - 2s - loss: 0.2056 - acc: 0.9198 - val_loss: 0.6249 - val_acc: 0.8003\n",
      "Epoch 194/300\n",
      " - 2s - loss: 0.2038 - acc: 0.9200 - val_loss: 0.6271 - val_acc: 0.8016\n",
      "Epoch 195/300\n",
      " - 2s - loss: 0.2024 - acc: 0.9219 - val_loss: 0.6260 - val_acc: 0.8020\n",
      "Epoch 196/300\n",
      " - 2s - loss: 0.2030 - acc: 0.9220 - val_loss: 0.6271 - val_acc: 0.8009\n",
      "Epoch 197/300\n",
      " - 2s - loss: 0.1992 - acc: 0.9244 - val_loss: 0.6285 - val_acc: 0.7996\n",
      "Epoch 198/300\n",
      " - 2s - loss: 0.2012 - acc: 0.9225 - val_loss: 0.6318 - val_acc: 0.7990\n",
      "Epoch 199/300\n",
      " - 2s - loss: 0.2009 - acc: 0.9230 - val_loss: 0.6315 - val_acc: 0.7994\n",
      "Epoch 200/300\n",
      " - 2s - loss: 0.2030 - acc: 0.9207 - val_loss: 0.6296 - val_acc: 0.7997\n",
      "Epoch 201/300\n",
      " - 2s - loss: 0.1974 - acc: 0.9239 - val_loss: 0.6334 - val_acc: 0.8002\n",
      "Epoch 202/300\n",
      " - 1s - loss: 0.2011 - acc: 0.9249 - val_loss: 0.6332 - val_acc: 0.8000\n",
      "Epoch 203/300\n",
      " - 1s - loss: 0.2001 - acc: 0.9234 - val_loss: 0.6357 - val_acc: 0.7997\n",
      "Epoch 204/300\n",
      " - 2s - loss: 0.1969 - acc: 0.9250 - val_loss: 0.6332 - val_acc: 0.8001\n",
      "Epoch 205/300\n",
      " - 2s - loss: 0.1970 - acc: 0.9239 - val_loss: 0.6359 - val_acc: 0.8017\n",
      "Epoch 206/300\n",
      " - 2s - loss: 0.1956 - acc: 0.9264 - val_loss: 0.6384 - val_acc: 0.8011\n",
      "Epoch 207/300\n",
      " - 1s - loss: 0.1989 - acc: 0.9233 - val_loss: 0.6365 - val_acc: 0.7991\n",
      "Epoch 208/300\n",
      " - 1s - loss: 0.1962 - acc: 0.9251 - val_loss: 0.6367 - val_acc: 0.7985\n",
      "Epoch 209/300\n",
      " - 2s - loss: 0.1966 - acc: 0.9248 - val_loss: 0.6348 - val_acc: 0.7978\n",
      "Epoch 210/300\n",
      " - 1s - loss: 0.1945 - acc: 0.9256 - val_loss: 0.6389 - val_acc: 0.7986\n",
      "Epoch 211/300\n",
      " - 1s - loss: 0.1940 - acc: 0.9263 - val_loss: 0.6391 - val_acc: 0.7986\n",
      "Epoch 212/300\n",
      " - 1s - loss: 0.1971 - acc: 0.9244 - val_loss: 0.6392 - val_acc: 0.8001\n",
      "Epoch 213/300\n",
      " - 2s - loss: 0.1955 - acc: 0.9232 - val_loss: 0.6379 - val_acc: 0.7990\n",
      "Epoch 214/300\n",
      " - 1s - loss: 0.1956 - acc: 0.9257 - val_loss: 0.6416 - val_acc: 0.8006\n",
      "Epoch 215/300\n",
      " - 1s - loss: 0.1946 - acc: 0.9260 - val_loss: 0.6384 - val_acc: 0.8003\n",
      "Epoch 216/300\n",
      " - 2s - loss: 0.1898 - acc: 0.9281 - val_loss: 0.6456 - val_acc: 0.8003\n",
      "Epoch 217/300\n",
      " - 1s - loss: 0.1936 - acc: 0.9247 - val_loss: 0.6421 - val_acc: 0.7994\n",
      "Epoch 218/300\n",
      " - 1s - loss: 0.1928 - acc: 0.9240 - val_loss: 0.6417 - val_acc: 0.7977\n",
      "Epoch 219/300\n",
      " - 2s - loss: 0.1913 - acc: 0.9265 - val_loss: 0.6432 - val_acc: 0.7985\n",
      "Epoch 220/300\n",
      " - 2s - loss: 0.1948 - acc: 0.9251 - val_loss: 0.6434 - val_acc: 0.8003\n",
      "Epoch 221/300\n",
      " - 2s - loss: 0.1914 - acc: 0.9261 - val_loss: 0.6465 - val_acc: 0.7996\n",
      "Epoch 222/300\n",
      " - 2s - loss: 0.1863 - acc: 0.9277 - val_loss: 0.6479 - val_acc: 0.7990\n",
      "Epoch 223/300\n",
      " - 2s - loss: 0.1916 - acc: 0.9253 - val_loss: 0.6436 - val_acc: 0.7992\n",
      "Epoch 224/300\n",
      " - 2s - loss: 0.1910 - acc: 0.9271 - val_loss: 0.6444 - val_acc: 0.7995\n",
      "Epoch 225/300\n",
      " - 2s - loss: 0.1924 - acc: 0.9258 - val_loss: 0.6434 - val_acc: 0.7983\n",
      "Epoch 226/300\n",
      " - 2s - loss: 0.1890 - acc: 0.9270 - val_loss: 0.6441 - val_acc: 0.7983\n",
      "Epoch 227/300\n",
      " - 2s - loss: 0.1933 - acc: 0.9249 - val_loss: 0.6432 - val_acc: 0.7989\n",
      "Epoch 228/300\n",
      " - 2s - loss: 0.1888 - acc: 0.9271 - val_loss: 0.6469 - val_acc: 0.7979\n",
      "Epoch 229/300\n",
      " - 2s - loss: 0.1893 - acc: 0.9279 - val_loss: 0.6451 - val_acc: 0.7977\n",
      "Epoch 230/300\n",
      " - 2s - loss: 0.1882 - acc: 0.9269 - val_loss: 0.6465 - val_acc: 0.7988\n",
      "Epoch 231/300\n",
      " - 2s - loss: 0.1892 - acc: 0.9277 - val_loss: 0.6506 - val_acc: 0.7990\n",
      "Epoch 232/300\n",
      " - 2s - loss: 0.1851 - acc: 0.9295 - val_loss: 0.6489 - val_acc: 0.7979\n",
      "Epoch 233/300\n",
      " - 2s - loss: 0.1849 - acc: 0.9282 - val_loss: 0.6527 - val_acc: 0.7980\n",
      "Epoch 234/300\n",
      " - 1s - loss: 0.1854 - acc: 0.9286 - val_loss: 0.6547 - val_acc: 0.7978\n",
      "Epoch 235/300\n",
      " - 2s - loss: 0.1839 - acc: 0.9293 - val_loss: 0.6537 - val_acc: 0.7979\n",
      "Epoch 236/300\n",
      " - 2s - loss: 0.1849 - acc: 0.9287 - val_loss: 0.6504 - val_acc: 0.7984\n",
      "Epoch 237/300\n",
      " - 2s - loss: 0.1851 - acc: 0.9294 - val_loss: 0.6542 - val_acc: 0.7983\n",
      "Epoch 238/300\n",
      " - 2s - loss: 0.1851 - acc: 0.9286 - val_loss: 0.6522 - val_acc: 0.7972\n",
      "Epoch 239/300\n",
      " - 2s - loss: 0.1819 - acc: 0.9300 - val_loss: 0.6550 - val_acc: 0.7978\n",
      "Epoch 240/300\n",
      " - 2s - loss: 0.1842 - acc: 0.9297 - val_loss: 0.6557 - val_acc: 0.7985\n",
      "Epoch 241/300\n",
      " - 2s - loss: 0.1859 - acc: 0.9295 - val_loss: 0.6524 - val_acc: 0.7982\n",
      "Epoch 242/300\n",
      " - 2s - loss: 0.1820 - acc: 0.9301 - val_loss: 0.6562 - val_acc: 0.7982\n",
      "Epoch 243/300\n",
      " - 2s - loss: 0.1813 - acc: 0.9309 - val_loss: 0.6592 - val_acc: 0.8012\n",
      "Epoch 244/300\n",
      " - 2s - loss: 0.1857 - acc: 0.9286 - val_loss: 0.6547 - val_acc: 0.7995\n",
      "Epoch 245/300\n",
      " - 2s - loss: 0.1830 - acc: 0.9297 - val_loss: 0.6549 - val_acc: 0.8006\n",
      "Epoch 246/300\n",
      " - 2s - loss: 0.1826 - acc: 0.9299 - val_loss: 0.6540 - val_acc: 0.8002\n",
      "Epoch 247/300\n",
      " - 1s - loss: 0.1808 - acc: 0.9309 - val_loss: 0.6553 - val_acc: 0.7990\n",
      "Epoch 248/300\n",
      " - 2s - loss: 0.1842 - acc: 0.9296 - val_loss: 0.6575 - val_acc: 0.7986\n",
      "Epoch 249/300\n",
      " - 2s - loss: 0.1823 - acc: 0.9279 - val_loss: 0.6576 - val_acc: 0.7995\n",
      "Epoch 250/300\n",
      " - 2s - loss: 0.1793 - acc: 0.9316 - val_loss: 0.6590 - val_acc: 0.7982\n",
      "Epoch 251/300\n",
      " - 2s - loss: 0.1822 - acc: 0.9312 - val_loss: 0.6581 - val_acc: 0.7982\n",
      "Epoch 252/300\n",
      " - 2s - loss: 0.1776 - acc: 0.9333 - val_loss: 0.6608 - val_acc: 0.7991\n",
      "Epoch 253/300\n",
      " - 2s - loss: 0.1802 - acc: 0.9308 - val_loss: 0.6594 - val_acc: 0.7976\n",
      "Epoch 254/300\n",
      " - 2s - loss: 0.1767 - acc: 0.9327 - val_loss: 0.6594 - val_acc: 0.7965\n",
      "Epoch 255/300\n",
      " - 2s - loss: 0.1778 - acc: 0.9320 - val_loss: 0.6596 - val_acc: 0.7982\n",
      "Epoch 256/300\n",
      " - 2s - loss: 0.1796 - acc: 0.9307 - val_loss: 0.6618 - val_acc: 0.7994\n",
      "Epoch 257/300\n",
      " - 2s - loss: 0.1761 - acc: 0.9327 - val_loss: 0.6633 - val_acc: 0.7985\n",
      "Epoch 258/300\n",
      " - 2s - loss: 0.1769 - acc: 0.9326 - val_loss: 0.6627 - val_acc: 0.7985\n",
      "Epoch 259/300\n",
      " - 1s - loss: 0.1773 - acc: 0.9314 - val_loss: 0.6627 - val_acc: 0.7983\n",
      "Epoch 260/300\n",
      " - 1s - loss: 0.1764 - acc: 0.9324 - val_loss: 0.6642 - val_acc: 0.7980\n",
      "Epoch 261/300\n",
      " - 2s - loss: 0.1776 - acc: 0.9321 - val_loss: 0.6636 - val_acc: 0.7985\n",
      "Epoch 262/300\n",
      " - 1s - loss: 0.1797 - acc: 0.9315 - val_loss: 0.6622 - val_acc: 0.7990\n",
      "Epoch 263/300\n",
      " - 2s - loss: 0.1753 - acc: 0.9334 - val_loss: 0.6642 - val_acc: 0.7989\n",
      "Epoch 264/300\n",
      " - 2s - loss: 0.1765 - acc: 0.9328 - val_loss: 0.6653 - val_acc: 0.7989\n",
      "Epoch 265/300\n",
      " - 2s - loss: 0.1740 - acc: 0.9338 - val_loss: 0.6683 - val_acc: 0.7983\n",
      "Epoch 266/300\n",
      " - 2s - loss: 0.1755 - acc: 0.9331 - val_loss: 0.6673 - val_acc: 0.7980\n",
      "Epoch 267/300\n",
      " - 2s - loss: 0.1747 - acc: 0.9326 - val_loss: 0.6664 - val_acc: 0.7971\n",
      "Epoch 268/300\n",
      " - 2s - loss: 0.1759 - acc: 0.9308 - val_loss: 0.6683 - val_acc: 0.7973\n",
      "Epoch 269/300\n",
      " - 2s - loss: 0.1728 - acc: 0.9334 - val_loss: 0.6686 - val_acc: 0.7996\n",
      "Epoch 270/300\n",
      " - 2s - loss: 0.1757 - acc: 0.9316 - val_loss: 0.6702 - val_acc: 0.8002\n",
      "Epoch 271/300\n",
      " - 2s - loss: 0.1725 - acc: 0.9334 - val_loss: 0.6682 - val_acc: 0.7977\n",
      "Epoch 272/300\n",
      " - 2s - loss: 0.1734 - acc: 0.9331 - val_loss: 0.6684 - val_acc: 0.7980\n",
      "Epoch 273/300\n",
      " - 2s - loss: 0.1778 - acc: 0.9320 - val_loss: 0.6696 - val_acc: 0.7978\n",
      "Epoch 274/300\n",
      " - 2s - loss: 0.1726 - acc: 0.9333 - val_loss: 0.6706 - val_acc: 0.7994\n",
      "Epoch 275/300\n",
      " - 2s - loss: 0.1731 - acc: 0.9353 - val_loss: 0.6678 - val_acc: 0.7983\n",
      "Epoch 276/300\n",
      " - 2s - loss: 0.1729 - acc: 0.9334 - val_loss: 0.6728 - val_acc: 0.7985\n",
      "Epoch 277/300\n",
      " - 2s - loss: 0.1722 - acc: 0.9355 - val_loss: 0.6733 - val_acc: 0.7999\n",
      "Epoch 278/300\n",
      " - 2s - loss: 0.1721 - acc: 0.9340 - val_loss: 0.6732 - val_acc: 0.7986\n",
      "Epoch 279/300\n",
      " - 2s - loss: 0.1716 - acc: 0.9347 - val_loss: 0.6716 - val_acc: 0.7986\n",
      "Epoch 280/300\n",
      " - 2s - loss: 0.1732 - acc: 0.9334 - val_loss: 0.6719 - val_acc: 0.7986\n",
      "Epoch 281/300\n",
      " - 2s - loss: 0.1696 - acc: 0.9348 - val_loss: 0.6753 - val_acc: 0.7990\n",
      "Epoch 282/300\n",
      " - 2s - loss: 0.1711 - acc: 0.9347 - val_loss: 0.6721 - val_acc: 0.7982\n",
      "Epoch 283/300\n",
      " - 2s - loss: 0.1703 - acc: 0.9361 - val_loss: 0.6740 - val_acc: 0.7985\n",
      "Epoch 284/300\n",
      " - 2s - loss: 0.1719 - acc: 0.9354 - val_loss: 0.6754 - val_acc: 0.7995\n",
      "Epoch 285/300\n",
      " - 2s - loss: 0.1667 - acc: 0.9363 - val_loss: 0.6764 - val_acc: 0.7990\n",
      "Epoch 286/300\n",
      " - 2s - loss: 0.1660 - acc: 0.9372 - val_loss: 0.6762 - val_acc: 0.7992\n",
      "Epoch 287/300\n",
      " - 2s - loss: 0.1671 - acc: 0.9363 - val_loss: 0.6766 - val_acc: 0.7977\n",
      "Epoch 288/300\n",
      " - 2s - loss: 0.1687 - acc: 0.9374 - val_loss: 0.6763 - val_acc: 0.7973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/300\n",
      " - 2s - loss: 0.1640 - acc: 0.9367 - val_loss: 0.6767 - val_acc: 0.7992\n",
      "Epoch 290/300\n",
      " - 2s - loss: 0.1672 - acc: 0.9358 - val_loss: 0.6775 - val_acc: 0.7977\n",
      "Epoch 291/300\n",
      " - 2s - loss: 0.1697 - acc: 0.9366 - val_loss: 0.6763 - val_acc: 0.7974\n",
      "Epoch 292/300\n",
      " - 2s - loss: 0.1632 - acc: 0.9385 - val_loss: 0.6787 - val_acc: 0.7997\n",
      "Epoch 293/300\n",
      " - 2s - loss: 0.1669 - acc: 0.9365 - val_loss: 0.6804 - val_acc: 0.7974\n",
      "Epoch 294/300\n",
      " - 2s - loss: 0.1672 - acc: 0.9347 - val_loss: 0.6805 - val_acc: 0.7989\n",
      "Epoch 295/300\n",
      " - 2s - loss: 0.1713 - acc: 0.9328 - val_loss: 0.6774 - val_acc: 0.7989\n",
      "Epoch 296/300\n",
      " - 2s - loss: 0.1661 - acc: 0.9365 - val_loss: 0.6807 - val_acc: 0.7997\n",
      "Epoch 297/300\n",
      " - 2s - loss: 0.1689 - acc: 0.9348 - val_loss: 0.6797 - val_acc: 0.7986\n",
      "Epoch 298/300\n",
      " - 2s - loss: 0.1626 - acc: 0.9386 - val_loss: 0.6827 - val_acc: 0.7966\n",
      "Epoch 299/300\n",
      " - 2s - loss: 0.1642 - acc: 0.9381 - val_loss: 0.6822 - val_acc: 0.7978\n",
      "Epoch 300/300\n",
      " - 2s - loss: 0.1667 - acc: 0.9365 - val_loss: 0.6809 - val_acc: 0.7971\n"
     ]
    }
   ],
   "source": [
    "model5 = getModel(increasedDropout, neurons1=increasedNeurons1, learningRate=0.008)\n",
    "net5 = model5.fit(X, y, epochs=300, batch_size=512, verbose=2, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519600273558755\n"
     ]
    }
   ],
   "source": [
    "#checking the best validation loss after decreasing the learning rate\n",
    "valid_loss5 = min(net5.history[\"val_loss\"])\n",
    "print(valid_loss5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEMCAYAAABtKgnyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation loss 0.519600273558755\n"
     ]
    }
   ],
   "source": [
    "valid_loss5 = net5.history[\"val_loss\"]\n",
    "plt.plot(valid_loss1, linewidth=3, label=\"The baseline\")\n",
    "#plt.plot(valid_loss3, linewidth=3, label=\"Increased dropout & epoch\")\n",
    "plt.plot(valid_loss4, linewidth=3, label=\"Increased neurons\")\n",
    "plt.plot(valid_loss5, linewidth=3, label=\"Increased neurons & decr. learning rate\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"validation loss\")\n",
    "plt.xlim(0, 40)\n",
    "plt.ylim(0.47, 0.8)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "print('best validation loss', min(valid_loss5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low learning rate smoothed down validation loss curve and improved prediction quality. \n",
    "Now there is a good balance between number of neurons, dropout and learning rate. <br>\n",
    "Note that other parameters could have been tuned too: number of layers, activation function etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried increasing the number of layers and checking if there was any improvement validation loss. The results are:\n",
    "+ With 5 layers and learning rate of 0.004, with 500 epochs the validation loss was 0.5421819078270516\n",
    "+ With 4 layers and learning rate of 0.004, with 500 epochs the validation loss was 0.5330044831748673\n",
    "+ With 3 layers and learning rate of 0.004, with 500 epochs the validation loss was 0.5290813801692954\n",
    "+ neuron1 =500, 0.5284347782959894"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#incrreasing the number of layers in the nn\n",
    "def getModel2(dropout=0.4, neurons1=500, neurons2=250,neurons3=250,\n",
    "             learningRate=0.004):\n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu', input_dim=num_features,\n",
    "                    name='Dense_1'))\n",
    "    model.add(Dropout(dropout,name='Dropout_1'))\n",
    "    model.add(Dense(neurons2, activation='relu',name='Dense_2'))\n",
    "    model.add(Dropout(dropout,name='Dropout_2'))\n",
    "    model.add(Dense(neurons3, activation='relu', name='Dense_3'))\n",
    "    model.add(Dropout(dropout, name='Dropout_3'))\n",
    "    model.add(Dense(num_classes, activation='softmax',name='Output'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adagrad(lr=learningRate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model2 = getModel2()\n",
    "\n",
    "#SVG(model_to_dot(model2).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/500\n",
      " - 2s - loss: 0.9818 - acc: 0.6551 - val_loss: 0.6923 - val_acc: 0.7409\n",
      "Epoch 2/500\n",
      " - 1s - loss: 0.7619 - acc: 0.7193 - val_loss: 0.6448 - val_acc: 0.7552\n",
      "Epoch 3/500\n",
      " - 1s - loss: 0.7214 - acc: 0.7307 - val_loss: 0.6266 - val_acc: 0.7628\n",
      "Epoch 4/500\n",
      " - 1s - loss: 0.6993 - acc: 0.7360 - val_loss: 0.6192 - val_acc: 0.7646\n",
      "Epoch 5/500\n",
      " - 1s - loss: 0.6784 - acc: 0.7449 - val_loss: 0.6114 - val_acc: 0.7657\n",
      "Epoch 6/500\n",
      " - 1s - loss: 0.6670 - acc: 0.7466 - val_loss: 0.6033 - val_acc: 0.7683\n",
      "Epoch 7/500\n",
      " - 1s - loss: 0.6596 - acc: 0.7505 - val_loss: 0.5975 - val_acc: 0.7703\n",
      "Epoch 8/500\n",
      " - 1s - loss: 0.6474 - acc: 0.7536 - val_loss: 0.5895 - val_acc: 0.7746\n",
      "Epoch 9/500\n",
      " - 1s - loss: 0.6429 - acc: 0.7558 - val_loss: 0.5868 - val_acc: 0.7759\n",
      "Epoch 10/500\n",
      " - 1s - loss: 0.6343 - acc: 0.7606 - val_loss: 0.5853 - val_acc: 0.7729\n",
      "Epoch 11/500\n",
      " - 1s - loss: 0.6302 - acc: 0.7602 - val_loss: 0.5818 - val_acc: 0.7768\n",
      "Epoch 12/500\n",
      " - 1s - loss: 0.6241 - acc: 0.7610 - val_loss: 0.5785 - val_acc: 0.7772\n",
      "Epoch 13/500\n",
      " - 1s - loss: 0.6210 - acc: 0.7638 - val_loss: 0.5757 - val_acc: 0.7800\n",
      "Epoch 14/500\n",
      " - 1s - loss: 0.6160 - acc: 0.7650 - val_loss: 0.5759 - val_acc: 0.7763\n",
      "Epoch 15/500\n",
      " - 1s - loss: 0.6093 - acc: 0.7681 - val_loss: 0.5713 - val_acc: 0.7807\n",
      "Epoch 16/500\n",
      " - 1s - loss: 0.6041 - acc: 0.7693 - val_loss: 0.5693 - val_acc: 0.7848\n",
      "Epoch 17/500\n",
      " - 1s - loss: 0.6014 - acc: 0.7697 - val_loss: 0.5694 - val_acc: 0.7796\n",
      "Epoch 18/500\n",
      " - 1s - loss: 0.5979 - acc: 0.7705 - val_loss: 0.5660 - val_acc: 0.7814\n",
      "Epoch 19/500\n",
      " - 1s - loss: 0.5942 - acc: 0.7720 - val_loss: 0.5646 - val_acc: 0.7836\n",
      "Epoch 20/500\n",
      " - 1s - loss: 0.5912 - acc: 0.7721 - val_loss: 0.5627 - val_acc: 0.7836\n",
      "Epoch 21/500\n",
      " - 1s - loss: 0.5875 - acc: 0.7750 - val_loss: 0.5617 - val_acc: 0.7851\n",
      "Epoch 22/500\n",
      " - 1s - loss: 0.5848 - acc: 0.7756 - val_loss: 0.5605 - val_acc: 0.7845\n",
      "Epoch 23/500\n",
      " - 1s - loss: 0.5853 - acc: 0.7748 - val_loss: 0.5598 - val_acc: 0.7842\n",
      "Epoch 24/500\n",
      " - 1s - loss: 0.5802 - acc: 0.7787 - val_loss: 0.5582 - val_acc: 0.7849\n",
      "Epoch 25/500\n",
      " - 1s - loss: 0.5806 - acc: 0.7776 - val_loss: 0.5585 - val_acc: 0.7848\n",
      "Epoch 26/500\n",
      " - 1s - loss: 0.5753 - acc: 0.7782 - val_loss: 0.5557 - val_acc: 0.7868\n",
      "Epoch 27/500\n",
      " - 1s - loss: 0.5722 - acc: 0.7804 - val_loss: 0.5555 - val_acc: 0.7872\n",
      "Epoch 28/500\n",
      " - 1s - loss: 0.5719 - acc: 0.7798 - val_loss: 0.5542 - val_acc: 0.7871\n",
      "Epoch 29/500\n",
      " - 1s - loss: 0.5664 - acc: 0.7825 - val_loss: 0.5530 - val_acc: 0.7885\n",
      "Epoch 30/500\n",
      " - 1s - loss: 0.5667 - acc: 0.7804 - val_loss: 0.5524 - val_acc: 0.7877\n",
      "Epoch 31/500\n",
      " - 1s - loss: 0.5652 - acc: 0.7816 - val_loss: 0.5523 - val_acc: 0.7877\n",
      "Epoch 32/500\n",
      " - 1s - loss: 0.5597 - acc: 0.7844 - val_loss: 0.5518 - val_acc: 0.7859\n",
      "Epoch 33/500\n",
      " - 1s - loss: 0.5594 - acc: 0.7852 - val_loss: 0.5501 - val_acc: 0.7904\n",
      "Epoch 34/500\n",
      " - 1s - loss: 0.5601 - acc: 0.7833 - val_loss: 0.5495 - val_acc: 0.7896\n",
      "Epoch 35/500\n",
      " - 1s - loss: 0.5556 - acc: 0.7859 - val_loss: 0.5490 - val_acc: 0.7886\n",
      "Epoch 36/500\n",
      " - 1s - loss: 0.5551 - acc: 0.7836 - val_loss: 0.5487 - val_acc: 0.7887\n",
      "Epoch 37/500\n",
      " - 1s - loss: 0.5521 - acc: 0.7873 - val_loss: 0.5484 - val_acc: 0.7866\n",
      "Epoch 38/500\n",
      " - 1s - loss: 0.5517 - acc: 0.7858 - val_loss: 0.5459 - val_acc: 0.7911\n",
      "Epoch 39/500\n",
      " - 2s - loss: 0.5506 - acc: 0.7866 - val_loss: 0.5468 - val_acc: 0.7891\n",
      "Epoch 40/500\n",
      " - 1s - loss: 0.5508 - acc: 0.7868 - val_loss: 0.5458 - val_acc: 0.7897\n",
      "Epoch 41/500\n",
      " - 1s - loss: 0.5470 - acc: 0.7881 - val_loss: 0.5462 - val_acc: 0.7880\n",
      "Epoch 42/500\n",
      " - 1s - loss: 0.5431 - acc: 0.7883 - val_loss: 0.5453 - val_acc: 0.7893\n",
      "Epoch 43/500\n",
      " - 1s - loss: 0.5430 - acc: 0.7881 - val_loss: 0.5434 - val_acc: 0.7926\n",
      "Epoch 44/500\n",
      " - 1s - loss: 0.5374 - acc: 0.7907 - val_loss: 0.5438 - val_acc: 0.7893\n",
      "Epoch 45/500\n",
      " - 1s - loss: 0.5387 - acc: 0.7901 - val_loss: 0.5435 - val_acc: 0.7906\n",
      "Epoch 46/500\n",
      " - 1s - loss: 0.5385 - acc: 0.7907 - val_loss: 0.5417 - val_acc: 0.7929\n",
      "Epoch 47/500\n",
      " - 1s - loss: 0.5363 - acc: 0.7928 - val_loss: 0.5437 - val_acc: 0.7894\n",
      "Epoch 48/500\n",
      " - 1s - loss: 0.5375 - acc: 0.7908 - val_loss: 0.5434 - val_acc: 0.7881\n",
      "Epoch 49/500\n",
      " - 1s - loss: 0.5356 - acc: 0.7920 - val_loss: 0.5415 - val_acc: 0.7927\n",
      "Epoch 50/500\n",
      " - 2s - loss: 0.5303 - acc: 0.7928 - val_loss: 0.5417 - val_acc: 0.7898\n",
      "Epoch 51/500\n",
      " - 2s - loss: 0.5304 - acc: 0.7934 - val_loss: 0.5409 - val_acc: 0.7925\n",
      "Epoch 52/500\n",
      " - 1s - loss: 0.5293 - acc: 0.7956 - val_loss: 0.5408 - val_acc: 0.7929\n",
      "Epoch 53/500\n",
      " - 2s - loss: 0.5286 - acc: 0.7928 - val_loss: 0.5406 - val_acc: 0.7919\n",
      "Epoch 54/500\n",
      " - 2s - loss: 0.5277 - acc: 0.7936 - val_loss: 0.5403 - val_acc: 0.7917\n",
      "Epoch 55/500\n",
      " - 2s - loss: 0.5248 - acc: 0.7977 - val_loss: 0.5390 - val_acc: 0.7936\n",
      "Epoch 56/500\n",
      " - 1s - loss: 0.5244 - acc: 0.7937 - val_loss: 0.5390 - val_acc: 0.7925\n",
      "Epoch 57/500\n",
      " - 1s - loss: 0.5221 - acc: 0.7949 - val_loss: 0.5386 - val_acc: 0.7932\n",
      "Epoch 58/500\n",
      " - 2s - loss: 0.5220 - acc: 0.7952 - val_loss: 0.5392 - val_acc: 0.7921\n",
      "Epoch 59/500\n",
      " - 2s - loss: 0.5226 - acc: 0.7980 - val_loss: 0.5386 - val_acc: 0.7926\n",
      "Epoch 60/500\n",
      " - 1s - loss: 0.5207 - acc: 0.7962 - val_loss: 0.5376 - val_acc: 0.7923\n",
      "Epoch 61/500\n",
      " - 2s - loss: 0.5198 - acc: 0.7967 - val_loss: 0.5384 - val_acc: 0.7917\n",
      "Epoch 62/500\n",
      " - 2s - loss: 0.5174 - acc: 0.7989 - val_loss: 0.5370 - val_acc: 0.7938\n",
      "Epoch 63/500\n",
      " - 1s - loss: 0.5165 - acc: 0.7965 - val_loss: 0.5368 - val_acc: 0.7928\n",
      "Epoch 64/500\n",
      " - 1s - loss: 0.5201 - acc: 0.7982 - val_loss: 0.5363 - val_acc: 0.7933\n",
      "Epoch 65/500\n",
      " - 2s - loss: 0.5124 - acc: 0.7992 - val_loss: 0.5363 - val_acc: 0.7931\n",
      "Epoch 66/500\n",
      " - 1s - loss: 0.5153 - acc: 0.7990 - val_loss: 0.5367 - val_acc: 0.7934\n",
      "Epoch 67/500\n",
      " - 1s - loss: 0.5129 - acc: 0.8008 - val_loss: 0.5369 - val_acc: 0.7925\n",
      "Epoch 68/500\n",
      " - 1s - loss: 0.5112 - acc: 0.8022 - val_loss: 0.5363 - val_acc: 0.7942\n",
      "Epoch 69/500\n",
      " - 1s - loss: 0.5091 - acc: 0.7998 - val_loss: 0.5361 - val_acc: 0.7943\n",
      "Epoch 70/500\n",
      " - 1s - loss: 0.5056 - acc: 0.8025 - val_loss: 0.5359 - val_acc: 0.7937\n",
      "Epoch 71/500\n",
      " - 1s - loss: 0.5070 - acc: 0.8016 - val_loss: 0.5348 - val_acc: 0.7938\n",
      "Epoch 72/500\n",
      " - 1s - loss: 0.5071 - acc: 0.8010 - val_loss: 0.5352 - val_acc: 0.7940\n",
      "Epoch 73/500\n",
      " - 1s - loss: 0.5032 - acc: 0.8015 - val_loss: 0.5354 - val_acc: 0.7936\n",
      "Epoch 74/500\n",
      " - 1s - loss: 0.5031 - acc: 0.8010 - val_loss: 0.5343 - val_acc: 0.7921\n",
      "Epoch 75/500\n",
      " - 1s - loss: 0.5034 - acc: 0.8030 - val_loss: 0.5346 - val_acc: 0.7932\n",
      "Epoch 76/500\n",
      " - 1s - loss: 0.5035 - acc: 0.8007 - val_loss: 0.5342 - val_acc: 0.7938\n",
      "Epoch 77/500\n",
      " - 1s - loss: 0.5016 - acc: 0.8017 - val_loss: 0.5356 - val_acc: 0.7932\n",
      "Epoch 78/500\n",
      " - 1s - loss: 0.5034 - acc: 0.8048 - val_loss: 0.5347 - val_acc: 0.7938\n",
      "Epoch 79/500\n",
      " - 1s - loss: 0.4994 - acc: 0.8044 - val_loss: 0.5342 - val_acc: 0.7945\n",
      "Epoch 80/500\n",
      " - 1s - loss: 0.4963 - acc: 0.8043 - val_loss: 0.5339 - val_acc: 0.7932\n",
      "Epoch 81/500\n",
      " - 1s - loss: 0.5000 - acc: 0.8049 - val_loss: 0.5341 - val_acc: 0.7945\n",
      "Epoch 82/500\n",
      " - 1s - loss: 0.4964 - acc: 0.8065 - val_loss: 0.5340 - val_acc: 0.7948\n",
      "Epoch 83/500\n",
      " - 1s - loss: 0.4966 - acc: 0.8049 - val_loss: 0.5342 - val_acc: 0.7939\n",
      "Epoch 84/500\n",
      " - 1s - loss: 0.4925 - acc: 0.8050 - val_loss: 0.5323 - val_acc: 0.7951\n",
      "Epoch 85/500\n",
      " - 1s - loss: 0.4963 - acc: 0.8060 - val_loss: 0.5331 - val_acc: 0.7950\n",
      "Epoch 86/500\n",
      " - 1s - loss: 0.4957 - acc: 0.8067 - val_loss: 0.5330 - val_acc: 0.7952\n",
      "Epoch 87/500\n",
      " - 1s - loss: 0.4943 - acc: 0.8065 - val_loss: 0.5320 - val_acc: 0.7955\n",
      "Epoch 88/500\n",
      " - 1s - loss: 0.4922 - acc: 0.8076 - val_loss: 0.5322 - val_acc: 0.7950\n",
      "Epoch 89/500\n",
      " - 1s - loss: 0.4907 - acc: 0.8081 - val_loss: 0.5320 - val_acc: 0.7949\n",
      "Epoch 90/500\n",
      " - 2s - loss: 0.4921 - acc: 0.8052 - val_loss: 0.5323 - val_acc: 0.7951\n",
      "Epoch 91/500\n",
      " - 1s - loss: 0.4880 - acc: 0.8078 - val_loss: 0.5318 - val_acc: 0.7957\n",
      "Epoch 92/500\n",
      " - 1s - loss: 0.4903 - acc: 0.8065 - val_loss: 0.5319 - val_acc: 0.7957\n",
      "Epoch 93/500\n",
      " - 1s - loss: 0.4855 - acc: 0.8088 - val_loss: 0.5319 - val_acc: 0.7955\n",
      "Epoch 94/500\n",
      " - 1s - loss: 0.4836 - acc: 0.8109 - val_loss: 0.5321 - val_acc: 0.7955\n",
      "Epoch 95/500\n",
      " - 1s - loss: 0.4895 - acc: 0.8071 - val_loss: 0.5312 - val_acc: 0.7965\n",
      "Epoch 96/500\n",
      " - 1s - loss: 0.4859 - acc: 0.8098 - val_loss: 0.5311 - val_acc: 0.7952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/500\n",
      " - 1s - loss: 0.4851 - acc: 0.8096 - val_loss: 0.5306 - val_acc: 0.7957\n",
      "Epoch 98/500\n",
      " - 1s - loss: 0.4838 - acc: 0.8092 - val_loss: 0.5314 - val_acc: 0.7956\n",
      "Epoch 99/500\n",
      " - 1s - loss: 0.4804 - acc: 0.8091 - val_loss: 0.5311 - val_acc: 0.7956\n",
      "Epoch 100/500\n",
      " - 1s - loss: 0.4829 - acc: 0.8083 - val_loss: 0.5316 - val_acc: 0.7956\n",
      "Epoch 101/500\n",
      " - 1s - loss: 0.4808 - acc: 0.8111 - val_loss: 0.5323 - val_acc: 0.7951\n",
      "Epoch 102/500\n",
      " - 2s - loss: 0.4844 - acc: 0.8088 - val_loss: 0.5308 - val_acc: 0.7954\n",
      "Epoch 103/500\n",
      " - 2s - loss: 0.4816 - acc: 0.8101 - val_loss: 0.5307 - val_acc: 0.7948\n",
      "Epoch 104/500\n",
      " - 1s - loss: 0.4779 - acc: 0.8110 - val_loss: 0.5309 - val_acc: 0.7952\n",
      "Epoch 105/500\n",
      " - 1s - loss: 0.4776 - acc: 0.8107 - val_loss: 0.5306 - val_acc: 0.7961\n",
      "Epoch 106/500\n",
      " - 1s - loss: 0.4774 - acc: 0.8084 - val_loss: 0.5307 - val_acc: 0.7956\n",
      "Epoch 107/500\n",
      " - 1s - loss: 0.4757 - acc: 0.8112 - val_loss: 0.5303 - val_acc: 0.7962\n",
      "Epoch 108/500\n",
      " - 1s - loss: 0.4777 - acc: 0.8119 - val_loss: 0.5302 - val_acc: 0.7966\n",
      "Epoch 109/500\n",
      " - 1s - loss: 0.4738 - acc: 0.8136 - val_loss: 0.5300 - val_acc: 0.7962\n",
      "Epoch 110/500\n",
      " - 1s - loss: 0.4742 - acc: 0.8142 - val_loss: 0.5305 - val_acc: 0.7963\n",
      "Epoch 111/500\n",
      " - 1s - loss: 0.4772 - acc: 0.8115 - val_loss: 0.5306 - val_acc: 0.7965\n",
      "Epoch 112/500\n",
      " - 1s - loss: 0.4701 - acc: 0.8165 - val_loss: 0.5303 - val_acc: 0.7954\n",
      "Epoch 113/500\n",
      " - 1s - loss: 0.4718 - acc: 0.8138 - val_loss: 0.5302 - val_acc: 0.7957\n",
      "Epoch 114/500\n",
      " - 1s - loss: 0.4733 - acc: 0.8140 - val_loss: 0.5301 - val_acc: 0.7960\n",
      "Epoch 115/500\n",
      " - 1s - loss: 0.4709 - acc: 0.8123 - val_loss: 0.5301 - val_acc: 0.7960\n",
      "Epoch 116/500\n",
      " - 1s - loss: 0.4707 - acc: 0.8149 - val_loss: 0.5304 - val_acc: 0.7956\n",
      "Epoch 117/500\n",
      " - 1s - loss: 0.4724 - acc: 0.8134 - val_loss: 0.5299 - val_acc: 0.7960\n",
      "Epoch 118/500\n",
      " - 1s - loss: 0.4685 - acc: 0.8150 - val_loss: 0.5305 - val_acc: 0.7959\n",
      "Epoch 119/500\n",
      " - 1s - loss: 0.4662 - acc: 0.8175 - val_loss: 0.5298 - val_acc: 0.7962\n",
      "Epoch 120/500\n",
      " - 1s - loss: 0.4677 - acc: 0.8145 - val_loss: 0.5297 - val_acc: 0.7956\n",
      "Epoch 121/500\n",
      " - 1s - loss: 0.4643 - acc: 0.8151 - val_loss: 0.5296 - val_acc: 0.7955\n",
      "Epoch 122/500\n",
      " - 1s - loss: 0.4656 - acc: 0.8145 - val_loss: 0.5295 - val_acc: 0.7954\n",
      "Epoch 123/500\n",
      " - 1s - loss: 0.4702 - acc: 0.8159 - val_loss: 0.5287 - val_acc: 0.7960\n",
      "Epoch 124/500\n",
      " - 1s - loss: 0.4663 - acc: 0.8133 - val_loss: 0.5288 - val_acc: 0.7965\n",
      "Epoch 125/500\n",
      " - 1s - loss: 0.4628 - acc: 0.8164 - val_loss: 0.5294 - val_acc: 0.7967\n",
      "Epoch 126/500\n",
      " - 1s - loss: 0.4621 - acc: 0.8160 - val_loss: 0.5293 - val_acc: 0.7950\n",
      "Epoch 127/500\n",
      " - 1s - loss: 0.4604 - acc: 0.8181 - val_loss: 0.5298 - val_acc: 0.7955\n",
      "Epoch 128/500\n",
      " - 1s - loss: 0.4645 - acc: 0.8142 - val_loss: 0.5295 - val_acc: 0.7951\n",
      "Epoch 129/500\n",
      " - 1s - loss: 0.4615 - acc: 0.8160 - val_loss: 0.5287 - val_acc: 0.7959\n",
      "Epoch 130/500\n",
      " - 1s - loss: 0.4626 - acc: 0.8165 - val_loss: 0.5291 - val_acc: 0.7955\n",
      "Epoch 131/500\n",
      " - 1s - loss: 0.4618 - acc: 0.8173 - val_loss: 0.5291 - val_acc: 0.7959\n",
      "Epoch 132/500\n",
      " - 1s - loss: 0.4581 - acc: 0.8157 - val_loss: 0.5299 - val_acc: 0.7955\n",
      "Epoch 133/500\n",
      " - 1s - loss: 0.4581 - acc: 0.8187 - val_loss: 0.5290 - val_acc: 0.7957\n",
      "Epoch 134/500\n",
      " - 1s - loss: 0.4581 - acc: 0.8189 - val_loss: 0.5293 - val_acc: 0.7963\n",
      "Epoch 135/500\n",
      " - 1s - loss: 0.4586 - acc: 0.8169 - val_loss: 0.5294 - val_acc: 0.7963\n",
      "Epoch 136/500\n",
      " - 1s - loss: 0.4585 - acc: 0.8174 - val_loss: 0.5295 - val_acc: 0.7961\n",
      "Epoch 137/500\n",
      " - 1s - loss: 0.4566 - acc: 0.8180 - val_loss: 0.5291 - val_acc: 0.7957\n",
      "Epoch 138/500\n",
      " - 1s - loss: 0.4624 - acc: 0.8170 - val_loss: 0.5287 - val_acc: 0.7959\n",
      "Epoch 139/500\n",
      " - 1s - loss: 0.4554 - acc: 0.8187 - val_loss: 0.5286 - val_acc: 0.7961\n",
      "Epoch 140/500\n",
      " - 1s - loss: 0.4543 - acc: 0.8194 - val_loss: 0.5291 - val_acc: 0.7955\n",
      "Epoch 141/500\n",
      " - 1s - loss: 0.4569 - acc: 0.8163 - val_loss: 0.5286 - val_acc: 0.7965\n",
      "Epoch 142/500\n",
      " - 1s - loss: 0.4558 - acc: 0.8194 - val_loss: 0.5289 - val_acc: 0.7959\n",
      "Epoch 143/500\n",
      " - 1s - loss: 0.4548 - acc: 0.8209 - val_loss: 0.5294 - val_acc: 0.7956\n",
      "Epoch 144/500\n",
      " - 1s - loss: 0.4530 - acc: 0.8203 - val_loss: 0.5299 - val_acc: 0.7966\n",
      "Epoch 145/500\n",
      " - 1s - loss: 0.4511 - acc: 0.8208 - val_loss: 0.5294 - val_acc: 0.7965\n",
      "Epoch 146/500\n",
      " - 1s - loss: 0.4529 - acc: 0.8211 - val_loss: 0.5294 - val_acc: 0.7957\n",
      "Epoch 147/500\n",
      " - 1s - loss: 0.4492 - acc: 0.8212 - val_loss: 0.5291 - val_acc: 0.7957\n",
      "Epoch 148/500\n",
      " - 1s - loss: 0.4486 - acc: 0.8194 - val_loss: 0.5291 - val_acc: 0.7952\n",
      "Epoch 149/500\n",
      " - 1s - loss: 0.4541 - acc: 0.8211 - val_loss: 0.5295 - val_acc: 0.7956\n",
      "Epoch 150/500\n",
      " - 1s - loss: 0.4478 - acc: 0.8234 - val_loss: 0.5295 - val_acc: 0.7956\n",
      "Epoch 151/500\n",
      " - 1s - loss: 0.4488 - acc: 0.8214 - val_loss: 0.5294 - val_acc: 0.7955\n",
      "Epoch 152/500\n",
      " - 1s - loss: 0.4502 - acc: 0.8216 - val_loss: 0.5291 - val_acc: 0.7960\n",
      "Epoch 153/500\n",
      " - 1s - loss: 0.4464 - acc: 0.8219 - val_loss: 0.5291 - val_acc: 0.7951\n",
      "Epoch 154/500\n",
      " - 1s - loss: 0.4460 - acc: 0.8217 - val_loss: 0.5293 - val_acc: 0.7951\n",
      "Epoch 155/500\n",
      " - 1s - loss: 0.4476 - acc: 0.8215 - val_loss: 0.5298 - val_acc: 0.7952\n",
      "Epoch 156/500\n",
      " - 1s - loss: 0.4464 - acc: 0.8215 - val_loss: 0.5298 - val_acc: 0.7952\n",
      "Epoch 157/500\n",
      " - 1s - loss: 0.4430 - acc: 0.8239 - val_loss: 0.5301 - val_acc: 0.7950\n",
      "Epoch 158/500\n",
      " - 1s - loss: 0.4479 - acc: 0.8209 - val_loss: 0.5299 - val_acc: 0.7956\n",
      "Epoch 159/500\n",
      " - 1s - loss: 0.4430 - acc: 0.8234 - val_loss: 0.5299 - val_acc: 0.7957\n",
      "Epoch 160/500\n",
      " - 1s - loss: 0.4429 - acc: 0.8210 - val_loss: 0.5301 - val_acc: 0.7960\n",
      "Epoch 161/500\n",
      " - 1s - loss: 0.4442 - acc: 0.8239 - val_loss: 0.5301 - val_acc: 0.7960\n",
      "Epoch 162/500\n",
      " - 1s - loss: 0.4412 - acc: 0.8225 - val_loss: 0.5297 - val_acc: 0.7957\n",
      "Epoch 163/500\n",
      " - 1s - loss: 0.4422 - acc: 0.8222 - val_loss: 0.5296 - val_acc: 0.7951\n",
      "Epoch 164/500\n",
      " - 1s - loss: 0.4420 - acc: 0.8247 - val_loss: 0.5295 - val_acc: 0.7961\n",
      "Epoch 165/500\n",
      " - 1s - loss: 0.4393 - acc: 0.8249 - val_loss: 0.5298 - val_acc: 0.7948\n",
      "Epoch 166/500\n",
      " - 1s - loss: 0.4384 - acc: 0.8248 - val_loss: 0.5308 - val_acc: 0.7966\n",
      "Epoch 167/500\n",
      " - 1s - loss: 0.4406 - acc: 0.8255 - val_loss: 0.5298 - val_acc: 0.7969\n",
      "Epoch 168/500\n",
      " - 1s - loss: 0.4394 - acc: 0.8263 - val_loss: 0.5298 - val_acc: 0.7966\n",
      "Epoch 169/500\n",
      " - 1s - loss: 0.4394 - acc: 0.8254 - val_loss: 0.5297 - val_acc: 0.7961\n",
      "Epoch 170/500\n",
      " - 1s - loss: 0.4392 - acc: 0.8249 - val_loss: 0.5298 - val_acc: 0.7963\n",
      "Epoch 171/500\n",
      " - 1s - loss: 0.4392 - acc: 0.8248 - val_loss: 0.5294 - val_acc: 0.7955\n",
      "Epoch 172/500\n",
      " - 1s - loss: 0.4358 - acc: 0.8244 - val_loss: 0.5300 - val_acc: 0.7966\n",
      "Epoch 173/500\n",
      " - 1s - loss: 0.4369 - acc: 0.8252 - val_loss: 0.5295 - val_acc: 0.7960\n",
      "Epoch 174/500\n",
      " - 1s - loss: 0.4374 - acc: 0.8237 - val_loss: 0.5298 - val_acc: 0.7948\n",
      "Epoch 175/500\n",
      " - 1s - loss: 0.4385 - acc: 0.8241 - val_loss: 0.5300 - val_acc: 0.7950\n",
      "Epoch 176/500\n",
      " - 1s - loss: 0.4359 - acc: 0.8243 - val_loss: 0.5301 - val_acc: 0.7956\n",
      "Epoch 177/500\n",
      " - 1s - loss: 0.4374 - acc: 0.8256 - val_loss: 0.5293 - val_acc: 0.7957\n",
      "Epoch 178/500\n",
      " - 1s - loss: 0.4359 - acc: 0.8252 - val_loss: 0.5298 - val_acc: 0.7961\n",
      "Epoch 179/500\n",
      " - 1s - loss: 0.4352 - acc: 0.8257 - val_loss: 0.5299 - val_acc: 0.7967\n",
      "Epoch 180/500\n",
      " - 1s - loss: 0.4346 - acc: 0.8265 - val_loss: 0.5299 - val_acc: 0.7959\n",
      "Epoch 181/500\n",
      " - 1s - loss: 0.4369 - acc: 0.8261 - val_loss: 0.5295 - val_acc: 0.7965\n",
      "Epoch 182/500\n",
      " - 1s - loss: 0.4331 - acc: 0.8284 - val_loss: 0.5293 - val_acc: 0.7966\n",
      "Epoch 183/500\n",
      " - 1s - loss: 0.4347 - acc: 0.8239 - val_loss: 0.5296 - val_acc: 0.7954\n",
      "Epoch 184/500\n",
      " - 1s - loss: 0.4328 - acc: 0.8260 - val_loss: 0.5297 - val_acc: 0.7963\n",
      "Epoch 185/500\n",
      " - 1s - loss: 0.4332 - acc: 0.8275 - val_loss: 0.5294 - val_acc: 0.7963\n",
      "Epoch 186/500\n",
      " - 1s - loss: 0.4318 - acc: 0.8269 - val_loss: 0.5299 - val_acc: 0.7962\n",
      "Epoch 187/500\n",
      " - 1s - loss: 0.4302 - acc: 0.8296 - val_loss: 0.5301 - val_acc: 0.7961\n",
      "Epoch 188/500\n",
      " - 1s - loss: 0.4305 - acc: 0.8276 - val_loss: 0.5301 - val_acc: 0.7952\n",
      "Epoch 189/500\n",
      " - 1s - loss: 0.4303 - acc: 0.8282 - val_loss: 0.5299 - val_acc: 0.7963\n",
      "Epoch 190/500\n",
      " - 1s - loss: 0.4314 - acc: 0.8271 - val_loss: 0.5303 - val_acc: 0.7967\n",
      "Epoch 191/500\n",
      " - 1s - loss: 0.4279 - acc: 0.8305 - val_loss: 0.5299 - val_acc: 0.7963\n",
      "Epoch 192/500\n",
      " - 1s - loss: 0.4303 - acc: 0.8279 - val_loss: 0.5298 - val_acc: 0.7966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/500\n",
      " - 1s - loss: 0.4263 - acc: 0.8283 - val_loss: 0.5304 - val_acc: 0.7959\n",
      "Epoch 194/500\n",
      " - 1s - loss: 0.4275 - acc: 0.8303 - val_loss: 0.5304 - val_acc: 0.7973\n",
      "Epoch 195/500\n",
      " - 1s - loss: 0.4293 - acc: 0.8271 - val_loss: 0.5306 - val_acc: 0.7968\n",
      "Epoch 196/500\n",
      " - 1s - loss: 0.4289 - acc: 0.8268 - val_loss: 0.5304 - val_acc: 0.7961\n",
      "Epoch 197/500\n",
      " - 1s - loss: 0.4235 - acc: 0.8291 - val_loss: 0.5314 - val_acc: 0.7961\n",
      "Epoch 198/500\n",
      " - 1s - loss: 0.4259 - acc: 0.8293 - val_loss: 0.5308 - val_acc: 0.7968\n",
      "Epoch 199/500\n",
      " - 1s - loss: 0.4269 - acc: 0.8256 - val_loss: 0.5313 - val_acc: 0.7961\n",
      "Epoch 200/500\n",
      " - 1s - loss: 0.4245 - acc: 0.8302 - val_loss: 0.5310 - val_acc: 0.7976\n",
      "Epoch 201/500\n",
      " - 1s - loss: 0.4261 - acc: 0.8289 - val_loss: 0.5312 - val_acc: 0.7966\n",
      "Epoch 202/500\n",
      " - 1s - loss: 0.4266 - acc: 0.8283 - val_loss: 0.5313 - val_acc: 0.7969\n",
      "Epoch 203/500\n",
      " - 1s - loss: 0.4268 - acc: 0.8280 - val_loss: 0.5312 - val_acc: 0.7972\n",
      "Epoch 204/500\n",
      " - 1s - loss: 0.4233 - acc: 0.8313 - val_loss: 0.5312 - val_acc: 0.7984\n",
      "Epoch 205/500\n",
      " - 1s - loss: 0.4241 - acc: 0.8323 - val_loss: 0.5306 - val_acc: 0.7980\n",
      "Epoch 206/500\n",
      " - 1s - loss: 0.4216 - acc: 0.8322 - val_loss: 0.5312 - val_acc: 0.7978\n",
      "Epoch 207/500\n",
      " - 2s - loss: 0.4221 - acc: 0.8309 - val_loss: 0.5318 - val_acc: 0.7977\n",
      "Epoch 208/500\n",
      " - 1s - loss: 0.4173 - acc: 0.8313 - val_loss: 0.5313 - val_acc: 0.7977\n",
      "Epoch 209/500\n",
      " - 1s - loss: 0.4205 - acc: 0.8301 - val_loss: 0.5320 - val_acc: 0.7980\n",
      "Epoch 210/500\n",
      " - 1s - loss: 0.4219 - acc: 0.8304 - val_loss: 0.5319 - val_acc: 0.7976\n",
      "Epoch 211/500\n",
      " - 1s - loss: 0.4183 - acc: 0.8313 - val_loss: 0.5317 - val_acc: 0.7978\n",
      "Epoch 212/500\n",
      " - 1s - loss: 0.4192 - acc: 0.8323 - val_loss: 0.5323 - val_acc: 0.7978\n",
      "Epoch 213/500\n",
      " - 1s - loss: 0.4187 - acc: 0.8314 - val_loss: 0.5320 - val_acc: 0.7977\n",
      "Epoch 214/500\n",
      " - 1s - loss: 0.4189 - acc: 0.8326 - val_loss: 0.5318 - val_acc: 0.7971\n",
      "Epoch 215/500\n",
      " - 1s - loss: 0.4167 - acc: 0.8332 - val_loss: 0.5318 - val_acc: 0.7979\n",
      "Epoch 216/500\n",
      " - 1s - loss: 0.4191 - acc: 0.8303 - val_loss: 0.5325 - val_acc: 0.7980\n",
      "Epoch 217/500\n",
      " - 1s - loss: 0.4168 - acc: 0.8334 - val_loss: 0.5330 - val_acc: 0.7977\n",
      "Epoch 218/500\n",
      " - 1s - loss: 0.4189 - acc: 0.8308 - val_loss: 0.5326 - val_acc: 0.7983\n",
      "Epoch 219/500\n",
      " - 1s - loss: 0.4174 - acc: 0.8335 - val_loss: 0.5323 - val_acc: 0.7988\n",
      "Epoch 220/500\n",
      " - 1s - loss: 0.4178 - acc: 0.8336 - val_loss: 0.5325 - val_acc: 0.7983\n",
      "Epoch 221/500\n",
      " - 1s - loss: 0.4138 - acc: 0.8337 - val_loss: 0.5327 - val_acc: 0.7985\n",
      "Epoch 222/500\n",
      " - 1s - loss: 0.4164 - acc: 0.8316 - val_loss: 0.5328 - val_acc: 0.7971\n",
      "Epoch 223/500\n",
      " - 1s - loss: 0.4189 - acc: 0.8341 - val_loss: 0.5321 - val_acc: 0.7985\n",
      "Epoch 224/500\n",
      " - 1s - loss: 0.4164 - acc: 0.8325 - val_loss: 0.5323 - val_acc: 0.7978\n",
      "Epoch 225/500\n",
      " - 1s - loss: 0.4148 - acc: 0.8327 - val_loss: 0.5320 - val_acc: 0.7979\n",
      "Epoch 226/500\n",
      " - 1s - loss: 0.4156 - acc: 0.8359 - val_loss: 0.5324 - val_acc: 0.7982\n",
      "Epoch 227/500\n",
      " - 1s - loss: 0.4184 - acc: 0.8327 - val_loss: 0.5323 - val_acc: 0.7978\n",
      "Epoch 228/500\n",
      " - 1s - loss: 0.4162 - acc: 0.8333 - val_loss: 0.5319 - val_acc: 0.7983\n",
      "Epoch 229/500\n",
      " - 1s - loss: 0.4128 - acc: 0.8322 - val_loss: 0.5324 - val_acc: 0.7986\n",
      "Epoch 230/500\n",
      " - 1s - loss: 0.4109 - acc: 0.8348 - val_loss: 0.5329 - val_acc: 0.7988\n",
      "Epoch 231/500\n",
      " - 1s - loss: 0.4149 - acc: 0.8339 - val_loss: 0.5326 - val_acc: 0.7976\n",
      "Epoch 232/500\n",
      " - 1s - loss: 0.4143 - acc: 0.8336 - val_loss: 0.5330 - val_acc: 0.7979\n",
      "Epoch 233/500\n",
      " - 1s - loss: 0.4127 - acc: 0.8322 - val_loss: 0.5319 - val_acc: 0.7984\n",
      "Epoch 234/500\n",
      " - 1s - loss: 0.4154 - acc: 0.8349 - val_loss: 0.5316 - val_acc: 0.7989\n",
      "Epoch 235/500\n",
      " - 1s - loss: 0.4113 - acc: 0.8337 - val_loss: 0.5320 - val_acc: 0.7989\n",
      "Epoch 236/500\n",
      " - 1s - loss: 0.4107 - acc: 0.8351 - val_loss: 0.5327 - val_acc: 0.7990\n",
      "Epoch 237/500\n",
      " - 1s - loss: 0.4107 - acc: 0.8342 - val_loss: 0.5328 - val_acc: 0.7988\n",
      "Epoch 238/500\n",
      " - 1s - loss: 0.4111 - acc: 0.8350 - val_loss: 0.5326 - val_acc: 0.7989\n",
      "Epoch 239/500\n",
      " - 1s - loss: 0.4098 - acc: 0.8346 - val_loss: 0.5331 - val_acc: 0.7986\n",
      "Epoch 240/500\n",
      " - 1s - loss: 0.4107 - acc: 0.8352 - val_loss: 0.5334 - val_acc: 0.7984\n",
      "Epoch 241/500\n",
      " - 1s - loss: 0.4119 - acc: 0.8339 - val_loss: 0.5330 - val_acc: 0.7974\n",
      "Epoch 242/500\n",
      " - 1s - loss: 0.4070 - acc: 0.8379 - val_loss: 0.5337 - val_acc: 0.7983\n",
      "Epoch 243/500\n",
      " - 1s - loss: 0.4075 - acc: 0.8365 - val_loss: 0.5331 - val_acc: 0.7982\n",
      "Epoch 244/500\n",
      " - 1s - loss: 0.4092 - acc: 0.8357 - val_loss: 0.5334 - val_acc: 0.7986\n",
      "Epoch 245/500\n",
      " - 1s - loss: 0.4083 - acc: 0.8374 - val_loss: 0.5331 - val_acc: 0.7984\n",
      "Epoch 246/500\n",
      " - 1s - loss: 0.4090 - acc: 0.8352 - val_loss: 0.5332 - val_acc: 0.7988\n",
      "Epoch 247/500\n",
      " - 1s - loss: 0.4075 - acc: 0.8354 - val_loss: 0.5330 - val_acc: 0.7978\n",
      "Epoch 248/500\n",
      " - 1s - loss: 0.4081 - acc: 0.8356 - val_loss: 0.5334 - val_acc: 0.7985\n",
      "Epoch 249/500\n",
      " - 1s - loss: 0.4076 - acc: 0.8370 - val_loss: 0.5334 - val_acc: 0.7982\n",
      "Epoch 250/500\n",
      " - 1s - loss: 0.4078 - acc: 0.8372 - val_loss: 0.5332 - val_acc: 0.7986\n",
      "Epoch 251/500\n",
      " - 1s - loss: 0.4069 - acc: 0.8363 - val_loss: 0.5333 - val_acc: 0.7991\n",
      "Epoch 252/500\n",
      " - 1s - loss: 0.4066 - acc: 0.8361 - val_loss: 0.5341 - val_acc: 0.7983\n",
      "Epoch 253/500\n",
      " - 1s - loss: 0.4049 - acc: 0.8387 - val_loss: 0.5338 - val_acc: 0.7979\n",
      "Epoch 254/500\n",
      " - 1s - loss: 0.4070 - acc: 0.8351 - val_loss: 0.5333 - val_acc: 0.7973\n",
      "Epoch 255/500\n",
      " - 1s - loss: 0.4038 - acc: 0.8393 - val_loss: 0.5337 - val_acc: 0.7986\n",
      "Epoch 256/500\n",
      " - 1s - loss: 0.4068 - acc: 0.8372 - val_loss: 0.5332 - val_acc: 0.7971\n",
      "Epoch 257/500\n",
      " - 1s - loss: 0.4053 - acc: 0.8358 - val_loss: 0.5339 - val_acc: 0.7984\n",
      "Epoch 258/500\n",
      " - 1s - loss: 0.4054 - acc: 0.8379 - val_loss: 0.5332 - val_acc: 0.7983\n",
      "Epoch 259/500\n",
      " - 1s - loss: 0.4070 - acc: 0.8364 - val_loss: 0.5332 - val_acc: 0.7980\n",
      "Epoch 260/500\n",
      " - 1s - loss: 0.4018 - acc: 0.8389 - val_loss: 0.5342 - val_acc: 0.7984\n",
      "Epoch 261/500\n",
      " - 1s - loss: 0.4055 - acc: 0.8352 - val_loss: 0.5340 - val_acc: 0.7973\n",
      "Epoch 262/500\n",
      " - 1s - loss: 0.4046 - acc: 0.8376 - val_loss: 0.5337 - val_acc: 0.7979\n",
      "Epoch 263/500\n",
      " - 1s - loss: 0.4007 - acc: 0.8375 - val_loss: 0.5337 - val_acc: 0.7983\n",
      "Epoch 264/500\n",
      " - 1s - loss: 0.4034 - acc: 0.8375 - val_loss: 0.5339 - val_acc: 0.7977\n",
      "Epoch 265/500\n",
      " - 1s - loss: 0.4042 - acc: 0.8360 - val_loss: 0.5335 - val_acc: 0.7972\n",
      "Epoch 266/500\n",
      " - 1s - loss: 0.4038 - acc: 0.8374 - val_loss: 0.5336 - val_acc: 0.7982\n",
      "Epoch 267/500\n",
      " - 1s - loss: 0.4018 - acc: 0.8402 - val_loss: 0.5343 - val_acc: 0.7983\n",
      "Epoch 268/500\n",
      " - 1s - loss: 0.4007 - acc: 0.8384 - val_loss: 0.5333 - val_acc: 0.7978\n",
      "Epoch 269/500\n",
      " - 1s - loss: 0.3996 - acc: 0.8381 - val_loss: 0.5345 - val_acc: 0.7980\n",
      "Epoch 270/500\n",
      " - 1s - loss: 0.4026 - acc: 0.8371 - val_loss: 0.5338 - val_acc: 0.7985\n",
      "Epoch 271/500\n",
      " - 1s - loss: 0.3981 - acc: 0.8386 - val_loss: 0.5345 - val_acc: 0.7980\n",
      "Epoch 272/500\n",
      " - 1s - loss: 0.3999 - acc: 0.8402 - val_loss: 0.5346 - val_acc: 0.7989\n",
      "Epoch 273/500\n",
      " - 1s - loss: 0.3992 - acc: 0.8380 - val_loss: 0.5352 - val_acc: 0.7999\n",
      "Epoch 274/500\n",
      " - 1s - loss: 0.3972 - acc: 0.8405 - val_loss: 0.5348 - val_acc: 0.7989\n",
      "Epoch 275/500\n",
      " - 1s - loss: 0.4021 - acc: 0.8372 - val_loss: 0.5349 - val_acc: 0.7994\n",
      "Epoch 276/500\n",
      " - 1s - loss: 0.3972 - acc: 0.8386 - val_loss: 0.5349 - val_acc: 0.7989\n",
      "Epoch 277/500\n",
      " - 1s - loss: 0.3974 - acc: 0.8396 - val_loss: 0.5352 - val_acc: 0.7986\n",
      "Epoch 278/500\n",
      " - 1s - loss: 0.3977 - acc: 0.8413 - val_loss: 0.5350 - val_acc: 0.7996\n",
      "Epoch 279/500\n",
      " - 1s - loss: 0.3977 - acc: 0.8396 - val_loss: 0.5356 - val_acc: 0.7996\n",
      "Epoch 280/500\n",
      " - 1s - loss: 0.3984 - acc: 0.8406 - val_loss: 0.5349 - val_acc: 0.7989\n",
      "Epoch 281/500\n",
      " - 1s - loss: 0.3969 - acc: 0.8408 - val_loss: 0.5348 - val_acc: 0.7979\n",
      "Epoch 282/500\n",
      " - 1s - loss: 0.3961 - acc: 0.8391 - val_loss: 0.5353 - val_acc: 0.7980\n",
      "Epoch 283/500\n",
      " - 1s - loss: 0.3954 - acc: 0.8404 - val_loss: 0.5357 - val_acc: 0.7976\n",
      "Epoch 284/500\n",
      " - 1s - loss: 0.3952 - acc: 0.8398 - val_loss: 0.5358 - val_acc: 0.7980\n",
      "Epoch 285/500\n",
      " - 1s - loss: 0.3959 - acc: 0.8407 - val_loss: 0.5362 - val_acc: 0.7986\n",
      "Epoch 286/500\n",
      " - 1s - loss: 0.3932 - acc: 0.8410 - val_loss: 0.5360 - val_acc: 0.7984\n",
      "Epoch 287/500\n",
      " - 1s - loss: 0.3976 - acc: 0.8401 - val_loss: 0.5358 - val_acc: 0.7989\n",
      "Epoch 288/500\n",
      " - 1s - loss: 0.3946 - acc: 0.8415 - val_loss: 0.5361 - val_acc: 0.7986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/500\n",
      " - 1s - loss: 0.3935 - acc: 0.8396 - val_loss: 0.5359 - val_acc: 0.7982\n",
      "Epoch 290/500\n",
      " - 1s - loss: 0.3965 - acc: 0.8400 - val_loss: 0.5360 - val_acc: 0.7985\n",
      "Epoch 291/500\n",
      " - 1s - loss: 0.3972 - acc: 0.8376 - val_loss: 0.5356 - val_acc: 0.7986\n",
      "Epoch 292/500\n",
      " - 1s - loss: 0.3956 - acc: 0.8394 - val_loss: 0.5361 - val_acc: 0.7979\n",
      "Epoch 293/500\n",
      " - 1s - loss: 0.3948 - acc: 0.8400 - val_loss: 0.5364 - val_acc: 0.7972\n",
      "Epoch 294/500\n",
      " - 1s - loss: 0.3928 - acc: 0.8419 - val_loss: 0.5367 - val_acc: 0.7985\n",
      "Epoch 295/500\n",
      " - 1s - loss: 0.3933 - acc: 0.8401 - val_loss: 0.5370 - val_acc: 0.7976\n",
      "Epoch 296/500\n",
      " - 1s - loss: 0.3904 - acc: 0.8431 - val_loss: 0.5366 - val_acc: 0.7984\n",
      "Epoch 297/500\n",
      " - 1s - loss: 0.3930 - acc: 0.8402 - val_loss: 0.5365 - val_acc: 0.7986\n",
      "Epoch 298/500\n",
      " - 1s - loss: 0.3924 - acc: 0.8414 - val_loss: 0.5369 - val_acc: 0.7984\n",
      "Epoch 299/500\n",
      " - 1s - loss: 0.3956 - acc: 0.8401 - val_loss: 0.5367 - val_acc: 0.7991\n",
      "Epoch 300/500\n",
      " - 1s - loss: 0.3891 - acc: 0.8421 - val_loss: 0.5366 - val_acc: 0.7986\n",
      "Epoch 301/500\n",
      " - 1s - loss: 0.3918 - acc: 0.8430 - val_loss: 0.5368 - val_acc: 0.7994\n",
      "Epoch 302/500\n",
      " - 1s - loss: 0.3895 - acc: 0.8411 - val_loss: 0.5376 - val_acc: 0.7992\n",
      "Epoch 303/500\n",
      " - 1s - loss: 0.3903 - acc: 0.8406 - val_loss: 0.5376 - val_acc: 0.7985\n",
      "Epoch 304/500\n",
      " - 1s - loss: 0.3883 - acc: 0.8415 - val_loss: 0.5372 - val_acc: 0.7990\n",
      "Epoch 305/500\n",
      " - 1s - loss: 0.3865 - acc: 0.8445 - val_loss: 0.5379 - val_acc: 0.7985\n",
      "Epoch 306/500\n",
      " - 1s - loss: 0.3881 - acc: 0.8444 - val_loss: 0.5376 - val_acc: 0.7982\n",
      "Epoch 307/500\n",
      " - 1s - loss: 0.3880 - acc: 0.8429 - val_loss: 0.5374 - val_acc: 0.7991\n",
      "Epoch 308/500\n",
      " - 1s - loss: 0.3879 - acc: 0.8438 - val_loss: 0.5375 - val_acc: 0.7992\n",
      "Epoch 309/500\n",
      " - 1s - loss: 0.3900 - acc: 0.8436 - val_loss: 0.5376 - val_acc: 0.8006\n",
      "Epoch 310/500\n",
      " - 1s - loss: 0.3898 - acc: 0.8423 - val_loss: 0.5369 - val_acc: 0.7990\n",
      "Epoch 311/500\n",
      " - 1s - loss: 0.3882 - acc: 0.8423 - val_loss: 0.5371 - val_acc: 0.7995\n",
      "Epoch 312/500\n",
      " - 1s - loss: 0.3879 - acc: 0.8431 - val_loss: 0.5376 - val_acc: 0.7996\n",
      "Epoch 313/500\n",
      " - 1s - loss: 0.3849 - acc: 0.8443 - val_loss: 0.5385 - val_acc: 0.7997\n",
      "Epoch 314/500\n",
      " - 1s - loss: 0.3888 - acc: 0.8439 - val_loss: 0.5381 - val_acc: 0.7983\n",
      "Epoch 315/500\n",
      " - 1s - loss: 0.3887 - acc: 0.8435 - val_loss: 0.5377 - val_acc: 0.7986\n",
      "Epoch 316/500\n",
      " - 1s - loss: 0.3890 - acc: 0.8427 - val_loss: 0.5377 - val_acc: 0.7994\n",
      "Epoch 317/500\n",
      " - 1s - loss: 0.3844 - acc: 0.8448 - val_loss: 0.5381 - val_acc: 0.7994\n",
      "Epoch 318/500\n",
      " - 1s - loss: 0.3870 - acc: 0.8433 - val_loss: 0.5385 - val_acc: 0.7995\n",
      "Epoch 319/500\n",
      " - 1s - loss: 0.3857 - acc: 0.8432 - val_loss: 0.5381 - val_acc: 0.7992\n",
      "Epoch 320/500\n",
      " - 1s - loss: 0.3886 - acc: 0.8440 - val_loss: 0.5380 - val_acc: 0.7988\n",
      "Epoch 321/500\n",
      " - 1s - loss: 0.3836 - acc: 0.8448 - val_loss: 0.5386 - val_acc: 0.7992\n",
      "Epoch 322/500\n",
      " - 1s - loss: 0.3891 - acc: 0.8442 - val_loss: 0.5387 - val_acc: 0.7980\n",
      "Epoch 323/500\n",
      " - 1s - loss: 0.3861 - acc: 0.8446 - val_loss: 0.5391 - val_acc: 0.7985\n",
      "Epoch 324/500\n",
      " - 1s - loss: 0.3856 - acc: 0.8448 - val_loss: 0.5389 - val_acc: 0.7980\n",
      "Epoch 325/500\n",
      " - 1s - loss: 0.3825 - acc: 0.8467 - val_loss: 0.5395 - val_acc: 0.7991\n",
      "Epoch 326/500\n",
      " - 1s - loss: 0.3845 - acc: 0.8462 - val_loss: 0.5395 - val_acc: 0.7984\n",
      "Epoch 327/500\n",
      " - 1s - loss: 0.3854 - acc: 0.8424 - val_loss: 0.5390 - val_acc: 0.7986\n",
      "Epoch 328/500\n",
      " - 1s - loss: 0.3874 - acc: 0.8435 - val_loss: 0.5386 - val_acc: 0.7992\n",
      "Epoch 329/500\n",
      " - 1s - loss: 0.3848 - acc: 0.8453 - val_loss: 0.5395 - val_acc: 0.7989\n",
      "Epoch 330/500\n",
      " - 1s - loss: 0.3837 - acc: 0.8466 - val_loss: 0.5393 - val_acc: 0.7983\n",
      "Epoch 331/500\n",
      " - 1s - loss: 0.3824 - acc: 0.8463 - val_loss: 0.5397 - val_acc: 0.7997\n",
      "Epoch 332/500\n",
      " - 1s - loss: 0.3832 - acc: 0.8452 - val_loss: 0.5395 - val_acc: 0.7989\n",
      "Epoch 333/500\n",
      " - 1s - loss: 0.3820 - acc: 0.8442 - val_loss: 0.5405 - val_acc: 0.7990\n",
      "Epoch 334/500\n",
      " - 1s - loss: 0.3814 - acc: 0.8444 - val_loss: 0.5398 - val_acc: 0.7989\n",
      "Epoch 335/500\n",
      " - 1s - loss: 0.3792 - acc: 0.8457 - val_loss: 0.5401 - val_acc: 0.7991\n",
      "Epoch 336/500\n",
      " - 1s - loss: 0.3805 - acc: 0.8458 - val_loss: 0.5405 - val_acc: 0.7982\n",
      "Epoch 337/500\n",
      " - 1s - loss: 0.3836 - acc: 0.8449 - val_loss: 0.5405 - val_acc: 0.7979\n",
      "Epoch 338/500\n",
      " - 1s - loss: 0.3792 - acc: 0.8470 - val_loss: 0.5405 - val_acc: 0.7986\n",
      "Epoch 339/500\n",
      " - 1s - loss: 0.3790 - acc: 0.8472 - val_loss: 0.5406 - val_acc: 0.7991\n",
      "Epoch 340/500\n",
      " - 1s - loss: 0.3771 - acc: 0.8463 - val_loss: 0.5406 - val_acc: 0.7985\n",
      "Epoch 341/500\n",
      " - 1s - loss: 0.3830 - acc: 0.8450 - val_loss: 0.5407 - val_acc: 0.7980\n",
      "Epoch 342/500\n",
      " - 1s - loss: 0.3829 - acc: 0.8449 - val_loss: 0.5403 - val_acc: 0.7990\n",
      "Epoch 343/500\n",
      " - 1s - loss: 0.3788 - acc: 0.8462 - val_loss: 0.5407 - val_acc: 0.7985\n",
      "Epoch 344/500\n",
      " - 1s - loss: 0.3793 - acc: 0.8462 - val_loss: 0.5409 - val_acc: 0.7989\n",
      "Epoch 345/500\n",
      " - 1s - loss: 0.3773 - acc: 0.8465 - val_loss: 0.5413 - val_acc: 0.7992\n",
      "Epoch 346/500\n",
      " - 1s - loss: 0.3802 - acc: 0.8458 - val_loss: 0.5408 - val_acc: 0.7994\n",
      "Epoch 347/500\n",
      " - 1s - loss: 0.3797 - acc: 0.8462 - val_loss: 0.5410 - val_acc: 0.8001\n",
      "Epoch 348/500\n",
      " - 1s - loss: 0.3739 - acc: 0.8506 - val_loss: 0.5414 - val_acc: 0.8003\n",
      "Epoch 349/500\n",
      " - 1s - loss: 0.3757 - acc: 0.8488 - val_loss: 0.5415 - val_acc: 0.7992\n",
      "Epoch 350/500\n",
      " - 1s - loss: 0.3787 - acc: 0.8479 - val_loss: 0.5418 - val_acc: 0.7990\n",
      "Epoch 351/500\n",
      " - 1s - loss: 0.3741 - acc: 0.8487 - val_loss: 0.5418 - val_acc: 0.7986\n",
      "Epoch 352/500\n",
      " - 1s - loss: 0.3770 - acc: 0.8477 - val_loss: 0.5422 - val_acc: 0.7991\n",
      "Epoch 353/500\n",
      " - 1s - loss: 0.3744 - acc: 0.8480 - val_loss: 0.5424 - val_acc: 0.7979\n",
      "Epoch 354/500\n",
      " - 1s - loss: 0.3745 - acc: 0.8465 - val_loss: 0.5425 - val_acc: 0.7994\n",
      "Epoch 355/500\n",
      " - 1s - loss: 0.3768 - acc: 0.8464 - val_loss: 0.5422 - val_acc: 0.7992\n",
      "Epoch 356/500\n",
      " - 1s - loss: 0.3750 - acc: 0.8468 - val_loss: 0.5421 - val_acc: 0.7986\n",
      "Epoch 357/500\n",
      " - 1s - loss: 0.3718 - acc: 0.8506 - val_loss: 0.5424 - val_acc: 0.7992\n",
      "Epoch 358/500\n",
      " - 1s - loss: 0.3746 - acc: 0.8482 - val_loss: 0.5421 - val_acc: 0.7984\n",
      "Epoch 359/500\n",
      " - 1s - loss: 0.3764 - acc: 0.8461 - val_loss: 0.5422 - val_acc: 0.7994\n",
      "Epoch 360/500\n",
      " - 1s - loss: 0.3769 - acc: 0.8474 - val_loss: 0.5422 - val_acc: 0.7984\n",
      "Epoch 361/500\n",
      " - 1s - loss: 0.3734 - acc: 0.8497 - val_loss: 0.5417 - val_acc: 0.7991\n",
      "Epoch 362/500\n",
      " - 1s - loss: 0.3736 - acc: 0.8486 - val_loss: 0.5426 - val_acc: 0.7984\n",
      "Epoch 363/500\n",
      " - 1s - loss: 0.3762 - acc: 0.8463 - val_loss: 0.5426 - val_acc: 0.7983\n",
      "Epoch 364/500\n",
      " - 1s - loss: 0.3783 - acc: 0.8472 - val_loss: 0.5424 - val_acc: 0.7982\n",
      "Epoch 365/500\n",
      " - 1s - loss: 0.3749 - acc: 0.8475 - val_loss: 0.5425 - val_acc: 0.7982\n",
      "Epoch 366/500\n",
      " - 1s - loss: 0.3763 - acc: 0.8491 - val_loss: 0.5423 - val_acc: 0.7983\n",
      "Epoch 367/500\n",
      " - 1s - loss: 0.3718 - acc: 0.8512 - val_loss: 0.5428 - val_acc: 0.7985\n",
      "Epoch 368/500\n",
      " - 1s - loss: 0.3691 - acc: 0.8493 - val_loss: 0.5431 - val_acc: 0.7984\n",
      "Epoch 369/500\n",
      " - 1s - loss: 0.3735 - acc: 0.8488 - val_loss: 0.5432 - val_acc: 0.7982\n",
      "Epoch 370/500\n",
      " - 1s - loss: 0.3752 - acc: 0.8487 - val_loss: 0.5428 - val_acc: 0.7989\n",
      "Epoch 371/500\n",
      " - 1s - loss: 0.3709 - acc: 0.8518 - val_loss: 0.5434 - val_acc: 0.7990\n",
      "Epoch 372/500\n",
      " - 1s - loss: 0.3723 - acc: 0.8476 - val_loss: 0.5432 - val_acc: 0.7990\n",
      "Epoch 373/500\n",
      " - 1s - loss: 0.3731 - acc: 0.8482 - val_loss: 0.5428 - val_acc: 0.7991\n",
      "Epoch 374/500\n",
      " - 1s - loss: 0.3735 - acc: 0.8492 - val_loss: 0.5428 - val_acc: 0.7990\n",
      "Epoch 375/500\n",
      " - 1s - loss: 0.3717 - acc: 0.8488 - val_loss: 0.5428 - val_acc: 0.7997\n",
      "Epoch 376/500\n",
      " - 1s - loss: 0.3731 - acc: 0.8507 - val_loss: 0.5433 - val_acc: 0.7994\n",
      "Epoch 377/500\n",
      " - 1s - loss: 0.3750 - acc: 0.8479 - val_loss: 0.5428 - val_acc: 0.8001\n",
      "Epoch 378/500\n",
      " - 1s - loss: 0.3716 - acc: 0.8499 - val_loss: 0.5431 - val_acc: 0.8001\n",
      "Epoch 379/500\n",
      " - 1s - loss: 0.3692 - acc: 0.8501 - val_loss: 0.5434 - val_acc: 0.7995\n",
      "Epoch 380/500\n",
      " - 1s - loss: 0.3683 - acc: 0.8492 - val_loss: 0.5436 - val_acc: 0.7995\n",
      "Epoch 381/500\n",
      " - 1s - loss: 0.3709 - acc: 0.8491 - val_loss: 0.5436 - val_acc: 0.7999\n",
      "Epoch 382/500\n",
      " - 1s - loss: 0.3707 - acc: 0.8504 - val_loss: 0.5441 - val_acc: 0.7995\n",
      "Epoch 383/500\n",
      " - 1s - loss: 0.3712 - acc: 0.8506 - val_loss: 0.5438 - val_acc: 0.7996\n",
      "Epoch 384/500\n",
      " - 1s - loss: 0.3698 - acc: 0.8513 - val_loss: 0.5437 - val_acc: 0.7989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 385/500\n",
      " - 1s - loss: 0.3677 - acc: 0.8519 - val_loss: 0.5440 - val_acc: 0.7997\n",
      "Epoch 386/500\n",
      " - 1s - loss: 0.3654 - acc: 0.8523 - val_loss: 0.5446 - val_acc: 0.7994\n",
      "Epoch 387/500\n",
      " - 1s - loss: 0.3689 - acc: 0.8504 - val_loss: 0.5444 - val_acc: 0.7997\n",
      "Epoch 388/500\n",
      " - 1s - loss: 0.3644 - acc: 0.8530 - val_loss: 0.5445 - val_acc: 0.7997\n",
      "Epoch 389/500\n",
      " - 1s - loss: 0.3675 - acc: 0.8495 - val_loss: 0.5445 - val_acc: 0.7995\n",
      "Epoch 390/500\n",
      " - 1s - loss: 0.3652 - acc: 0.8513 - val_loss: 0.5448 - val_acc: 0.7994\n",
      "Epoch 391/500\n",
      " - 1s - loss: 0.3689 - acc: 0.8503 - val_loss: 0.5448 - val_acc: 0.7990\n",
      "Epoch 392/500\n",
      " - 1s - loss: 0.3728 - acc: 0.8515 - val_loss: 0.5442 - val_acc: 0.7995\n",
      "Epoch 393/500\n",
      " - 1s - loss: 0.3676 - acc: 0.8513 - val_loss: 0.5446 - val_acc: 0.7994\n",
      "Epoch 394/500\n",
      " - 1s - loss: 0.3685 - acc: 0.8499 - val_loss: 0.5446 - val_acc: 0.7989\n",
      "Epoch 395/500\n",
      " - 1s - loss: 0.3690 - acc: 0.8496 - val_loss: 0.5441 - val_acc: 0.7997\n",
      "Epoch 396/500\n",
      " - 1s - loss: 0.3645 - acc: 0.8523 - val_loss: 0.5454 - val_acc: 0.7994\n",
      "Epoch 397/500\n",
      " - 1s - loss: 0.3638 - acc: 0.8538 - val_loss: 0.5453 - val_acc: 0.7995\n",
      "Epoch 398/500\n",
      " - 1s - loss: 0.3701 - acc: 0.8505 - val_loss: 0.5446 - val_acc: 0.8008\n",
      "Epoch 399/500\n",
      " - 1s - loss: 0.3653 - acc: 0.8522 - val_loss: 0.5453 - val_acc: 0.7994\n",
      "Epoch 400/500\n",
      " - 1s - loss: 0.3664 - acc: 0.8508 - val_loss: 0.5458 - val_acc: 0.7989\n",
      "Epoch 401/500\n",
      " - 1s - loss: 0.3671 - acc: 0.8496 - val_loss: 0.5457 - val_acc: 0.7992\n",
      "Epoch 402/500\n",
      " - 1s - loss: 0.3639 - acc: 0.8515 - val_loss: 0.5458 - val_acc: 0.7989\n",
      "Epoch 403/500\n",
      " - 1s - loss: 0.3660 - acc: 0.8525 - val_loss: 0.5460 - val_acc: 0.7994\n",
      "Epoch 404/500\n",
      " - 1s - loss: 0.3645 - acc: 0.8508 - val_loss: 0.5462 - val_acc: 0.7990\n",
      "Epoch 405/500\n",
      " - 1s - loss: 0.3627 - acc: 0.8525 - val_loss: 0.5459 - val_acc: 0.7991\n",
      "Epoch 406/500\n",
      " - 1s - loss: 0.3636 - acc: 0.8536 - val_loss: 0.5460 - val_acc: 0.7991\n",
      "Epoch 407/500\n",
      " - 1s - loss: 0.3692 - acc: 0.8500 - val_loss: 0.5452 - val_acc: 0.7986\n",
      "Epoch 408/500\n",
      " - 1s - loss: 0.3640 - acc: 0.8515 - val_loss: 0.5458 - val_acc: 0.7989\n",
      "Epoch 409/500\n",
      " - 1s - loss: 0.3654 - acc: 0.8512 - val_loss: 0.5463 - val_acc: 0.7980\n",
      "Epoch 410/500\n",
      " - 1s - loss: 0.3657 - acc: 0.8533 - val_loss: 0.5461 - val_acc: 0.7986\n",
      "Epoch 411/500\n",
      " - 1s - loss: 0.3647 - acc: 0.8521 - val_loss: 0.5457 - val_acc: 0.7986\n",
      "Epoch 412/500\n",
      " - 1s - loss: 0.3683 - acc: 0.8503 - val_loss: 0.5454 - val_acc: 0.7980\n",
      "Epoch 413/500\n",
      " - 1s - loss: 0.3623 - acc: 0.8512 - val_loss: 0.5464 - val_acc: 0.7982\n",
      "Epoch 414/500\n",
      " - 1s - loss: 0.3653 - acc: 0.8517 - val_loss: 0.5463 - val_acc: 0.7985\n",
      "Epoch 415/500\n",
      " - 1s - loss: 0.3610 - acc: 0.8541 - val_loss: 0.5459 - val_acc: 0.7988\n",
      "Epoch 416/500\n",
      " - 1s - loss: 0.3641 - acc: 0.8528 - val_loss: 0.5459 - val_acc: 0.7992\n",
      "Epoch 417/500\n",
      " - 1s - loss: 0.3636 - acc: 0.8509 - val_loss: 0.5465 - val_acc: 0.7985\n",
      "Epoch 418/500\n",
      " - 1s - loss: 0.3650 - acc: 0.8521 - val_loss: 0.5460 - val_acc: 0.7985\n",
      "Epoch 419/500\n",
      " - 1s - loss: 0.3610 - acc: 0.8521 - val_loss: 0.5466 - val_acc: 0.7991\n",
      "Epoch 420/500\n",
      " - 1s - loss: 0.3612 - acc: 0.8532 - val_loss: 0.5467 - val_acc: 0.7990\n",
      "Epoch 421/500\n",
      " - 1s - loss: 0.3613 - acc: 0.8533 - val_loss: 0.5470 - val_acc: 0.7989\n",
      "Epoch 422/500\n",
      " - 1s - loss: 0.3638 - acc: 0.8520 - val_loss: 0.5478 - val_acc: 0.7989\n",
      "Epoch 423/500\n",
      " - 1s - loss: 0.3610 - acc: 0.8542 - val_loss: 0.5476 - val_acc: 0.7994\n",
      "Epoch 424/500\n",
      " - 1s - loss: 0.3595 - acc: 0.8546 - val_loss: 0.5475 - val_acc: 0.8001\n",
      "Epoch 425/500\n",
      " - 1s - loss: 0.3598 - acc: 0.8539 - val_loss: 0.5478 - val_acc: 0.7982\n",
      "Epoch 426/500\n",
      " - 1s - loss: 0.3630 - acc: 0.8543 - val_loss: 0.5473 - val_acc: 0.7990\n",
      "Epoch 427/500\n",
      " - 1s - loss: 0.3574 - acc: 0.8534 - val_loss: 0.5479 - val_acc: 0.7992\n",
      "Epoch 428/500\n",
      " - 1s - loss: 0.3579 - acc: 0.8539 - val_loss: 0.5477 - val_acc: 0.7988\n",
      "Epoch 429/500\n",
      " - 1s - loss: 0.3584 - acc: 0.8535 - val_loss: 0.5482 - val_acc: 0.7986\n",
      "Epoch 430/500\n",
      " - 1s - loss: 0.3601 - acc: 0.8536 - val_loss: 0.5479 - val_acc: 0.7995\n",
      "Epoch 431/500\n",
      " - 1s - loss: 0.3587 - acc: 0.8539 - val_loss: 0.5485 - val_acc: 0.8000\n",
      "Epoch 432/500\n",
      " - 1s - loss: 0.3608 - acc: 0.8549 - val_loss: 0.5486 - val_acc: 0.7983\n",
      "Epoch 433/500\n",
      " - 1s - loss: 0.3576 - acc: 0.8551 - val_loss: 0.5487 - val_acc: 0.7989\n",
      "Epoch 434/500\n",
      " - 1s - loss: 0.3609 - acc: 0.8551 - val_loss: 0.5488 - val_acc: 0.7999\n",
      "Epoch 435/500\n",
      " - 1s - loss: 0.3616 - acc: 0.8538 - val_loss: 0.5489 - val_acc: 0.7990\n",
      "Epoch 436/500\n",
      " - 1s - loss: 0.3584 - acc: 0.8569 - val_loss: 0.5493 - val_acc: 0.7997\n",
      "Epoch 437/500\n",
      " - 1s - loss: 0.3567 - acc: 0.8542 - val_loss: 0.5499 - val_acc: 0.7986\n",
      "Epoch 438/500\n",
      " - 1s - loss: 0.3594 - acc: 0.8547 - val_loss: 0.5493 - val_acc: 0.8003\n",
      "Epoch 439/500\n",
      " - 1s - loss: 0.3575 - acc: 0.8540 - val_loss: 0.5488 - val_acc: 0.8000\n",
      "Epoch 440/500\n",
      " - 1s - loss: 0.3603 - acc: 0.8542 - val_loss: 0.5489 - val_acc: 0.8000\n",
      "Epoch 441/500\n",
      " - 1s - loss: 0.3579 - acc: 0.8554 - val_loss: 0.5492 - val_acc: 0.7994\n",
      "Epoch 442/500\n",
      " - 1s - loss: 0.3613 - acc: 0.8529 - val_loss: 0.5493 - val_acc: 0.7992\n",
      "Epoch 443/500\n",
      " - 1s - loss: 0.3541 - acc: 0.8594 - val_loss: 0.5492 - val_acc: 0.7985\n",
      "Epoch 444/500\n",
      " - 1s - loss: 0.3566 - acc: 0.8553 - val_loss: 0.5489 - val_acc: 0.7989\n",
      "Epoch 445/500\n",
      " - 1s - loss: 0.3588 - acc: 0.8552 - val_loss: 0.5493 - val_acc: 0.7982\n",
      "Epoch 446/500\n",
      " - 1s - loss: 0.3537 - acc: 0.8565 - val_loss: 0.5499 - val_acc: 0.7994\n",
      "Epoch 447/500\n",
      " - 1s - loss: 0.3571 - acc: 0.8542 - val_loss: 0.5495 - val_acc: 0.7995\n",
      "Epoch 448/500\n",
      " - 1s - loss: 0.3550 - acc: 0.8542 - val_loss: 0.5492 - val_acc: 0.7999\n",
      "Epoch 449/500\n",
      " - 1s - loss: 0.3588 - acc: 0.8529 - val_loss: 0.5494 - val_acc: 0.7994\n",
      "Epoch 450/500\n",
      " - 1s - loss: 0.3551 - acc: 0.8572 - val_loss: 0.5502 - val_acc: 0.7986\n",
      "Epoch 451/500\n",
      " - 1s - loss: 0.3587 - acc: 0.8543 - val_loss: 0.5502 - val_acc: 0.7994\n",
      "Epoch 452/500\n",
      " - 1s - loss: 0.3580 - acc: 0.8526 - val_loss: 0.5498 - val_acc: 0.7995\n",
      "Epoch 453/500\n",
      " - 1s - loss: 0.3566 - acc: 0.8553 - val_loss: 0.5496 - val_acc: 0.7989\n",
      "Epoch 454/500\n",
      " - 1s - loss: 0.3563 - acc: 0.8565 - val_loss: 0.5500 - val_acc: 0.7989\n",
      "Epoch 455/500\n",
      " - 1s - loss: 0.3529 - acc: 0.8552 - val_loss: 0.5505 - val_acc: 0.7994\n",
      "Epoch 456/500\n",
      " - 1s - loss: 0.3539 - acc: 0.8548 - val_loss: 0.5510 - val_acc: 0.7994\n",
      "Epoch 457/500\n",
      " - 1s - loss: 0.3557 - acc: 0.8546 - val_loss: 0.5512 - val_acc: 0.7995\n",
      "Epoch 458/500\n",
      " - 1s - loss: 0.3571 - acc: 0.8535 - val_loss: 0.5509 - val_acc: 0.7985\n",
      "Epoch 459/500\n",
      " - 1s - loss: 0.3568 - acc: 0.8545 - val_loss: 0.5506 - val_acc: 0.7994\n",
      "Epoch 460/500\n",
      " - 1s - loss: 0.3564 - acc: 0.8556 - val_loss: 0.5505 - val_acc: 0.7996\n",
      "Epoch 461/500\n",
      " - 1s - loss: 0.3535 - acc: 0.8553 - val_loss: 0.5512 - val_acc: 0.7982\n",
      "Epoch 462/500\n",
      " - 1s - loss: 0.3558 - acc: 0.8560 - val_loss: 0.5507 - val_acc: 0.7990\n",
      "Epoch 463/500\n",
      " - 1s - loss: 0.3520 - acc: 0.8573 - val_loss: 0.5505 - val_acc: 0.7999\n",
      "Epoch 464/500\n",
      " - 1s - loss: 0.3544 - acc: 0.8567 - val_loss: 0.5509 - val_acc: 0.8001\n",
      "Epoch 465/500\n",
      " - 1s - loss: 0.3552 - acc: 0.8576 - val_loss: 0.5510 - val_acc: 0.7991\n",
      "Epoch 466/500\n",
      " - 1s - loss: 0.3524 - acc: 0.8559 - val_loss: 0.5515 - val_acc: 0.7996\n",
      "Epoch 467/500\n",
      " - 1s - loss: 0.3576 - acc: 0.8543 - val_loss: 0.5510 - val_acc: 0.8003\n",
      "Epoch 468/500\n",
      " - 1s - loss: 0.3528 - acc: 0.8572 - val_loss: 0.5513 - val_acc: 0.8000\n",
      "Epoch 469/500\n",
      " - 1s - loss: 0.3538 - acc: 0.8560 - val_loss: 0.5518 - val_acc: 0.7992\n",
      "Epoch 470/500\n",
      " - 1s - loss: 0.3517 - acc: 0.8575 - val_loss: 0.5514 - val_acc: 0.7996\n",
      "Epoch 471/500\n",
      " - 1s - loss: 0.3550 - acc: 0.8546 - val_loss: 0.5510 - val_acc: 0.7997\n",
      "Epoch 472/500\n",
      " - 1s - loss: 0.3518 - acc: 0.8564 - val_loss: 0.5513 - val_acc: 0.8003\n",
      "Epoch 473/500\n",
      " - 1s - loss: 0.3497 - acc: 0.8579 - val_loss: 0.5513 - val_acc: 0.7986\n",
      "Epoch 474/500\n",
      " - 1s - loss: 0.3503 - acc: 0.8568 - val_loss: 0.5516 - val_acc: 0.7986\n",
      "Epoch 475/500\n",
      " - 1s - loss: 0.3470 - acc: 0.8581 - val_loss: 0.5526 - val_acc: 0.7996\n",
      "Epoch 476/500\n",
      " - 1s - loss: 0.3473 - acc: 0.8589 - val_loss: 0.5531 - val_acc: 0.7997\n",
      "Epoch 477/500\n",
      " - 1s - loss: 0.3524 - acc: 0.8561 - val_loss: 0.5524 - val_acc: 0.7997\n",
      "Epoch 478/500\n",
      " - 1s - loss: 0.3534 - acc: 0.8564 - val_loss: 0.5523 - val_acc: 0.7985\n",
      "Epoch 479/500\n",
      " - 1s - loss: 0.3486 - acc: 0.8572 - val_loss: 0.5523 - val_acc: 0.8000\n",
      "Epoch 480/500\n",
      " - 1s - loss: 0.3464 - acc: 0.8587 - val_loss: 0.5533 - val_acc: 0.7991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 481/500\n",
      " - 1s - loss: 0.3511 - acc: 0.8555 - val_loss: 0.5532 - val_acc: 0.7989\n",
      "Epoch 482/500\n",
      " - 1s - loss: 0.3519 - acc: 0.8581 - val_loss: 0.5536 - val_acc: 0.7986\n",
      "Epoch 483/500\n",
      " - 1s - loss: 0.3514 - acc: 0.8575 - val_loss: 0.5531 - val_acc: 0.7985\n",
      "Epoch 484/500\n",
      " - 1s - loss: 0.3508 - acc: 0.8572 - val_loss: 0.5532 - val_acc: 0.7990\n",
      "Epoch 485/500\n",
      " - 1s - loss: 0.3479 - acc: 0.8579 - val_loss: 0.5537 - val_acc: 0.8001\n",
      "Epoch 486/500\n",
      " - 1s - loss: 0.3484 - acc: 0.8595 - val_loss: 0.5535 - val_acc: 0.7988\n",
      "Epoch 487/500\n",
      " - 1s - loss: 0.3510 - acc: 0.8578 - val_loss: 0.5536 - val_acc: 0.8000\n",
      "Epoch 488/500\n",
      " - 1s - loss: 0.3474 - acc: 0.8571 - val_loss: 0.5534 - val_acc: 0.7984\n",
      "Epoch 489/500\n",
      " - 1s - loss: 0.3477 - acc: 0.8586 - val_loss: 0.5536 - val_acc: 0.7996\n",
      "Epoch 490/500\n",
      " - 1s - loss: 0.3493 - acc: 0.8572 - val_loss: 0.5534 - val_acc: 0.7989\n",
      "Epoch 491/500\n",
      " - 1s - loss: 0.3516 - acc: 0.8572 - val_loss: 0.5527 - val_acc: 0.7997\n",
      "Epoch 492/500\n",
      " - 1s - loss: 0.3480 - acc: 0.8588 - val_loss: 0.5529 - val_acc: 0.8002\n",
      "Epoch 493/500\n",
      " - 1s - loss: 0.3456 - acc: 0.8593 - val_loss: 0.5542 - val_acc: 0.7980\n",
      "Epoch 494/500\n",
      " - 1s - loss: 0.3491 - acc: 0.8578 - val_loss: 0.5540 - val_acc: 0.7988\n",
      "Epoch 495/500\n",
      " - 1s - loss: 0.3478 - acc: 0.8605 - val_loss: 0.5539 - val_acc: 0.7996\n",
      "Epoch 496/500\n",
      " - 1s - loss: 0.3522 - acc: 0.8578 - val_loss: 0.5535 - val_acc: 0.7980\n",
      "Epoch 497/500\n",
      " - 1s - loss: 0.3461 - acc: 0.8605 - val_loss: 0.5537 - val_acc: 0.7983\n",
      "Epoch 498/500\n",
      " - 1s - loss: 0.3503 - acc: 0.8573 - val_loss: 0.5539 - val_acc: 0.7988\n",
      "Epoch 499/500\n",
      " - 1s - loss: 0.3501 - acc: 0.8575 - val_loss: 0.5538 - val_acc: 0.7995\n",
      "Epoch 500/500\n",
      " - 1s - loss: 0.3463 - acc: 0.8599 - val_loss: 0.5540 - val_acc: 0.7991\n"
     ]
    }
   ],
   "source": [
    "model2 = getModel2(increasedDropout)\n",
    "net7 = model2.fit(X, y, epochs=500, batch_size=512, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5285650968970725\n"
     ]
    }
   ],
   "source": [
    "#checking the best validation loss after decreasing the learning rate\n",
    "valid_loss7 = min(net7.history[\"val_loss\"])\n",
    "print(valid_loss7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters with grid search\n",
    "\n",
    "Keras sequential models (single-input only) can be used in *scikit-learn* by wrapping them with the *KerasClassifier* or *KerasRegressor* class.\n",
    "\n",
    "To use these wrappers you must define a function that creates and returns your Keras sequential model, then pass this function to the *build_fn* argument when constructing the KerasClassifier class. *build_fn* should construct, compile and return a Keras model, which will then be used to fit/predict. One of the following three values could be passed to build_fn:  \n",
    "\n",
    "- A function\n",
    "- An instance of a class that implements the call method\n",
    "- None. This means you implement a class that inherits from either KerasClassifier or KerasRegressor. The call method of the present class will then be treated as the default *build_fn*.\n",
    "\n",
    "E.g. we can create KerasClassifier instance with the above defined function *getModel* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=getModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor for the KerasClassifier class take second argument   \n",
    "*ssk_params*: model parameters & fitting parameters.  \n",
    "We can pass there any legal model and fitting parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 512,\n",
       " 'build_fn': <function __main__.getModel>,\n",
       " 'epochs': 200,\n",
       " 'verbose': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=getModel, epochs=200, batch_size=512, verbose=0)\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is now scikit-learn Classifier, we can use Grid search - a model hyperparameter optimization technique provided in scikit-learn GridSearchCV class.  \n",
    "\n",
    "When constructing this class we provide a dictionary of hyperparameters to evaluate in the param_grid argument.\n",
    "\n",
    "By default, the grid search will only use one thread. <br>\n",
    "By setting the *n_jobs* argument in the *GridSearchCV* constructor to -1, the process will use all cores on your machine. <br>\n",
    "If *n_jobs* was set to a value higher than one, the data are copied for each point in the grid (and not *n_jobs* times). <br>\n",
    "This is done for efficiency if individual jobs take very little time, but may raise errors if the dataset is large and there is not enough memory available. <br>\n",
    "A workaround in this case is to set pre_dispatch. <br>\n",
    "Then, the memory is copied only pre_dispatch many times. <br>\n",
    "A reasonable value for pre_dispatch is $2~\\times~$n_jobs.\n",
    "\n",
    "The *GridSearchCV* process will then construct and evaluate one model for each combination of parameters. <br>\n",
    "Cross validation is used to evaluate each individual model and the default of 3-fold cross validation is used, although this can be overridden by specifying the cv argument to the GridSearchCV constructor.  \n",
    "\n",
    "We set \"neg_log_loss\" (negative logloss) as a scoring measure since  *GridSearchCV* is trying to maximize score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the grid search parameters\n",
    "param_grid = dict(dropout=[0.1, 0.2, 0.3], neurons1=[300, 500, 700])\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                    scoring=\"neg_log_loss\", n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_result = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract results from *grid_result* attribute *cv_results_*. \n",
    "\n",
    "Recall that we were maximizing negative logloss. So, we output *-mean*  as logloss mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.969491 using {'dropout': 0.3, 'neurons1': 300}\n",
      "1.432594 (0.019329) with: {'dropout': 0.1, 'neurons1': 300}\n",
      "1.651521 (0.037130) with: {'dropout': 0.1, 'neurons1': 500}\n",
      "1.714247 (0.054301) with: {'dropout': 0.1, 'neurons1': 700}\n",
      "1.163371 (0.010785) with: {'dropout': 0.2, 'neurons1': 300}\n",
      "1.411013 (0.031086) with: {'dropout': 0.2, 'neurons1': 500}\n",
      "1.537629 (0.057455) with: {'dropout': 0.2, 'neurons1': 700}\n",
      "0.969491 (0.015789) with: {'dropout': 0.3, 'neurons1': 300}\n",
      "1.213389 (0.016753) with: {'dropout': 0.3, 'neurons1': 500}\n",
      "1.315439 (0.080900) with: {'dropout': 0.3, 'neurons1': 700}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (-grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (-mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Optimization Algorithm\n",
    "\n",
    "Grid search may be applied not only to numerical parameters. \n",
    "\n",
    "We can find the best optimizer using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getModel1(dropout=0.1, neurons1=500, neurons2=250,optimizer=\"Adagrad\"):\n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu', input_dim=num_features))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(neurons2, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=getModel1, dropout=0.3, neurons1=300,\n",
    "                        epochs=20, batch_size=512, verbose=0)\n",
    "# define the grid search parameters\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(optimizer=optimizer),\n",
    "                    scoring=\"neg_log_loss\",n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.545733 using {'optimizer': 'Adam'}\n",
      "0.796686 (0.005469) with: {'optimizer': 'SGD'}\n",
      "0.555684 (0.002417) with: {'optimizer': 'RMSprop'}\n",
      "0.553144 (0.004310) with: {'optimizer': 'Adagrad'}\n",
      "0.556329 (0.003893) with: {'optimizer': 'Adadelta'}\n",
      "0.545733 (0.005981) with: {'optimizer': 'Adam'}\n",
      "0.552629 (0.005630) with: {'optimizer': 'Adamax'}\n",
      "0.569726 (0.001170) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(X, y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (-grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (-mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The results dont seemt to be significantly being improved, however we can select the best optimizer from the above results and tweak the architecture a bit more to obtain better results. Lets see, how the network behaves with an increased dropout rate and decreasing the number of neurons in the network. Decreasing the number of neurons in the network will be helpful as we have lower number of parameters to estimate and given that our data set is not huge, we might be able to get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#incrreasing the number of layers in the nn\n",
    "def getModel3(dropout=0.4, neurons1=300, neurons2=250,neurons3=250,\n",
    "             learningRate=0.008):\n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu', input_dim=num_features,\n",
    "                    name='Dense_1'))\n",
    "    model.add(Dropout(dropout,name='Dropout_1'))\n",
    "    model.add(Dense(neurons2, activation='relu',name='Dense_2'))\n",
    "    model.add(Dropout(dropout,name='Dropout_2'))\n",
    "    model.add(Dense(neurons3, activation='relu', name='Dense_3'))\n",
    "    model.add(Dropout(dropout, name='Dropout_3'))\n",
    "    model.add(Dense(num_classes, activation='softmax',name='Output'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=learningRate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model3 = getModel3()\n",
    "\n",
    "#SVG(model_to_dot(model2).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/300\n",
      " - 3s - loss: 0.9022 - acc: 0.6808 - val_loss: 0.6524 - val_acc: 0.7609\n",
      "Epoch 2/300\n",
      " - 1s - loss: 0.6946 - acc: 0.7423 - val_loss: 0.6082 - val_acc: 0.7721\n",
      "Epoch 3/300\n",
      " - 1s - loss: 0.6554 - acc: 0.7559 - val_loss: 0.5903 - val_acc: 0.7739\n",
      "Epoch 4/300\n",
      " - 1s - loss: 0.6390 - acc: 0.7573 - val_loss: 0.5910 - val_acc: 0.7761\n",
      "Epoch 5/300\n",
      " - 1s - loss: 0.6246 - acc: 0.7623 - val_loss: 0.6051 - val_acc: 0.7626\n",
      "Epoch 6/300\n",
      " - 1s - loss: 0.6097 - acc: 0.7683 - val_loss: 0.5849 - val_acc: 0.7806\n",
      "Epoch 7/300\n",
      " - 1s - loss: 0.5998 - acc: 0.7705 - val_loss: 0.5819 - val_acc: 0.7751\n",
      "Epoch 8/300\n",
      " - 1s - loss: 0.5911 - acc: 0.7759 - val_loss: 0.5783 - val_acc: 0.7786\n",
      "Epoch 9/300\n",
      " - 1s - loss: 0.5868 - acc: 0.7768 - val_loss: 0.5760 - val_acc: 0.7808\n",
      "Epoch 10/300\n",
      " - 2s - loss: 0.5834 - acc: 0.7760 - val_loss: 0.5702 - val_acc: 0.7802\n",
      "Epoch 11/300\n",
      " - 1s - loss: 0.5757 - acc: 0.7810 - val_loss: 0.5698 - val_acc: 0.7874\n",
      "Epoch 12/300\n",
      " - 1s - loss: 0.5751 - acc: 0.7806 - val_loss: 0.5636 - val_acc: 0.7835\n",
      "Epoch 13/300\n",
      " - 1s - loss: 0.5679 - acc: 0.7830 - val_loss: 0.5697 - val_acc: 0.7876\n",
      "Epoch 14/300\n",
      " - 1s - loss: 0.5643 - acc: 0.7855 - val_loss: 0.5643 - val_acc: 0.7846\n",
      "Epoch 15/300\n",
      " - 1s - loss: 0.5556 - acc: 0.7839 - val_loss: 0.5616 - val_acc: 0.7855\n",
      "Epoch 16/300\n",
      " - 1s - loss: 0.5455 - acc: 0.7905 - val_loss: 0.5727 - val_acc: 0.7824\n",
      "Epoch 17/300\n",
      " - 1s - loss: 0.5459 - acc: 0.7891 - val_loss: 0.5653 - val_acc: 0.7908\n",
      "Epoch 18/300\n",
      " - 1s - loss: 0.5451 - acc: 0.7919 - val_loss: 0.5774 - val_acc: 0.7826\n",
      "Epoch 19/300\n",
      " - 1s - loss: 0.5415 - acc: 0.7888 - val_loss: 0.5786 - val_acc: 0.7806\n",
      "Epoch 20/300\n",
      " - 1s - loss: 0.5348 - acc: 0.7934 - val_loss: 0.5680 - val_acc: 0.7902\n",
      "Epoch 21/300\n",
      " - 1s - loss: 0.5319 - acc: 0.7934 - val_loss: 0.5770 - val_acc: 0.7790\n",
      "Epoch 22/300\n",
      " - 1s - loss: 0.5332 - acc: 0.7944 - val_loss: 0.5582 - val_acc: 0.7869\n",
      "Epoch 23/300\n",
      " - 1s - loss: 0.5339 - acc: 0.7959 - val_loss: 0.5733 - val_acc: 0.7865\n",
      "Epoch 24/300\n",
      " - 1s - loss: 0.5331 - acc: 0.7949 - val_loss: 0.5687 - val_acc: 0.7845\n",
      "Epoch 25/300\n",
      " - 1s - loss: 0.5232 - acc: 0.7992 - val_loss: 0.5809 - val_acc: 0.7848\n",
      "Epoch 26/300\n",
      " - 1s - loss: 0.5271 - acc: 0.7958 - val_loss: 0.5780 - val_acc: 0.7777\n",
      "Epoch 27/300\n",
      " - 1s - loss: 0.5224 - acc: 0.7994 - val_loss: 0.5657 - val_acc: 0.7887\n",
      "Epoch 28/300\n",
      " - 1s - loss: 0.5195 - acc: 0.8011 - val_loss: 0.5710 - val_acc: 0.7859\n",
      "Epoch 29/300\n",
      " - 1s - loss: 0.5202 - acc: 0.8008 - val_loss: 0.5714 - val_acc: 0.7899\n",
      "Epoch 30/300\n",
      " - 1s - loss: 0.5177 - acc: 0.8008 - val_loss: 0.5713 - val_acc: 0.7880\n",
      "Epoch 31/300\n",
      " - 1s - loss: 0.5114 - acc: 0.8012 - val_loss: 0.5744 - val_acc: 0.7863\n",
      "Epoch 32/300\n",
      " - 1s - loss: 0.5066 - acc: 0.8061 - val_loss: 0.5742 - val_acc: 0.7846\n",
      "Epoch 33/300\n",
      " - 1s - loss: 0.5109 - acc: 0.8031 - val_loss: 0.5693 - val_acc: 0.7872\n",
      "Epoch 34/300\n",
      " - 1s - loss: 0.5106 - acc: 0.8043 - val_loss: 0.5758 - val_acc: 0.7862\n",
      "Epoch 35/300\n",
      " - 1s - loss: 0.4965 - acc: 0.8085 - val_loss: 0.5698 - val_acc: 0.7840\n",
      "Epoch 36/300\n",
      " - 1s - loss: 0.5018 - acc: 0.8067 - val_loss: 0.5719 - val_acc: 0.7891\n",
      "Epoch 37/300\n",
      " - 1s - loss: 0.5035 - acc: 0.8073 - val_loss: 0.5709 - val_acc: 0.7883\n",
      "Epoch 38/300\n",
      " - 1s - loss: 0.5046 - acc: 0.8050 - val_loss: 0.5719 - val_acc: 0.7869\n",
      "Epoch 39/300\n",
      " - 1s - loss: 0.4928 - acc: 0.8088 - val_loss: 0.5783 - val_acc: 0.7914\n",
      "Epoch 40/300\n",
      " - 1s - loss: 0.4977 - acc: 0.8060 - val_loss: 0.5742 - val_acc: 0.7865\n",
      "Epoch 41/300\n",
      " - 1s - loss: 0.4889 - acc: 0.8117 - val_loss: 0.5898 - val_acc: 0.7851\n",
      "Epoch 42/300\n",
      " - 1s - loss: 0.5009 - acc: 0.8089 - val_loss: 0.5722 - val_acc: 0.7874\n",
      "Epoch 43/300\n",
      " - 1s - loss: 0.4994 - acc: 0.8092 - val_loss: 0.5760 - val_acc: 0.7880\n",
      "Epoch 44/300\n",
      " - 1s - loss: 0.4878 - acc: 0.8104 - val_loss: 0.5882 - val_acc: 0.7894\n",
      "Epoch 45/300\n",
      " - 1s - loss: 0.4961 - acc: 0.8079 - val_loss: 0.5829 - val_acc: 0.7853\n",
      "Epoch 46/300\n",
      " - 1s - loss: 0.4918 - acc: 0.8111 - val_loss: 0.5894 - val_acc: 0.7900\n",
      "Epoch 47/300\n",
      " - 1s - loss: 0.4811 - acc: 0.8158 - val_loss: 0.5950 - val_acc: 0.7863\n",
      "Epoch 48/300\n",
      " - 1s - loss: 0.4814 - acc: 0.8118 - val_loss: 0.5975 - val_acc: 0.7859\n",
      "Epoch 49/300\n",
      " - 1s - loss: 0.4826 - acc: 0.8122 - val_loss: 0.5904 - val_acc: 0.7887\n",
      "Epoch 50/300\n",
      " - 1s - loss: 0.4819 - acc: 0.8148 - val_loss: 0.5842 - val_acc: 0.7874\n",
      "Epoch 51/300\n",
      " - 1s - loss: 0.4869 - acc: 0.8147 - val_loss: 0.6053 - val_acc: 0.7854\n",
      "Epoch 52/300\n",
      " - 1s - loss: 0.4805 - acc: 0.8135 - val_loss: 0.6012 - val_acc: 0.7894\n",
      "Epoch 53/300\n",
      " - 1s - loss: 0.4802 - acc: 0.8157 - val_loss: 0.5928 - val_acc: 0.7865\n",
      "Epoch 54/300\n",
      " - 1s - loss: 0.4749 - acc: 0.8143 - val_loss: 0.5908 - val_acc: 0.7871\n",
      "Epoch 55/300\n",
      " - 1s - loss: 0.4711 - acc: 0.8176 - val_loss: 0.5929 - val_acc: 0.7916\n",
      "Epoch 56/300\n",
      " - 1s - loss: 0.4749 - acc: 0.8174 - val_loss: 0.6082 - val_acc: 0.7834\n",
      "Epoch 57/300\n",
      " - 1s - loss: 0.4773 - acc: 0.8165 - val_loss: 0.5992 - val_acc: 0.7849\n",
      "Epoch 58/300\n",
      " - 1s - loss: 0.4752 - acc: 0.8168 - val_loss: 0.6027 - val_acc: 0.7876\n",
      "Epoch 59/300\n",
      " - 1s - loss: 0.4692 - acc: 0.8184 - val_loss: 0.5943 - val_acc: 0.7882\n",
      "Epoch 60/300\n",
      " - 1s - loss: 0.4695 - acc: 0.8190 - val_loss: 0.6048 - val_acc: 0.7792\n",
      "Epoch 61/300\n",
      " - 1s - loss: 0.4639 - acc: 0.8225 - val_loss: 0.5987 - val_acc: 0.7892\n",
      "Epoch 62/300\n",
      " - 1s - loss: 0.4680 - acc: 0.8175 - val_loss: 0.6066 - val_acc: 0.7879\n",
      "Epoch 63/300\n",
      " - 1s - loss: 0.4690 - acc: 0.8196 - val_loss: 0.5994 - val_acc: 0.7789\n",
      "Epoch 64/300\n",
      " - 1s - loss: 0.4766 - acc: 0.8215 - val_loss: 0.5974 - val_acc: 0.7817\n",
      "Epoch 65/300\n",
      " - 1s - loss: 0.4702 - acc: 0.8188 - val_loss: 0.5993 - val_acc: 0.7883\n",
      "Epoch 66/300\n",
      " - 1s - loss: 0.4631 - acc: 0.8235 - val_loss: 0.5964 - val_acc: 0.7824\n",
      "Epoch 67/300\n",
      " - 1s - loss: 0.4657 - acc: 0.8197 - val_loss: 0.6022 - val_acc: 0.7912\n",
      "Epoch 68/300\n",
      " - 1s - loss: 0.4650 - acc: 0.8214 - val_loss: 0.5943 - val_acc: 0.7900\n",
      "Epoch 69/300\n",
      " - 1s - loss: 0.4706 - acc: 0.8217 - val_loss: 0.6019 - val_acc: 0.7929\n",
      "Epoch 70/300\n",
      " - 1s - loss: 0.4649 - acc: 0.8230 - val_loss: 0.5945 - val_acc: 0.7889\n",
      "Epoch 71/300\n",
      " - 1s - loss: 0.4490 - acc: 0.8279 - val_loss: 0.5954 - val_acc: 0.7919\n",
      "Epoch 72/300\n",
      " - 1s - loss: 0.4609 - acc: 0.8248 - val_loss: 0.5994 - val_acc: 0.7936\n",
      "Epoch 73/300\n",
      " - 1s - loss: 0.4565 - acc: 0.8244 - val_loss: 0.6216 - val_acc: 0.7904\n",
      "Epoch 74/300\n",
      " - 1s - loss: 0.4572 - acc: 0.8241 - val_loss: 0.6059 - val_acc: 0.7921\n",
      "Epoch 75/300\n",
      " - 1s - loss: 0.4532 - acc: 0.8252 - val_loss: 0.6016 - val_acc: 0.7877\n",
      "Epoch 76/300\n",
      " - 1s - loss: 0.4650 - acc: 0.8207 - val_loss: 0.6045 - val_acc: 0.7937\n",
      "Epoch 77/300\n",
      " - 1s - loss: 0.4561 - acc: 0.8236 - val_loss: 0.6217 - val_acc: 0.7868\n",
      "Epoch 78/300\n",
      " - 1s - loss: 0.4601 - acc: 0.8247 - val_loss: 0.6225 - val_acc: 0.7926\n",
      "Epoch 79/300\n",
      " - 1s - loss: 0.4625 - acc: 0.8219 - val_loss: 0.6136 - val_acc: 0.7853\n",
      "Epoch 80/300\n",
      " - 1s - loss: 0.4487 - acc: 0.8279 - val_loss: 0.6094 - val_acc: 0.7899\n",
      "Epoch 81/300\n",
      " - 1s - loss: 0.4641 - acc: 0.8200 - val_loss: 0.6176 - val_acc: 0.7812\n",
      "Epoch 82/300\n",
      " - 1s - loss: 0.4490 - acc: 0.8257 - val_loss: 0.6168 - val_acc: 0.7862\n",
      "Epoch 83/300\n",
      " - 1s - loss: 0.4524 - acc: 0.8263 - val_loss: 0.6167 - val_acc: 0.7871\n",
      "Epoch 84/300\n",
      " - 1s - loss: 0.4557 - acc: 0.8238 - val_loss: 0.6080 - val_acc: 0.7891\n",
      "Epoch 85/300\n",
      " - 1s - loss: 0.4452 - acc: 0.8279 - val_loss: 0.6095 - val_acc: 0.7920\n",
      "Epoch 86/300\n",
      " - 1s - loss: 0.4470 - acc: 0.8294 - val_loss: 0.6155 - val_acc: 0.7900\n",
      "Epoch 87/300\n",
      " - 1s - loss: 0.4533 - acc: 0.8249 - val_loss: 0.6241 - val_acc: 0.7912\n",
      "Epoch 88/300\n",
      " - 1s - loss: 0.4450 - acc: 0.8295 - val_loss: 0.6112 - val_acc: 0.7882\n",
      "Epoch 89/300\n",
      " - 1s - loss: 0.4456 - acc: 0.8319 - val_loss: 0.6213 - val_acc: 0.7871\n",
      "Epoch 90/300\n",
      " - 1s - loss: 0.4492 - acc: 0.8287 - val_loss: 0.6229 - val_acc: 0.7868\n",
      "Epoch 91/300\n",
      " - 1s - loss: 0.4430 - acc: 0.8299 - val_loss: 0.6221 - val_acc: 0.7926\n",
      "Epoch 92/300\n",
      " - 1s - loss: 0.4540 - acc: 0.8293 - val_loss: 0.6175 - val_acc: 0.7887\n",
      "Epoch 93/300\n",
      " - 1s - loss: 0.4477 - acc: 0.8310 - val_loss: 0.6372 - val_acc: 0.7882\n",
      "Epoch 94/300\n",
      " - 1s - loss: 0.4557 - acc: 0.8280 - val_loss: 0.6222 - val_acc: 0.7888\n",
      "Epoch 95/300\n",
      " - 2s - loss: 0.4490 - acc: 0.8287 - val_loss: 0.6196 - val_acc: 0.7902\n",
      "Epoch 96/300\n",
      " - 1s - loss: 0.4501 - acc: 0.8285 - val_loss: 0.6225 - val_acc: 0.7897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300\n",
      " - 1s - loss: 0.4450 - acc: 0.8291 - val_loss: 0.6304 - val_acc: 0.7894\n",
      "Epoch 98/300\n",
      " - 1s - loss: 0.4463 - acc: 0.8322 - val_loss: 0.6301 - val_acc: 0.7875\n",
      "Epoch 99/300\n",
      " - 1s - loss: 0.4425 - acc: 0.8319 - val_loss: 0.6343 - val_acc: 0.7864\n",
      "Epoch 100/300\n",
      " - 1s - loss: 0.4494 - acc: 0.8275 - val_loss: 0.6300 - val_acc: 0.7825\n",
      "Epoch 101/300\n",
      " - 1s - loss: 0.4454 - acc: 0.8331 - val_loss: 0.6294 - val_acc: 0.7854\n",
      "Epoch 102/300\n",
      " - 1s - loss: 0.4393 - acc: 0.8313 - val_loss: 0.6223 - val_acc: 0.7855\n",
      "Epoch 103/300\n",
      " - 1s - loss: 0.4430 - acc: 0.8322 - val_loss: 0.6281 - val_acc: 0.7801\n",
      "Epoch 104/300\n",
      " - 1s - loss: 0.4424 - acc: 0.8329 - val_loss: 0.6295 - val_acc: 0.7842\n",
      "Epoch 105/300\n",
      " - 1s - loss: 0.4440 - acc: 0.8319 - val_loss: 0.6345 - val_acc: 0.7883\n",
      "Epoch 106/300\n",
      " - 1s - loss: 0.4454 - acc: 0.8315 - val_loss: 0.6235 - val_acc: 0.7880\n",
      "Epoch 107/300\n",
      " - 1s - loss: 0.4359 - acc: 0.8344 - val_loss: 0.6220 - val_acc: 0.7885\n",
      "Epoch 108/300\n",
      " - 1s - loss: 0.4316 - acc: 0.8382 - val_loss: 0.6344 - val_acc: 0.7923\n",
      "Epoch 109/300\n",
      " - 1s - loss: 0.4315 - acc: 0.8363 - val_loss: 0.6530 - val_acc: 0.7882\n",
      "Epoch 110/300\n",
      " - 1s - loss: 0.4361 - acc: 0.8329 - val_loss: 0.6355 - val_acc: 0.7872\n",
      "Epoch 111/300\n",
      " - 1s - loss: 0.4301 - acc: 0.8366 - val_loss: 0.6413 - val_acc: 0.7889\n",
      "Epoch 112/300\n",
      " - 1s - loss: 0.4266 - acc: 0.8375 - val_loss: 0.6284 - val_acc: 0.7876\n",
      "Epoch 113/300\n",
      " - 1s - loss: 0.4317 - acc: 0.8352 - val_loss: 0.6404 - val_acc: 0.7887\n",
      "Epoch 114/300\n",
      " - 1s - loss: 0.4285 - acc: 0.8365 - val_loss: 0.6408 - val_acc: 0.7853\n",
      "Epoch 115/300\n",
      " - 1s - loss: 0.4245 - acc: 0.8370 - val_loss: 0.6461 - val_acc: 0.7896\n",
      "Epoch 116/300\n",
      " - 1s - loss: 0.4288 - acc: 0.8371 - val_loss: 0.6452 - val_acc: 0.7868\n",
      "Epoch 117/300\n",
      " - 1s - loss: 0.4299 - acc: 0.8365 - val_loss: 0.6363 - val_acc: 0.7894\n",
      "Epoch 118/300\n",
      " - 1s - loss: 0.4353 - acc: 0.8343 - val_loss: 0.6361 - val_acc: 0.7837\n",
      "Epoch 119/300\n",
      " - 1s - loss: 0.4328 - acc: 0.8375 - val_loss: 0.6616 - val_acc: 0.7835\n",
      "Epoch 120/300\n",
      " - 1s - loss: 0.4269 - acc: 0.8359 - val_loss: 0.6449 - val_acc: 0.7843\n",
      "Epoch 121/300\n",
      " - 1s - loss: 0.4311 - acc: 0.8358 - val_loss: 0.6529 - val_acc: 0.7928\n",
      "Epoch 122/300\n",
      " - 1s - loss: 0.4271 - acc: 0.8394 - val_loss: 0.6422 - val_acc: 0.7871\n",
      "Epoch 123/300\n",
      " - 1s - loss: 0.4294 - acc: 0.8369 - val_loss: 0.6520 - val_acc: 0.7845\n",
      "Epoch 124/300\n",
      " - 1s - loss: 0.4259 - acc: 0.8385 - val_loss: 0.6510 - val_acc: 0.7874\n",
      "Epoch 125/300\n",
      " - 1s - loss: 0.4278 - acc: 0.8375 - val_loss: 0.6547 - val_acc: 0.7870\n",
      "Epoch 126/300\n",
      " - 1s - loss: 0.4274 - acc: 0.8392 - val_loss: 0.6417 - val_acc: 0.7882\n",
      "Epoch 127/300\n",
      " - 1s - loss: 0.4226 - acc: 0.8364 - val_loss: 0.6467 - val_acc: 0.7855\n",
      "Epoch 128/300\n",
      " - 1s - loss: 0.4285 - acc: 0.8389 - val_loss: 0.6635 - val_acc: 0.7841\n",
      "Epoch 129/300\n",
      " - 1s - loss: 0.4228 - acc: 0.8395 - val_loss: 0.6467 - val_acc: 0.7876\n",
      "Epoch 130/300\n",
      " - 1s - loss: 0.4229 - acc: 0.8387 - val_loss: 0.6588 - val_acc: 0.7840\n",
      "Epoch 131/300\n",
      " - 1s - loss: 0.4208 - acc: 0.8396 - val_loss: 0.6630 - val_acc: 0.7870\n",
      "Epoch 132/300\n",
      " - 1s - loss: 0.4258 - acc: 0.8362 - val_loss: 0.6547 - val_acc: 0.7839\n",
      "Epoch 133/300\n",
      " - 1s - loss: 0.4261 - acc: 0.8367 - val_loss: 0.6648 - val_acc: 0.7862\n",
      "Epoch 134/300\n",
      " - 1s - loss: 0.4193 - acc: 0.8417 - val_loss: 0.6759 - val_acc: 0.7899\n",
      "Epoch 135/300\n",
      " - 1s - loss: 0.4242 - acc: 0.8378 - val_loss: 0.6475 - val_acc: 0.7872\n",
      "Epoch 136/300\n",
      " - 1s - loss: 0.4210 - acc: 0.8394 - val_loss: 0.6535 - val_acc: 0.7891\n",
      "Epoch 137/300\n",
      " - 1s - loss: 0.4189 - acc: 0.8386 - val_loss: 0.6661 - val_acc: 0.7854\n",
      "Epoch 138/300\n",
      " - 1s - loss: 0.4238 - acc: 0.8385 - val_loss: 0.6615 - val_acc: 0.7843\n",
      "Epoch 139/300\n",
      " - 1s - loss: 0.4238 - acc: 0.8376 - val_loss: 0.6721 - val_acc: 0.7845\n",
      "Epoch 140/300\n",
      " - 1s - loss: 0.4190 - acc: 0.8404 - val_loss: 0.6541 - val_acc: 0.7825\n",
      "Epoch 141/300\n",
      " - 1s - loss: 0.4287 - acc: 0.8390 - val_loss: 0.6633 - val_acc: 0.7875\n",
      "Epoch 142/300\n",
      " - 1s - loss: 0.4203 - acc: 0.8416 - val_loss: 0.6753 - val_acc: 0.7835\n",
      "Epoch 143/300\n",
      " - 1s - loss: 0.4237 - acc: 0.8396 - val_loss: 0.6660 - val_acc: 0.7898\n",
      "Epoch 144/300\n",
      " - 1s - loss: 0.4325 - acc: 0.8366 - val_loss: 0.6702 - val_acc: 0.7874\n",
      "Epoch 145/300\n",
      " - 1s - loss: 0.4173 - acc: 0.8422 - val_loss: 0.6778 - val_acc: 0.7862\n",
      "Epoch 146/300\n",
      " - 1s - loss: 0.4221 - acc: 0.8408 - val_loss: 0.6636 - val_acc: 0.7876\n",
      "Epoch 147/300\n",
      " - 1s - loss: 0.4166 - acc: 0.8406 - val_loss: 0.6626 - val_acc: 0.7898\n",
      "Epoch 148/300\n",
      " - 1s - loss: 0.4145 - acc: 0.8431 - val_loss: 0.6623 - val_acc: 0.7886\n",
      "Epoch 149/300\n",
      " - 1s - loss: 0.4224 - acc: 0.8389 - val_loss: 0.6871 - val_acc: 0.7889\n",
      "Epoch 150/300\n",
      " - 1s - loss: 0.4185 - acc: 0.8437 - val_loss: 0.6940 - val_acc: 0.7883\n",
      "Epoch 151/300\n",
      " - 1s - loss: 0.4233 - acc: 0.8407 - val_loss: 0.6664 - val_acc: 0.7853\n",
      "Epoch 152/300\n",
      " - 1s - loss: 0.4163 - acc: 0.8423 - val_loss: 0.7076 - val_acc: 0.7891\n",
      "Epoch 153/300\n",
      " - 1s - loss: 0.4091 - acc: 0.8440 - val_loss: 0.6930 - val_acc: 0.7881\n",
      "Epoch 154/300\n",
      " - 1s - loss: 0.4269 - acc: 0.8405 - val_loss: 0.6708 - val_acc: 0.7851\n",
      "Epoch 155/300\n",
      " - 1s - loss: 0.4212 - acc: 0.8411 - val_loss: 0.6689 - val_acc: 0.7906\n",
      "Epoch 156/300\n",
      " - 1s - loss: 0.4149 - acc: 0.8416 - val_loss: 0.6796 - val_acc: 0.7854\n",
      "Epoch 157/300\n",
      " - 1s - loss: 0.4243 - acc: 0.8416 - val_loss: 0.6830 - val_acc: 0.7858\n",
      "Epoch 158/300\n",
      " - 1s - loss: 0.4223 - acc: 0.8426 - val_loss: 0.6698 - val_acc: 0.7872\n",
      "Epoch 159/300\n",
      " - 1s - loss: 0.4224 - acc: 0.8400 - val_loss: 0.6797 - val_acc: 0.7880\n",
      "Epoch 160/300\n",
      " - 1s - loss: 0.4108 - acc: 0.8439 - val_loss: 0.6812 - val_acc: 0.7872\n",
      "Epoch 161/300\n",
      " - 1s - loss: 0.4141 - acc: 0.8426 - val_loss: 0.6759 - val_acc: 0.7854\n",
      "Epoch 162/300\n",
      " - 1s - loss: 0.4142 - acc: 0.8448 - val_loss: 0.6878 - val_acc: 0.7883\n",
      "Epoch 163/300\n",
      " - 1s - loss: 0.4082 - acc: 0.8425 - val_loss: 0.6992 - val_acc: 0.7834\n",
      "Epoch 164/300\n",
      " - 1s - loss: 0.4112 - acc: 0.8434 - val_loss: 0.6599 - val_acc: 0.7881\n",
      "Epoch 165/300\n",
      " - 1s - loss: 0.4112 - acc: 0.8443 - val_loss: 0.6971 - val_acc: 0.7752\n",
      "Epoch 166/300\n",
      " - 1s - loss: 0.4112 - acc: 0.8454 - val_loss: 0.6952 - val_acc: 0.7846\n",
      "Epoch 167/300\n",
      " - 1s - loss: 0.4121 - acc: 0.8451 - val_loss: 0.6725 - val_acc: 0.7922\n",
      "Epoch 168/300\n",
      " - 1s - loss: 0.4143 - acc: 0.8461 - val_loss: 0.6789 - val_acc: 0.7826\n",
      "Epoch 169/300\n",
      " - 1s - loss: 0.4181 - acc: 0.8436 - val_loss: 0.6844 - val_acc: 0.7868\n",
      "Epoch 170/300\n",
      " - 1s - loss: 0.4127 - acc: 0.8452 - val_loss: 0.6910 - val_acc: 0.7905\n",
      "Epoch 171/300\n",
      " - 1s - loss: 0.4103 - acc: 0.8430 - val_loss: 0.6730 - val_acc: 0.7891\n",
      "Epoch 172/300\n",
      " - 1s - loss: 0.4098 - acc: 0.8474 - val_loss: 0.6698 - val_acc: 0.7880\n",
      "Epoch 173/300\n",
      " - 1s - loss: 0.4151 - acc: 0.8442 - val_loss: 0.6828 - val_acc: 0.7886\n",
      "Epoch 174/300\n",
      " - 1s - loss: 0.4096 - acc: 0.8461 - val_loss: 0.6767 - val_acc: 0.7847\n",
      "Epoch 175/300\n",
      " - 1s - loss: 0.4198 - acc: 0.8444 - val_loss: 0.6974 - val_acc: 0.7841\n",
      "Epoch 176/300\n",
      " - 1s - loss: 0.4064 - acc: 0.8461 - val_loss: 0.6736 - val_acc: 0.7886\n",
      "Epoch 177/300\n",
      " - 1s - loss: 0.4084 - acc: 0.8423 - val_loss: 0.7151 - val_acc: 0.7888\n",
      "Epoch 178/300\n",
      " - 1s - loss: 0.4089 - acc: 0.8482 - val_loss: 0.6950 - val_acc: 0.7870\n",
      "Epoch 179/300\n",
      " - 1s - loss: 0.4099 - acc: 0.8462 - val_loss: 0.6885 - val_acc: 0.7852\n",
      "Epoch 180/300\n",
      " - 1s - loss: 0.4130 - acc: 0.8448 - val_loss: 0.6717 - val_acc: 0.7880\n",
      "Epoch 181/300\n",
      " - 1s - loss: 0.4048 - acc: 0.8477 - val_loss: 0.6811 - val_acc: 0.7863\n",
      "Epoch 182/300\n",
      " - 1s - loss: 0.4015 - acc: 0.8487 - val_loss: 0.6829 - val_acc: 0.7841\n",
      "Epoch 183/300\n",
      " - 1s - loss: 0.4014 - acc: 0.8460 - val_loss: 0.6943 - val_acc: 0.7843\n",
      "Epoch 184/300\n",
      " - 1s - loss: 0.4024 - acc: 0.8472 - val_loss: 0.6917 - val_acc: 0.7874\n",
      "Epoch 185/300\n",
      " - 1s - loss: 0.3960 - acc: 0.8509 - val_loss: 0.6976 - val_acc: 0.7840\n",
      "Epoch 186/300\n",
      " - 1s - loss: 0.4040 - acc: 0.8490 - val_loss: 0.6981 - val_acc: 0.7823\n",
      "Epoch 187/300\n",
      " - 1s - loss: 0.4055 - acc: 0.8462 - val_loss: 0.6896 - val_acc: 0.7839\n",
      "Epoch 188/300\n",
      " - 1s - loss: 0.3990 - acc: 0.8502 - val_loss: 0.6985 - val_acc: 0.7828\n",
      "Epoch 189/300\n",
      " - 1s - loss: 0.4029 - acc: 0.8486 - val_loss: 0.7007 - val_acc: 0.7858\n",
      "Epoch 190/300\n",
      " - 1s - loss: 0.3951 - acc: 0.8526 - val_loss: 0.7145 - val_acc: 0.7849\n",
      "Epoch 191/300\n",
      " - 1s - loss: 0.4064 - acc: 0.8468 - val_loss: 0.6964 - val_acc: 0.7836\n",
      "Epoch 192/300\n",
      " - 1s - loss: 0.4096 - acc: 0.8479 - val_loss: 0.6962 - val_acc: 0.7819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/300\n",
      " - 1s - loss: 0.4089 - acc: 0.8460 - val_loss: 0.7039 - val_acc: 0.7853\n",
      "Epoch 194/300\n",
      " - 1s - loss: 0.4088 - acc: 0.8471 - val_loss: 0.6863 - val_acc: 0.7885\n",
      "Epoch 195/300\n",
      " - 1s - loss: 0.4053 - acc: 0.8465 - val_loss: 0.6943 - val_acc: 0.7870\n",
      "Epoch 196/300\n",
      " - 1s - loss: 0.3974 - acc: 0.8499 - val_loss: 0.7092 - val_acc: 0.7847\n",
      "Epoch 197/300\n",
      " - 1s - loss: 0.4036 - acc: 0.8473 - val_loss: 0.7083 - val_acc: 0.7785\n",
      "Epoch 198/300\n",
      " - 1s - loss: 0.3972 - acc: 0.8494 - val_loss: 0.6970 - val_acc: 0.7852\n",
      "Epoch 199/300\n",
      " - 1s - loss: 0.3938 - acc: 0.8521 - val_loss: 0.7129 - val_acc: 0.7853\n",
      "Epoch 200/300\n",
      " - 1s - loss: 0.4054 - acc: 0.8486 - val_loss: 0.6843 - val_acc: 0.7897\n",
      "Epoch 201/300\n",
      " - 1s - loss: 0.3963 - acc: 0.8498 - val_loss: 0.7171 - val_acc: 0.7853\n",
      "Epoch 202/300\n",
      " - 1s - loss: 0.4025 - acc: 0.8494 - val_loss: 0.6916 - val_acc: 0.7881\n",
      "Epoch 203/300\n",
      " - 1s - loss: 0.4017 - acc: 0.8478 - val_loss: 0.6907 - val_acc: 0.7859\n",
      "Epoch 204/300\n",
      " - 1s - loss: 0.4045 - acc: 0.8494 - val_loss: 0.7060 - val_acc: 0.7837\n",
      "Epoch 205/300\n",
      " - 1s - loss: 0.3979 - acc: 0.8511 - val_loss: 0.7136 - val_acc: 0.7872\n",
      "Epoch 206/300\n",
      " - 1s - loss: 0.4027 - acc: 0.8494 - val_loss: 0.7014 - val_acc: 0.7847\n",
      "Epoch 207/300\n",
      " - 1s - loss: 0.3960 - acc: 0.8495 - val_loss: 0.7088 - val_acc: 0.7866\n",
      "Epoch 208/300\n",
      " - 1s - loss: 0.4047 - acc: 0.8486 - val_loss: 0.7103 - val_acc: 0.7835\n",
      "Epoch 209/300\n",
      " - 1s - loss: 0.4058 - acc: 0.8479 - val_loss: 0.7050 - val_acc: 0.7849\n",
      "Epoch 210/300\n",
      " - 1s - loss: 0.4051 - acc: 0.8470 - val_loss: 0.7191 - val_acc: 0.7831\n",
      "Epoch 211/300\n",
      " - 1s - loss: 0.4005 - acc: 0.8483 - val_loss: 0.7120 - val_acc: 0.7858\n",
      "Epoch 212/300\n",
      " - 1s - loss: 0.3965 - acc: 0.8503 - val_loss: 0.7224 - val_acc: 0.7872\n",
      "Epoch 213/300\n",
      " - 1s - loss: 0.4051 - acc: 0.8510 - val_loss: 0.7064 - val_acc: 0.7868\n",
      "Epoch 214/300\n",
      " - 1s - loss: 0.4058 - acc: 0.8504 - val_loss: 0.7117 - val_acc: 0.7863\n",
      "Epoch 215/300\n",
      " - 1s - loss: 0.4023 - acc: 0.8509 - val_loss: 0.6841 - val_acc: 0.7870\n",
      "Epoch 216/300\n",
      " - 1s - loss: 0.3938 - acc: 0.8526 - val_loss: 0.7132 - val_acc: 0.7837\n",
      "Epoch 217/300\n",
      " - 1s - loss: 0.4060 - acc: 0.8470 - val_loss: 0.7172 - val_acc: 0.7860\n",
      "Epoch 218/300\n",
      " - 1s - loss: 0.3955 - acc: 0.8515 - val_loss: 0.7146 - val_acc: 0.7847\n",
      "Epoch 219/300\n",
      " - 1s - loss: 0.3922 - acc: 0.8540 - val_loss: 0.7282 - val_acc: 0.7849\n",
      "Epoch 220/300\n",
      " - 1s - loss: 0.3909 - acc: 0.8516 - val_loss: 0.7489 - val_acc: 0.7858\n",
      "Epoch 221/300\n",
      " - 1s - loss: 0.3987 - acc: 0.8508 - val_loss: 0.7230 - val_acc: 0.7818\n",
      "Epoch 222/300\n",
      " - 1s - loss: 0.3980 - acc: 0.8489 - val_loss: 0.7235 - val_acc: 0.7855\n",
      "Epoch 223/300\n",
      " - 1s - loss: 0.4008 - acc: 0.8497 - val_loss: 0.7038 - val_acc: 0.7865\n",
      "Epoch 224/300\n",
      " - 1s - loss: 0.3981 - acc: 0.8480 - val_loss: 0.7354 - val_acc: 0.7888\n",
      "Epoch 225/300\n",
      " - 1s - loss: 0.3898 - acc: 0.8529 - val_loss: 0.7326 - val_acc: 0.7866\n",
      "Epoch 226/300\n",
      " - 1s - loss: 0.3928 - acc: 0.8519 - val_loss: 0.7492 - val_acc: 0.7889\n",
      "Epoch 227/300\n",
      " - 1s - loss: 0.3949 - acc: 0.8534 - val_loss: 0.7289 - val_acc: 0.7839\n",
      "Epoch 228/300\n",
      " - 1s - loss: 0.3907 - acc: 0.8542 - val_loss: 0.7232 - val_acc: 0.7847\n",
      "Epoch 229/300\n",
      " - 1s - loss: 0.3982 - acc: 0.8533 - val_loss: 0.7351 - val_acc: 0.7834\n",
      "Epoch 230/300\n",
      " - 1s - loss: 0.3957 - acc: 0.8502 - val_loss: 0.7436 - val_acc: 0.7847\n",
      "Epoch 231/300\n",
      " - 1s - loss: 0.3912 - acc: 0.8519 - val_loss: 0.7219 - val_acc: 0.7829\n",
      "Epoch 232/300\n",
      " - 1s - loss: 0.3930 - acc: 0.8523 - val_loss: 0.7214 - val_acc: 0.7820\n",
      "Epoch 233/300\n",
      " - 1s - loss: 0.3951 - acc: 0.8540 - val_loss: 0.7394 - val_acc: 0.7830\n",
      "Epoch 234/300\n",
      " - 1s - loss: 0.3950 - acc: 0.8539 - val_loss: 0.7135 - val_acc: 0.7851\n",
      "Epoch 235/300\n",
      " - 1s - loss: 0.3952 - acc: 0.8507 - val_loss: 0.7193 - val_acc: 0.7857\n",
      "Epoch 236/300\n",
      " - 1s - loss: 0.3957 - acc: 0.8525 - val_loss: 0.7174 - val_acc: 0.7752\n",
      "Epoch 237/300\n",
      " - 1s - loss: 0.3990 - acc: 0.8499 - val_loss: 0.7285 - val_acc: 0.7843\n",
      "Epoch 238/300\n",
      " - 1s - loss: 0.3913 - acc: 0.8514 - val_loss: 0.7490 - val_acc: 0.7864\n",
      "Epoch 239/300\n",
      " - 1s - loss: 0.3907 - acc: 0.8536 - val_loss: 0.7117 - val_acc: 0.7811\n",
      "Epoch 240/300\n",
      " - 1s - loss: 0.3986 - acc: 0.8546 - val_loss: 0.7157 - val_acc: 0.7809\n",
      "Epoch 241/300\n",
      " - 1s - loss: 0.3926 - acc: 0.8530 - val_loss: 0.7058 - val_acc: 0.7823\n",
      "Epoch 242/300\n",
      " - 1s - loss: 0.3965 - acc: 0.8523 - val_loss: 0.7203 - val_acc: 0.7824\n",
      "Epoch 243/300\n",
      " - 1s - loss: 0.3868 - acc: 0.8539 - val_loss: 0.7308 - val_acc: 0.7841\n",
      "Epoch 244/300\n",
      " - 1s - loss: 0.3935 - acc: 0.8503 - val_loss: 0.7325 - val_acc: 0.7826\n",
      "Epoch 245/300\n",
      " - 1s - loss: 0.3924 - acc: 0.8552 - val_loss: 0.7381 - val_acc: 0.7813\n",
      "Epoch 246/300\n",
      " - 1s - loss: 0.3983 - acc: 0.8501 - val_loss: 0.7194 - val_acc: 0.7853\n",
      "Epoch 247/300\n",
      " - 1s - loss: 0.3846 - acc: 0.8538 - val_loss: 0.7399 - val_acc: 0.7846\n",
      "Epoch 248/300\n",
      " - 1s - loss: 0.3938 - acc: 0.8521 - val_loss: 0.7181 - val_acc: 0.7847\n",
      "Epoch 249/300\n",
      " - 1s - loss: 0.4060 - acc: 0.8486 - val_loss: 0.7169 - val_acc: 0.7795\n",
      "Epoch 250/300\n",
      " - 1s - loss: 0.3904 - acc: 0.8537 - val_loss: 0.7151 - val_acc: 0.7877\n",
      "Epoch 251/300\n",
      " - 1s - loss: 0.4051 - acc: 0.8519 - val_loss: 0.7249 - val_acc: 0.7839\n",
      "Epoch 252/300\n",
      " - 1s - loss: 0.3999 - acc: 0.8502 - val_loss: 0.7356 - val_acc: 0.7865\n",
      "Epoch 253/300\n",
      " - 1s - loss: 0.3898 - acc: 0.8531 - val_loss: 0.7508 - val_acc: 0.7847\n",
      "Epoch 254/300\n",
      " - 1s - loss: 0.4005 - acc: 0.8504 - val_loss: 0.7299 - val_acc: 0.7800\n",
      "Epoch 255/300\n",
      " - 1s - loss: 0.3889 - acc: 0.8567 - val_loss: 0.7375 - val_acc: 0.7881\n",
      "Epoch 256/300\n",
      " - 1s - loss: 0.3983 - acc: 0.8530 - val_loss: 0.7277 - val_acc: 0.7843\n",
      "Epoch 257/300\n",
      " - 1s - loss: 0.3968 - acc: 0.8507 - val_loss: 0.7233 - val_acc: 0.7822\n",
      "Epoch 258/300\n",
      " - 1s - loss: 0.3970 - acc: 0.8524 - val_loss: 0.7290 - val_acc: 0.7853\n",
      "Epoch 259/300\n",
      " - 1s - loss: 0.3853 - acc: 0.8565 - val_loss: 0.7477 - val_acc: 0.7869\n",
      "Epoch 260/300\n",
      " - 1s - loss: 0.4024 - acc: 0.8496 - val_loss: 0.7532 - val_acc: 0.7808\n",
      "Epoch 261/300\n",
      " - 1s - loss: 0.3987 - acc: 0.8524 - val_loss: 0.7673 - val_acc: 0.7731\n",
      "Epoch 262/300\n",
      " - 1s - loss: 0.3969 - acc: 0.8525 - val_loss: 0.7405 - val_acc: 0.7855\n",
      "Epoch 263/300\n",
      " - 1s - loss: 0.3986 - acc: 0.8533 - val_loss: 0.7450 - val_acc: 0.7829\n",
      "Epoch 264/300\n",
      " - 1s - loss: 0.3894 - acc: 0.8520 - val_loss: 0.7470 - val_acc: 0.7811\n",
      "Epoch 265/300\n",
      " - 1s - loss: 0.3990 - acc: 0.8513 - val_loss: 0.7640 - val_acc: 0.7835\n",
      "Epoch 266/300\n",
      " - 1s - loss: 0.3977 - acc: 0.8526 - val_loss: 0.7372 - val_acc: 0.7854\n",
      "Epoch 267/300\n",
      " - 1s - loss: 0.3904 - acc: 0.8537 - val_loss: 0.7364 - val_acc: 0.7818\n",
      "Epoch 268/300\n",
      " - 1s - loss: 0.3878 - acc: 0.8554 - val_loss: 0.7474 - val_acc: 0.7847\n",
      "Epoch 269/300\n",
      " - 1s - loss: 0.3998 - acc: 0.8511 - val_loss: 0.7649 - val_acc: 0.7794\n",
      "Epoch 270/300\n",
      " - 1s - loss: 0.3931 - acc: 0.8532 - val_loss: 0.7340 - val_acc: 0.7836\n",
      "Epoch 271/300\n",
      " - 1s - loss: 0.3898 - acc: 0.8556 - val_loss: 0.7455 - val_acc: 0.7799\n",
      "Epoch 272/300\n",
      " - 1s - loss: 0.3967 - acc: 0.8507 - val_loss: 0.7394 - val_acc: 0.7839\n",
      "Epoch 273/300\n",
      " - 1s - loss: 0.3832 - acc: 0.8543 - val_loss: 0.7468 - val_acc: 0.7870\n",
      "Epoch 274/300\n",
      " - 1s - loss: 0.3878 - acc: 0.8568 - val_loss: 0.7554 - val_acc: 0.7863\n",
      "Epoch 275/300\n",
      " - 1s - loss: 0.3846 - acc: 0.8560 - val_loss: 0.7523 - val_acc: 0.7855\n",
      "Epoch 276/300\n",
      " - 1s - loss: 0.3867 - acc: 0.8567 - val_loss: 0.7667 - val_acc: 0.7823\n",
      "Epoch 277/300\n",
      " - 1s - loss: 0.3962 - acc: 0.8528 - val_loss: 0.7473 - val_acc: 0.7849\n",
      "Epoch 278/300\n",
      " - 1s - loss: 0.3959 - acc: 0.8536 - val_loss: 0.7345 - val_acc: 0.7824\n",
      "Epoch 279/300\n",
      " - 1s - loss: 0.3863 - acc: 0.8564 - val_loss: 0.7529 - val_acc: 0.7797\n",
      "Epoch 280/300\n",
      " - 1s - loss: 0.3863 - acc: 0.8539 - val_loss: 0.7608 - val_acc: 0.7846\n",
      "Epoch 281/300\n",
      " - 1s - loss: 0.3849 - acc: 0.8558 - val_loss: 0.7536 - val_acc: 0.7866\n",
      "Epoch 282/300\n",
      " - 1s - loss: 0.3778 - acc: 0.8593 - val_loss: 0.7727 - val_acc: 0.7869\n",
      "Epoch 283/300\n",
      " - 1s - loss: 0.3942 - acc: 0.8541 - val_loss: 0.7639 - val_acc: 0.7820\n",
      "Epoch 284/300\n",
      " - 1s - loss: 0.3917 - acc: 0.8551 - val_loss: 0.7664 - val_acc: 0.7807\n",
      "Epoch 285/300\n",
      " - 1s - loss: 0.3919 - acc: 0.8566 - val_loss: 0.7409 - val_acc: 0.7785\n",
      "Epoch 286/300\n",
      " - 1s - loss: 0.4002 - acc: 0.8538 - val_loss: 0.7776 - val_acc: 0.7830\n",
      "Epoch 287/300\n",
      " - 1s - loss: 0.3889 - acc: 0.8551 - val_loss: 0.7421 - val_acc: 0.7868\n",
      "Epoch 288/300\n",
      " - 1s - loss: 0.3819 - acc: 0.8579 - val_loss: 0.7478 - val_acc: 0.7803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/300\n",
      " - 1s - loss: 0.3953 - acc: 0.8560 - val_loss: 0.7597 - val_acc: 0.7812\n",
      "Epoch 290/300\n",
      " - 1s - loss: 0.3897 - acc: 0.8563 - val_loss: 0.7383 - val_acc: 0.7871\n",
      "Epoch 291/300\n",
      " - 1s - loss: 0.3914 - acc: 0.8566 - val_loss: 0.7619 - val_acc: 0.7823\n",
      "Epoch 292/300\n",
      " - 1s - loss: 0.3889 - acc: 0.8546 - val_loss: 0.7459 - val_acc: 0.7818\n",
      "Epoch 293/300\n",
      " - 1s - loss: 0.3879 - acc: 0.8546 - val_loss: 0.7890 - val_acc: 0.7840\n",
      "Epoch 294/300\n",
      " - 1s - loss: 0.3912 - acc: 0.8575 - val_loss: 0.7684 - val_acc: 0.7828\n",
      "Epoch 295/300\n",
      " - 1s - loss: 0.3952 - acc: 0.8558 - val_loss: 0.7509 - val_acc: 0.7837\n",
      "Epoch 296/300\n",
      " - 1s - loss: 0.3765 - acc: 0.8597 - val_loss: 0.7714 - val_acc: 0.7819\n",
      "Epoch 297/300\n",
      " - 1s - loss: 0.3804 - acc: 0.8571 - val_loss: 0.7803 - val_acc: 0.7835\n",
      "Epoch 298/300\n",
      " - 1s - loss: 0.3791 - acc: 0.8596 - val_loss: 0.7720 - val_acc: 0.7759\n",
      "Epoch 299/300\n",
      " - 1s - loss: 0.3932 - acc: 0.8538 - val_loss: 0.7582 - val_acc: 0.7830\n",
      "Epoch 300/300\n",
      " - 1s - loss: 0.3855 - acc: 0.8563 - val_loss: 0.7711 - val_acc: 0.7842\n"
     ]
    }
   ],
   "source": [
    "net8 = model3.fit(X, y, epochs=300, batch_size=512, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5581972258410927\n"
     ]
    }
   ],
   "source": [
    "#checking the best validation loss after decreasing the learning rate\n",
    "valid_loss8 = min(net8.history[\"val_loss\"])\n",
    "print(valid_loss8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried the above architecture with different configurations as well, the results are as follows:\n",
    "+ 0.5284347782959894, with adam\n",
    "+ 0.555913948054631, with 1000 nuerons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the final model I used.\n",
    "\n",
    "It was advised for us that if during training we hit the val loss around 0.52 then we can use it on our test set (this is a different data set we used), for a test log loss for below 0.49 and it worked on my data set. However, I am still going to toy with some of the other parameters and check how they are performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the activation function and using tanh activation fucntion, as this used by the team which won this Kaggle Competition. I am also increasing the number of neurons in the first layer in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#incrreasing the number of layers in the nn\n",
    "def getModel4(dropout=0.4, neurons1=1000, neurons2=250,neurons3=250,\n",
    "             learningRate=0.08):\n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='tanh', input_dim=num_features,\n",
    "                    name='Dense_1'))\n",
    "    model.add(Dropout(dropout,name='Dropout_1'))\n",
    "    model.add(Dense(neurons2, activation='tanh',name='Dense_2'))\n",
    "    model.add(Dropout(dropout,name='Dropout_2'))\n",
    "    model.add(Dense(neurons3, activation='tanh', name='Dense_3'))\n",
    "    model.add(Dropout(dropout, name='Dropout_3'))\n",
    "    model.add(Dense(num_classes, activation='softmax',name='Output'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=learningRate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model4 = getModel4()\n",
    "\n",
    "#SVG(model_to_dot(model2).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/300\n",
      " - 5s - loss: 8.5039 - acc: 0.2589 - val_loss: 6.8659 - val_acc: 0.4392\n",
      "Epoch 2/300\n",
      " - 2s - loss: 7.1757 - acc: 0.3630 - val_loss: 7.3692 - val_acc: 0.4228\n",
      "Epoch 3/300\n",
      " - 2s - loss: 7.6440 - acc: 0.3368 - val_loss: 8.5924 - val_acc: 0.3411\n",
      "Epoch 4/300\n",
      " - 2s - loss: 8.6096 - acc: 0.3211 - val_loss: 8.6507 - val_acc: 0.4196\n",
      "Epoch 5/300\n",
      " - 2s - loss: 8.5129 - acc: 0.3388 - val_loss: 8.5067 - val_acc: 0.4339\n",
      "Epoch 6/300\n",
      " - 2s - loss: 8.5596 - acc: 0.3314 - val_loss: 8.6991 - val_acc: 0.3457\n",
      "Epoch 7/300\n",
      " - 2s - loss: 8.7261 - acc: 0.2844 - val_loss: 9.5197 - val_acc: 0.1304\n",
      "Epoch 8/300\n",
      " - 2s - loss: 8.8799 - acc: 0.2708 - val_loss: 8.7714 - val_acc: 0.3443\n",
      "Epoch 9/300\n",
      " - 2s - loss: 9.0567 - acc: 0.2242 - val_loss: 9.0726 - val_acc: 0.1297\n",
      "Epoch 10/300\n",
      " - 2s - loss: 8.9189 - acc: 0.2382 - val_loss: 8.7663 - val_acc: 0.2408\n",
      "Epoch 11/300\n",
      " - 2s - loss: 8.6470 - acc: 0.2677 - val_loss: 8.6776 - val_acc: 0.3459\n",
      "Epoch 12/300\n",
      " - 2s - loss: 8.7623 - acc: 0.2272 - val_loss: 9.1891 - val_acc: 0.1227\n",
      "Epoch 13/300\n",
      " - 2s - loss: 8.7494 - acc: 0.2497 - val_loss: 8.6856 - val_acc: 0.3227\n",
      "Epoch 14/300\n",
      " - 2s - loss: 8.5851 - acc: 0.2834 - val_loss: 8.6968 - val_acc: 0.3091\n",
      "Epoch 15/300\n",
      " - 2s - loss: 8.5783 - acc: 0.2867 - val_loss: 8.6539 - val_acc: 0.2451\n",
      "Epoch 16/300\n",
      " - 2s - loss: 8.9190 - acc: 0.2729 - val_loss: 8.6786 - val_acc: 0.3353\n",
      "Epoch 17/300\n",
      " - 2s - loss: 8.5912 - acc: 0.2979 - val_loss: 8.6775 - val_acc: 0.3465\n",
      "Epoch 18/300\n",
      " - 2s - loss: 8.5398 - acc: 0.3044 - val_loss: 8.6278 - val_acc: 0.3466\n",
      "Epoch 19/300\n",
      " - 2s - loss: 8.5155 - acc: 0.3082 - val_loss: 8.6030 - val_acc: 0.3466\n",
      "Epoch 20/300\n",
      " - 2s - loss: 8.6528 - acc: 0.2990 - val_loss: 8.6674 - val_acc: 0.3473\n",
      "Epoch 21/300\n",
      " - 2s - loss: 8.7331 - acc: 0.2999 - val_loss: 8.6057 - val_acc: 0.3470\n",
      "Epoch 22/300\n",
      " - 2s - loss: 8.5938 - acc: 0.3001 - val_loss: 8.7684 - val_acc: 0.2354\n",
      "Epoch 23/300\n",
      " - 2s - loss: 8.5076 - acc: 0.3124 - val_loss: 8.7674 - val_acc: 0.3472\n",
      "Epoch 24/300\n",
      " - 2s - loss: 8.4925 - acc: 0.3237 - val_loss: 8.6972 - val_acc: 0.3462\n",
      "Epoch 25/300\n",
      " - 2s - loss: 8.5360 - acc: 0.3097 - val_loss: 8.9415 - val_acc: 0.2461\n",
      "Epoch 26/300\n",
      " - 2s - loss: 8.6206 - acc: 0.3116 - val_loss: 8.5825 - val_acc: 0.3820\n",
      "Epoch 27/300\n",
      " - 2s - loss: 8.5845 - acc: 0.3131 - val_loss: 9.0933 - val_acc: 0.2380\n",
      "Epoch 28/300\n",
      " - 2s - loss: 8.5489 - acc: 0.3024 - val_loss: 8.5881 - val_acc: 0.3461\n",
      "Epoch 29/300\n",
      " - 2s - loss: 8.6401 - acc: 0.2917 - val_loss: 8.5466 - val_acc: 0.4002\n",
      "Epoch 30/300\n",
      " - 2s - loss: 8.6168 - acc: 0.2861 - val_loss: 8.7573 - val_acc: 0.3449\n",
      "Epoch 31/300\n",
      " - 2s - loss: 8.5622 - acc: 0.2959 - val_loss: 8.6106 - val_acc: 0.3453\n",
      "Epoch 32/300\n",
      " - 2s - loss: 8.6611 - acc: 0.2851 - val_loss: 8.7190 - val_acc: 0.3460\n",
      "Epoch 33/300\n",
      " - 2s - loss: 8.7662 - acc: 0.2772 - val_loss: 8.7329 - val_acc: 0.3466\n",
      "Epoch 34/300\n",
      " - 2s - loss: 8.6410 - acc: 0.2763 - val_loss: 8.6209 - val_acc: 0.3421\n",
      "Epoch 35/300\n",
      " - 2s - loss: 8.6598 - acc: 0.2826 - val_loss: 8.6878 - val_acc: 0.3388\n",
      "Epoch 36/300\n",
      " - 2s - loss: 8.5632 - acc: 0.2950 - val_loss: 8.6545 - val_acc: 0.3400\n",
      "Epoch 37/300\n",
      " - 2s - loss: 8.5512 - acc: 0.2983 - val_loss: 8.6568 - val_acc: 0.3436\n",
      "Epoch 38/300\n",
      " - 2s - loss: 8.8782 - acc: 0.2633 - val_loss: 9.0664 - val_acc: 0.3444\n",
      "Epoch 39/300\n",
      " - 3s - loss: 8.7063 - acc: 0.2796 - val_loss: 8.6844 - val_acc: 0.3438\n",
      "Epoch 40/300\n",
      " - 3s - loss: 8.7732 - acc: 0.2769 - val_loss: 8.5885 - val_acc: 0.4193\n",
      "Epoch 41/300\n",
      " - 3s - loss: 8.6280 - acc: 0.2781 - val_loss: 8.6561 - val_acc: 0.2372\n",
      "Epoch 42/300\n",
      " - 2s - loss: 8.5497 - acc: 0.2854 - val_loss: 8.6439 - val_acc: 0.3430\n",
      "Epoch 43/300\n",
      " - 3s - loss: 8.6919 - acc: 0.2782 - val_loss: 8.6669 - val_acc: 0.3465\n",
      "Epoch 44/300\n",
      " - 2s - loss: 9.0660 - acc: 0.2318 - val_loss: 9.0908 - val_acc: 0.2309\n",
      "Epoch 45/300\n",
      " - 3s - loss: 8.8088 - acc: 0.2352 - val_loss: 8.8029 - val_acc: 0.3306\n",
      "Epoch 46/300\n",
      " - 3s - loss: 8.6654 - acc: 0.2647 - val_loss: 8.9977 - val_acc: 0.2309\n",
      "Epoch 47/300\n",
      " - 4s - loss: 8.6171 - acc: 0.2813 - val_loss: 8.6836 - val_acc: 0.2431\n",
      "Epoch 48/300\n",
      " - 3s - loss: 8.8028 - acc: 0.2718 - val_loss: 8.7091 - val_acc: 0.2389\n",
      "Epoch 49/300\n",
      " - 3s - loss: 8.6435 - acc: 0.2703 - val_loss: 8.6985 - val_acc: 0.1988\n",
      "Epoch 50/300\n",
      " - 3s - loss: 8.7164 - acc: 0.2598 - val_loss: 8.6598 - val_acc: 0.3453\n",
      "Epoch 51/300\n",
      " - 4s - loss: 8.8935 - acc: 0.2651 - val_loss: 8.9276 - val_acc: 0.3421\n",
      "Epoch 52/300\n",
      " - 3s - loss: 8.7722 - acc: 0.2743 - val_loss: 8.6094 - val_acc: 0.3454\n",
      "Epoch 53/300\n",
      " - 3s - loss: 8.5627 - acc: 0.2986 - val_loss: 8.6469 - val_acc: 0.4134\n",
      "Epoch 54/300\n",
      " - 3s - loss: 8.5503 - acc: 0.2961 - val_loss: 8.6515 - val_acc: 0.3455\n",
      "Epoch 55/300\n",
      " - 2s - loss: 8.5661 - acc: 0.3009 - val_loss: 8.7389 - val_acc: 0.3454\n",
      "Epoch 56/300\n",
      " - 2s - loss: 8.6254 - acc: 0.2939 - val_loss: 8.6790 - val_acc: 0.2354\n",
      "Epoch 57/300\n",
      " - 2s - loss: 8.5995 - acc: 0.2806 - val_loss: 8.8059 - val_acc: 0.1297\n",
      "Epoch 58/300\n",
      " - 2s - loss: 8.9608 - acc: 0.2497 - val_loss: 8.9041 - val_acc: 0.1761\n",
      "Epoch 59/300\n",
      " - 2s - loss: 8.9504 - acc: 0.2395 - val_loss: 9.2838 - val_acc: 0.2942\n",
      "Epoch 60/300\n",
      " - 2s - loss: 8.8113 - acc: 0.2482 - val_loss: 8.8119 - val_acc: 0.2542\n",
      "Epoch 61/300\n",
      " - 2s - loss: 8.5965 - acc: 0.2903 - val_loss: 8.7272 - val_acc: 0.3438\n",
      "Epoch 62/300\n",
      " - 2s - loss: 8.6012 - acc: 0.3004 - val_loss: 8.8309 - val_acc: 0.2280\n",
      "Epoch 63/300\n",
      " - 2s - loss: 8.5575 - acc: 0.2980 - val_loss: 8.6200 - val_acc: 0.3456\n",
      "Epoch 64/300\n",
      " - 2s - loss: 8.5248 - acc: 0.3140 - val_loss: 8.6073 - val_acc: 0.3919\n",
      "Epoch 65/300\n",
      " - 2s - loss: 8.6195 - acc: 0.2961 - val_loss: 8.6608 - val_acc: 0.2423\n",
      "Epoch 66/300\n",
      " - 2s - loss: 8.6784 - acc: 0.2773 - val_loss: 8.6282 - val_acc: 0.3451\n",
      "Epoch 67/300\n",
      " - 4s - loss: 8.5681 - acc: 0.2927 - val_loss: 8.5634 - val_acc: 0.4020\n",
      "Epoch 68/300\n",
      " - 6s - loss: 8.5468 - acc: 0.3015 - val_loss: 8.7214 - val_acc: 0.3461\n",
      "Epoch 69/300\n",
      " - 4s - loss: 8.6247 - acc: 0.3028 - val_loss: 8.5565 - val_acc: 0.3781\n",
      "Epoch 70/300\n",
      " - 4s - loss: 8.6106 - acc: 0.2947 - val_loss: 8.6755 - val_acc: 0.3456\n",
      "Epoch 71/300\n",
      " - 4s - loss: 8.6048 - acc: 0.3103 - val_loss: 8.9643 - val_acc: 0.3445\n",
      "Epoch 72/300\n",
      " - 3s - loss: 8.6421 - acc: 0.2928 - val_loss: 9.2359 - val_acc: 0.2415\n",
      "Epoch 73/300\n",
      " - 3s - loss: 8.7778 - acc: 0.2844 - val_loss: 8.6177 - val_acc: 0.3446\n",
      "Epoch 74/300\n",
      " - 2s - loss: 8.5277 - acc: 0.3117 - val_loss: 8.7469 - val_acc: 0.2442\n",
      "Epoch 75/300\n",
      " - 3s - loss: 8.5885 - acc: 0.3047 - val_loss: 8.6270 - val_acc: 0.3471\n",
      "Epoch 76/300\n",
      " - 3s - loss: 8.6966 - acc: 0.2858 - val_loss: 8.6233 - val_acc: 0.3473\n",
      "Epoch 77/300\n",
      " - 4s - loss: 8.5990 - acc: 0.3002 - val_loss: 8.6347 - val_acc: 0.3465\n",
      "Epoch 78/300\n",
      " - 3s - loss: 8.6870 - acc: 0.2831 - val_loss: 8.8836 - val_acc: 0.3454\n",
      "Epoch 79/300\n",
      " - 3s - loss: 8.6649 - acc: 0.2960 - val_loss: 8.6670 - val_acc: 0.3465\n",
      "Epoch 80/300\n",
      " - 3s - loss: 8.6263 - acc: 0.2815 - val_loss: 8.6732 - val_acc: 0.2358\n",
      "Epoch 81/300\n",
      " - 3s - loss: 8.7711 - acc: 0.2655 - val_loss: 8.8904 - val_acc: 0.3404\n",
      "Epoch 82/300\n",
      " - 3s - loss: 8.6772 - acc: 0.2896 - val_loss: 9.2286 - val_acc: 0.2439\n",
      "Epoch 83/300\n",
      " - 2s - loss: 8.5953 - acc: 0.2957 - val_loss: 8.8455 - val_acc: 0.2421\n",
      "Epoch 84/300\n",
      " - 2s - loss: 8.5402 - acc: 0.2982 - val_loss: 8.6049 - val_acc: 0.4017\n",
      "Epoch 85/300\n",
      " - 2s - loss: 8.6319 - acc: 0.2896 - val_loss: 8.6560 - val_acc: 0.3376\n",
      "Epoch 86/300\n",
      " - 3s - loss: 8.7292 - acc: 0.2787 - val_loss: 9.6025 - val_acc: 0.3460\n",
      "Epoch 87/300\n",
      " - 2s - loss: 8.6691 - acc: 0.2999 - val_loss: 8.6546 - val_acc: 0.2942\n",
      "Epoch 88/300\n",
      " - 2s - loss: 8.5748 - acc: 0.3049 - val_loss: 8.5452 - val_acc: 0.4082\n",
      "Epoch 89/300\n",
      " - 3s - loss: 8.5646 - acc: 0.3233 - val_loss: 8.5475 - val_acc: 0.4142\n",
      "Epoch 90/300\n",
      " - 3s - loss: 8.5566 - acc: 0.3242 - val_loss: 8.5275 - val_acc: 0.4145\n",
      "Epoch 91/300\n",
      " - 3s - loss: 8.5087 - acc: 0.3327 - val_loss: 8.5585 - val_acc: 0.3465\n",
      "Epoch 92/300\n",
      " - 3s - loss: 8.5012 - acc: 0.3364 - val_loss: 8.5769 - val_acc: 0.3460\n",
      "Epoch 93/300\n",
      " - 2s - loss: 8.5426 - acc: 0.3267 - val_loss: 8.5342 - val_acc: 0.3448\n",
      "Epoch 94/300\n",
      " - 2s - loss: 8.5968 - acc: 0.3092 - val_loss: 8.5664 - val_acc: 0.4131\n",
      "Epoch 95/300\n",
      " - 2s - loss: 8.5451 - acc: 0.3307 - val_loss: 8.5530 - val_acc: 0.4190\n",
      "Epoch 96/300\n",
      " - 2s - loss: 8.5423 - acc: 0.3249 - val_loss: 8.7422 - val_acc: 0.3371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300\n",
      " - 2s - loss: 8.5694 - acc: 0.3093 - val_loss: 8.6362 - val_acc: 0.3427\n",
      "Epoch 98/300\n",
      " - 2s - loss: 8.6847 - acc: 0.2977 - val_loss: 8.6235 - val_acc: 0.3388\n",
      "Epoch 99/300\n",
      " - 2s - loss: 8.6554 - acc: 0.2981 - val_loss: 8.6567 - val_acc: 0.3837\n",
      "Epoch 100/300\n",
      " - 2s - loss: 8.6801 - acc: 0.2859 - val_loss: 8.5674 - val_acc: 0.3640\n",
      "Epoch 101/300\n",
      " - 2s - loss: 8.5203 - acc: 0.3210 - val_loss: 8.6137 - val_acc: 0.3472\n",
      "Epoch 102/300\n",
      " - 2s - loss: 8.5471 - acc: 0.3220 - val_loss: 8.8088 - val_acc: 0.3465\n",
      "Epoch 103/300\n",
      " - 2s - loss: 8.4778 - acc: 0.3382 - val_loss: 8.5752 - val_acc: 0.3996\n",
      "Epoch 104/300\n",
      " - 2s - loss: 8.7199 - acc: 0.2970 - val_loss: 8.7673 - val_acc: 0.3179\n",
      "Epoch 105/300\n",
      " - 2s - loss: 8.8627 - acc: 0.2671 - val_loss: 8.6565 - val_acc: 0.3306\n",
      "Epoch 106/300\n",
      " - 2s - loss: 8.7705 - acc: 0.2697 - val_loss: 8.6372 - val_acc: 0.3446\n",
      "Epoch 107/300\n",
      " - 2s - loss: 8.6791 - acc: 0.2933 - val_loss: 8.6320 - val_acc: 0.2435\n",
      "Epoch 108/300\n",
      " - 2s - loss: 8.5372 - acc: 0.3122 - val_loss: 8.5877 - val_acc: 0.3931\n",
      "Epoch 109/300\n",
      " - 3s - loss: 8.6490 - acc: 0.3032 - val_loss: 8.5924 - val_acc: 0.3462\n",
      "Epoch 110/300\n",
      " - 2s - loss: 8.5153 - acc: 0.3240 - val_loss: 8.8146 - val_acc: 0.2443\n",
      "Epoch 111/300\n",
      " - 2s - loss: 8.5983 - acc: 0.3066 - val_loss: 8.5821 - val_acc: 0.3462\n",
      "Epoch 112/300\n",
      " - 2s - loss: 8.5293 - acc: 0.3125 - val_loss: 8.5914 - val_acc: 0.3654\n",
      "Epoch 113/300\n",
      " - 3s - loss: 8.5416 - acc: 0.3136 - val_loss: 8.6598 - val_acc: 0.3637\n",
      "Epoch 114/300\n",
      " - 2s - loss: 8.5264 - acc: 0.3258 - val_loss: 8.5415 - val_acc: 0.4237\n",
      "Epoch 115/300\n",
      " - 2s - loss: 8.5600 - acc: 0.3300 - val_loss: 9.0308 - val_acc: 0.1297\n",
      "Epoch 116/300\n",
      " - 2s - loss: 8.5712 - acc: 0.3205 - val_loss: 8.6215 - val_acc: 0.3468\n",
      "Epoch 117/300\n",
      " - 2s - loss: 8.5017 - acc: 0.3273 - val_loss: 8.5380 - val_acc: 0.4228\n",
      "Epoch 118/300\n",
      " - 3s - loss: 8.5456 - acc: 0.3242 - val_loss: 8.5480 - val_acc: 0.3897\n",
      "Epoch 119/300\n",
      " - 3s - loss: 8.4615 - acc: 0.3373 - val_loss: 8.6296 - val_acc: 0.3606\n",
      "Epoch 120/300\n",
      " - 3s - loss: 8.5661 - acc: 0.3220 - val_loss: 8.6513 - val_acc: 0.3438\n",
      "Epoch 121/300\n",
      " - 2s - loss: 8.5351 - acc: 0.3272 - val_loss: 8.6508 - val_acc: 0.3434\n",
      "Epoch 122/300\n",
      " - 2s - loss: 8.5546 - acc: 0.3348 - val_loss: 8.5297 - val_acc: 0.3552\n",
      "Epoch 123/300\n",
      " - 2s - loss: 8.6001 - acc: 0.3209 - val_loss: 8.5993 - val_acc: 0.4004\n",
      "Epoch 124/300\n",
      " - 2s - loss: 8.4834 - acc: 0.3553 - val_loss: 8.5490 - val_acc: 0.3882\n",
      "Epoch 125/300\n",
      " - 3s - loss: 8.4849 - acc: 0.3443 - val_loss: 8.5361 - val_acc: 0.3994\n",
      "Epoch 126/300\n",
      " - 3s - loss: 8.4604 - acc: 0.3524 - val_loss: 8.5232 - val_acc: 0.4272\n",
      "Epoch 127/300\n",
      " - 3s - loss: 8.4704 - acc: 0.3518 - val_loss: 8.7864 - val_acc: 0.2448\n",
      "Epoch 128/300\n",
      " - 3s - loss: 8.5615 - acc: 0.3417 - val_loss: 8.5782 - val_acc: 0.3655\n",
      "Epoch 129/300\n",
      " - 2s - loss: 8.4555 - acc: 0.3511 - val_loss: 8.5266 - val_acc: 0.3450\n",
      "Epoch 130/300\n",
      " - 2s - loss: 8.4651 - acc: 0.3600 - val_loss: 8.4917 - val_acc: 0.4324\n",
      "Epoch 131/300\n",
      " - 2s - loss: 8.4348 - acc: 0.3700 - val_loss: 8.5867 - val_acc: 0.3472\n",
      "Epoch 132/300\n",
      " - 2s - loss: 8.4529 - acc: 0.3684 - val_loss: 8.4910 - val_acc: 0.4353\n",
      "Epoch 133/300\n",
      " - 2s - loss: 8.4638 - acc: 0.3594 - val_loss: 8.5394 - val_acc: 0.3587\n",
      "Epoch 134/300\n",
      " - 2s - loss: 8.5934 - acc: 0.3399 - val_loss: 8.5463 - val_acc: 0.4077\n",
      "Epoch 135/300\n",
      " - 2s - loss: 8.5036 - acc: 0.3515 - val_loss: 8.6206 - val_acc: 0.4313\n",
      "Epoch 136/300\n",
      " - 2s - loss: 8.4297 - acc: 0.3708 - val_loss: 8.5113 - val_acc: 0.4274\n",
      "Epoch 137/300\n",
      " - 2s - loss: 8.4147 - acc: 0.3722 - val_loss: 8.6663 - val_acc: 0.3463\n",
      "Epoch 138/300\n",
      " - 2s - loss: 8.5636 - acc: 0.3489 - val_loss: 8.6187 - val_acc: 0.3472\n",
      "Epoch 139/300\n",
      " - 2s - loss: 8.4820 - acc: 0.3560 - val_loss: 8.6536 - val_acc: 0.4365\n",
      "Epoch 140/300\n",
      " - 2s - loss: 8.4770 - acc: 0.3603 - val_loss: 8.5059 - val_acc: 0.4348\n",
      "Epoch 141/300\n",
      " - 2s - loss: 8.5051 - acc: 0.3561 - val_loss: 8.4988 - val_acc: 0.4357\n",
      "Epoch 142/300\n",
      " - 2s - loss: 8.4940 - acc: 0.3550 - val_loss: 8.5755 - val_acc: 0.4359\n",
      "Epoch 143/300\n",
      " - 2s - loss: 8.5051 - acc: 0.3568 - val_loss: 8.5107 - val_acc: 0.4351\n",
      "Epoch 144/300\n",
      " - 2s - loss: 8.4471 - acc: 0.3663 - val_loss: 8.5034 - val_acc: 0.4354\n",
      "Epoch 145/300\n",
      " - 2s - loss: 8.4820 - acc: 0.3601 - val_loss: 8.5413 - val_acc: 0.4359\n",
      "Epoch 146/300\n",
      " - 2s - loss: 8.6791 - acc: 0.3308 - val_loss: 8.7373 - val_acc: 0.3391\n",
      "Epoch 147/300\n",
      " - 2s - loss: 8.5414 - acc: 0.3520 - val_loss: 8.5755 - val_acc: 0.4317\n",
      "Epoch 148/300\n",
      " - 2s - loss: 8.4890 - acc: 0.3541 - val_loss: 8.5034 - val_acc: 0.4203\n",
      "Epoch 149/300\n",
      " - 2s - loss: 8.4743 - acc: 0.3606 - val_loss: 8.5461 - val_acc: 0.4239\n",
      "Epoch 150/300\n",
      " - 2s - loss: 8.6018 - acc: 0.3505 - val_loss: 8.6506 - val_acc: 0.3413\n",
      "Epoch 151/300\n",
      " - 2s - loss: 8.4761 - acc: 0.3663 - val_loss: 8.5283 - val_acc: 0.4188\n",
      "Epoch 152/300\n",
      " - 2s - loss: 8.4521 - acc: 0.3711 - val_loss: 8.5572 - val_acc: 0.4336\n",
      "Epoch 153/300\n",
      " - 2s - loss: 8.4739 - acc: 0.3674 - val_loss: 8.5191 - val_acc: 0.4319\n",
      "Epoch 154/300\n",
      " - 2s - loss: 8.4520 - acc: 0.3662 - val_loss: 8.5706 - val_acc: 0.3410\n",
      "Epoch 155/300\n",
      " - 2s - loss: 8.4837 - acc: 0.3587 - val_loss: 8.5072 - val_acc: 0.4202\n",
      "Epoch 156/300\n",
      " - 2s - loss: 8.4500 - acc: 0.3644 - val_loss: 8.6531 - val_acc: 0.3414\n",
      "Epoch 157/300\n",
      " - 2s - loss: 8.4451 - acc: 0.3679 - val_loss: 8.5720 - val_acc: 0.4180\n",
      "Epoch 158/300\n",
      " - 2s - loss: 8.4677 - acc: 0.3593 - val_loss: 8.5345 - val_acc: 0.4227\n",
      "Epoch 159/300\n",
      " - 2s - loss: 8.5427 - acc: 0.3464 - val_loss: 8.5811 - val_acc: 0.3406\n",
      "Epoch 160/300\n",
      " - 2s - loss: 8.6497 - acc: 0.3377 - val_loss: 8.5374 - val_acc: 0.4363\n",
      "Epoch 161/300\n",
      " - 2s - loss: 8.5307 - acc: 0.3540 - val_loss: 8.5524 - val_acc: 0.3684\n",
      "Epoch 162/300\n",
      " - 2s - loss: 8.5393 - acc: 0.3446 - val_loss: 8.5564 - val_acc: 0.4108\n",
      "Epoch 163/300\n",
      " - 2s - loss: 8.5131 - acc: 0.3527 - val_loss: 8.5232 - val_acc: 0.4312\n",
      "Epoch 164/300\n",
      " - 2s - loss: 8.4844 - acc: 0.3547 - val_loss: 8.5139 - val_acc: 0.4158\n",
      "Epoch 165/300\n",
      " - 2s - loss: 8.5104 - acc: 0.3528 - val_loss: 8.5206 - val_acc: 0.4225\n",
      "Epoch 166/300\n",
      " - 2s - loss: 8.4475 - acc: 0.3582 - val_loss: 8.5516 - val_acc: 0.4216\n",
      "Epoch 167/300\n",
      " - 2s - loss: 8.5219 - acc: 0.3491 - val_loss: 8.5698 - val_acc: 0.4251\n",
      "Epoch 168/300\n",
      " - 2s - loss: 8.4645 - acc: 0.3648 - val_loss: 8.5783 - val_acc: 0.3453\n",
      "Epoch 169/300\n",
      " - 2s - loss: 8.4885 - acc: 0.3554 - val_loss: 8.5613 - val_acc: 0.3798\n",
      "Epoch 170/300\n",
      " - 2s - loss: 8.4581 - acc: 0.3561 - val_loss: 8.5034 - val_acc: 0.4283\n",
      "Epoch 171/300\n",
      " - 2s - loss: 8.4803 - acc: 0.3595 - val_loss: 8.5760 - val_acc: 0.4305\n",
      "Epoch 172/300\n",
      " - 2s - loss: 8.5281 - acc: 0.3553 - val_loss: 8.6611 - val_acc: 0.3473\n",
      "Epoch 173/300\n",
      " - 2s - loss: 8.4706 - acc: 0.3657 - val_loss: 8.5154 - val_acc: 0.4261\n",
      "Epoch 174/300\n",
      " - 2s - loss: 8.5023 - acc: 0.3592 - val_loss: 8.5457 - val_acc: 0.3474\n",
      "Epoch 175/300\n",
      " - 2s - loss: 8.4421 - acc: 0.3688 - val_loss: 8.6256 - val_acc: 0.3944\n",
      "Epoch 176/300\n",
      " - 2s - loss: 8.5018 - acc: 0.3562 - val_loss: 8.8090 - val_acc: 0.4245\n",
      "Epoch 177/300\n",
      " - 2s - loss: 8.4695 - acc: 0.3775 - val_loss: 8.4780 - val_acc: 0.4379\n",
      "Epoch 178/300\n",
      " - 2s - loss: 8.4227 - acc: 0.3834 - val_loss: 8.5325 - val_acc: 0.4384\n",
      "Epoch 179/300\n",
      " - 2s - loss: 8.4701 - acc: 0.3725 - val_loss: 8.4938 - val_acc: 0.4384\n",
      "Epoch 180/300\n",
      " - 2s - loss: 8.4703 - acc: 0.3737 - val_loss: 8.7096 - val_acc: 0.3234\n",
      "Epoch 181/300\n",
      " - 2s - loss: 8.4750 - acc: 0.3837 - val_loss: 8.5880 - val_acc: 0.3472\n",
      "Epoch 182/300\n",
      " - 2s - loss: 8.4467 - acc: 0.3858 - val_loss: 8.5117 - val_acc: 0.4376\n",
      "Epoch 183/300\n",
      " - 2s - loss: 8.3975 - acc: 0.3965 - val_loss: 8.5036 - val_acc: 0.4365\n",
      "Epoch 184/300\n",
      " - 2s - loss: 8.4249 - acc: 0.3908 - val_loss: 8.4973 - val_acc: 0.4388\n",
      "Epoch 185/300\n",
      " - 2s - loss: 8.4933 - acc: 0.3763 - val_loss: 8.5533 - val_acc: 0.4356\n",
      "Epoch 186/300\n",
      " - 2s - loss: 8.4213 - acc: 0.3836 - val_loss: 8.5577 - val_acc: 0.3467\n",
      "Epoch 187/300\n",
      " - 2s - loss: 8.4469 - acc: 0.3770 - val_loss: 8.5678 - val_acc: 0.4391\n",
      "Epoch 188/300\n",
      " - 2s - loss: 8.5069 - acc: 0.3772 - val_loss: 8.5316 - val_acc: 0.4378\n",
      "Epoch 189/300\n",
      " - 2s - loss: 8.4297 - acc: 0.3867 - val_loss: 8.5142 - val_acc: 0.4376\n",
      "Epoch 190/300\n",
      " - 2s - loss: 8.5123 - acc: 0.3573 - val_loss: 8.5328 - val_acc: 0.4325\n",
      "Epoch 191/300\n",
      " - 2s - loss: 8.5638 - acc: 0.3474 - val_loss: 8.5270 - val_acc: 0.3991\n",
      "Epoch 192/300\n",
      " - 2s - loss: 8.5033 - acc: 0.3575 - val_loss: 8.5562 - val_acc: 0.3470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/300\n",
      " - 2s - loss: 8.4575 - acc: 0.3629 - val_loss: 8.5254 - val_acc: 0.4284\n",
      "Epoch 194/300\n",
      " - 2s - loss: 8.5420 - acc: 0.3556 - val_loss: 8.9449 - val_acc: 0.3470\n",
      "Epoch 195/300\n",
      " - 2s - loss: 8.5485 - acc: 0.3537 - val_loss: 8.5213 - val_acc: 0.3468\n",
      "Epoch 196/300\n",
      " - 2s - loss: 8.4646 - acc: 0.3586 - val_loss: 8.5025 - val_acc: 0.4285\n",
      "Epoch 197/300\n",
      " - 2s - loss: 8.4432 - acc: 0.3660 - val_loss: 9.0655 - val_acc: 0.3448\n",
      "Epoch 198/300\n",
      " - 2s - loss: 8.5263 - acc: 0.3668 - val_loss: 8.5734 - val_acc: 0.3448\n",
      "Epoch 199/300\n",
      " - 2s - loss: 8.4325 - acc: 0.3780 - val_loss: 8.6126 - val_acc: 0.3462\n",
      "Epoch 200/300\n",
      " - 2s - loss: 8.4980 - acc: 0.3645 - val_loss: 8.5370 - val_acc: 0.4329\n",
      "Epoch 201/300\n",
      " - 2s - loss: 8.4681 - acc: 0.3738 - val_loss: 8.8992 - val_acc: 0.3444\n",
      "Epoch 202/300\n",
      " - 3s - loss: 8.5279 - acc: 0.3448 - val_loss: 8.5034 - val_acc: 0.4288\n",
      "Epoch 203/300\n",
      " - 2s - loss: 8.4577 - acc: 0.3659 - val_loss: 8.4965 - val_acc: 0.4268\n",
      "Epoch 204/300\n",
      " - 2s - loss: 8.5147 - acc: 0.3594 - val_loss: 8.5478 - val_acc: 0.3439\n",
      "Epoch 205/300\n",
      " - 2s - loss: 8.5283 - acc: 0.3483 - val_loss: 8.4962 - val_acc: 0.4313\n",
      "Epoch 206/300\n",
      " - 2s - loss: 8.5826 - acc: 0.3504 - val_loss: 8.5221 - val_acc: 0.4316\n",
      "Epoch 207/300\n",
      " - 2s - loss: 8.4500 - acc: 0.3700 - val_loss: 8.5318 - val_acc: 0.4374\n",
      "Epoch 208/300\n",
      " - 2s - loss: 8.4268 - acc: 0.3758 - val_loss: 8.5052 - val_acc: 0.4284\n",
      "Epoch 209/300\n",
      " - 2s - loss: 8.4729 - acc: 0.3609 - val_loss: 8.5178 - val_acc: 0.4354\n",
      "Epoch 210/300\n",
      " - 2s - loss: 8.4712 - acc: 0.3670 - val_loss: 8.4936 - val_acc: 0.4314\n",
      "Epoch 211/300\n",
      " - 2s - loss: 8.5396 - acc: 0.3511 - val_loss: 8.5412 - val_acc: 0.4340\n",
      "Epoch 212/300\n",
      " - 2s - loss: 8.5035 - acc: 0.3574 - val_loss: 8.7567 - val_acc: 0.3460\n",
      "Epoch 213/300\n",
      " - 2s - loss: 8.5165 - acc: 0.3567 - val_loss: 8.4849 - val_acc: 0.4334\n",
      "Epoch 214/300\n",
      " - 2s - loss: 8.4840 - acc: 0.3583 - val_loss: 8.4897 - val_acc: 0.4378\n",
      "Epoch 215/300\n",
      " - 2s - loss: 8.4636 - acc: 0.3726 - val_loss: 8.5456 - val_acc: 0.4339\n",
      "Epoch 216/300\n",
      " - 2s - loss: 8.4623 - acc: 0.3659 - val_loss: 8.6230 - val_acc: 0.3360\n",
      "Epoch 217/300\n",
      " - 2s - loss: 8.5642 - acc: 0.3436 - val_loss: 8.5881 - val_acc: 0.3445\n",
      "Epoch 218/300\n",
      " - 2s - loss: 8.5144 - acc: 0.3518 - val_loss: 8.6318 - val_acc: 0.3463\n",
      "Epoch 219/300\n",
      " - 2s - loss: 8.4685 - acc: 0.3655 - val_loss: 8.7373 - val_acc: 0.3368\n",
      "Epoch 220/300\n",
      " - 2s - loss: 8.4502 - acc: 0.3753 - val_loss: 8.5567 - val_acc: 0.4276\n",
      "Epoch 221/300\n",
      " - 2s - loss: 8.5112 - acc: 0.3640 - val_loss: 8.5841 - val_acc: 0.3451\n",
      "Epoch 222/300\n",
      " - 2s - loss: 8.4374 - acc: 0.3804 - val_loss: 8.4920 - val_acc: 0.4331\n",
      "Epoch 223/300\n",
      " - 2s - loss: 8.4090 - acc: 0.3823 - val_loss: 8.6408 - val_acc: 0.3451\n",
      "Epoch 224/300\n",
      " - 2s - loss: 8.4792 - acc: 0.3636 - val_loss: 8.5991 - val_acc: 0.4211\n",
      "Epoch 225/300\n",
      " - 2s - loss: 8.4479 - acc: 0.3739 - val_loss: 8.5731 - val_acc: 0.4210\n",
      "Epoch 226/300\n",
      " - 2s - loss: 8.4230 - acc: 0.3774 - val_loss: 8.4789 - val_acc: 0.4392\n",
      "Epoch 227/300\n",
      " - 2s - loss: 8.4171 - acc: 0.3841 - val_loss: 8.5015 - val_acc: 0.4392\n",
      "Epoch 228/300\n",
      " - 2s - loss: 8.4128 - acc: 0.3922 - val_loss: 8.5668 - val_acc: 0.4293\n",
      "Epoch 229/300\n",
      " - 2s - loss: 8.4985 - acc: 0.3725 - val_loss: 8.6262 - val_acc: 0.4243\n",
      "Epoch 230/300\n",
      " - 2s - loss: 8.5527 - acc: 0.3638 - val_loss: 8.5157 - val_acc: 0.4380\n",
      "Epoch 231/300\n",
      " - 2s - loss: 8.4536 - acc: 0.3861 - val_loss: 8.5972 - val_acc: 0.4356\n",
      "Epoch 232/300\n",
      " - 2s - loss: 8.4606 - acc: 0.3733 - val_loss: 8.4907 - val_acc: 0.4379\n",
      "Epoch 233/300\n",
      " - 2s - loss: 8.4359 - acc: 0.3805 - val_loss: 8.5114 - val_acc: 0.4316\n",
      "Epoch 234/300\n",
      " - 2s - loss: 8.4497 - acc: 0.3787 - val_loss: 8.5240 - val_acc: 0.4340\n",
      "Epoch 235/300\n",
      " - 2s - loss: 8.4332 - acc: 0.3849 - val_loss: 8.5290 - val_acc: 0.4367\n",
      "Epoch 236/300\n",
      " - 2s - loss: 8.3979 - acc: 0.3965 - val_loss: 8.5237 - val_acc: 0.4344\n",
      "Epoch 237/300\n",
      " - 2s - loss: 8.4733 - acc: 0.3779 - val_loss: 8.5181 - val_acc: 0.4340\n",
      "Epoch 238/300\n",
      " - 2s - loss: 8.4140 - acc: 0.3898 - val_loss: 8.5178 - val_acc: 0.4344\n",
      "Epoch 239/300\n",
      " - 2s - loss: 8.4550 - acc: 0.3779 - val_loss: 8.5546 - val_acc: 0.4313\n",
      "Epoch 240/300\n",
      " - 2s - loss: 8.4628 - acc: 0.3735 - val_loss: 8.6097 - val_acc: 0.3459\n",
      "Epoch 241/300\n",
      " - 2s - loss: 8.5389 - acc: 0.3670 - val_loss: 9.0082 - val_acc: 0.3463\n",
      "Epoch 242/300\n",
      " - 2s - loss: 8.5261 - acc: 0.3729 - val_loss: 8.5691 - val_acc: 0.3462\n",
      "Epoch 243/300\n",
      " - 2s - loss: 8.4312 - acc: 0.3775 - val_loss: 8.5188 - val_acc: 0.4386\n",
      "Epoch 244/300\n",
      " - 3s - loss: 8.4736 - acc: 0.3707 - val_loss: 8.4883 - val_acc: 0.4387\n",
      "Epoch 245/300\n",
      " - 2s - loss: 8.4490 - acc: 0.3757 - val_loss: 8.5168 - val_acc: 0.4388\n",
      "Epoch 246/300\n",
      " - 2s - loss: 8.4729 - acc: 0.3758 - val_loss: 8.5272 - val_acc: 0.3919\n",
      "Epoch 247/300\n",
      " - 2s - loss: 8.5068 - acc: 0.3632 - val_loss: 8.5372 - val_acc: 0.3756\n",
      "Epoch 248/300\n",
      " - 2s - loss: 8.4372 - acc: 0.3746 - val_loss: 8.5130 - val_acc: 0.4339\n",
      "Epoch 249/300\n",
      " - 2s - loss: 8.4439 - acc: 0.3798 - val_loss: 8.5099 - val_acc: 0.4373\n",
      "Epoch 250/300\n",
      " - 2s - loss: 8.4675 - acc: 0.3712 - val_loss: 8.5182 - val_acc: 0.4321\n",
      "Epoch 251/300\n",
      " - 2s - loss: 8.5486 - acc: 0.3681 - val_loss: 8.5733 - val_acc: 0.4356\n",
      "Epoch 252/300\n",
      " - 2s - loss: 8.4426 - acc: 0.3777 - val_loss: 8.4919 - val_acc: 0.4387\n",
      "Epoch 253/300\n",
      " - 3s - loss: 8.4718 - acc: 0.3795 - val_loss: 8.5366 - val_acc: 0.4362\n",
      "Epoch 254/300\n",
      " - 3s - loss: 8.4686 - acc: 0.3727 - val_loss: 8.6307 - val_acc: 0.3478\n",
      "Epoch 255/300\n",
      " - 2s - loss: 8.5089 - acc: 0.3748 - val_loss: 8.5384 - val_acc: 0.4380\n",
      "Epoch 256/300\n",
      " - 3s - loss: 8.4661 - acc: 0.3736 - val_loss: 8.8311 - val_acc: 0.3476\n",
      "Epoch 257/300\n",
      " - 2s - loss: 8.4796 - acc: 0.3719 - val_loss: 8.5708 - val_acc: 0.3470\n",
      "Epoch 258/300\n",
      " - 2s - loss: 8.4407 - acc: 0.3783 - val_loss: 8.4922 - val_acc: 0.4363\n",
      "Epoch 259/300\n",
      " - 2s - loss: 8.4956 - acc: 0.3638 - val_loss: 8.5004 - val_acc: 0.4399\n",
      "Epoch 260/300\n",
      " - 2s - loss: 8.4287 - acc: 0.3788 - val_loss: 8.4835 - val_acc: 0.4340\n",
      "Epoch 261/300\n",
      " - 2s - loss: 8.4597 - acc: 0.3780 - val_loss: 8.5423 - val_acc: 0.3478\n",
      "Epoch 262/300\n",
      " - 2s - loss: 8.4417 - acc: 0.3788 - val_loss: 8.5119 - val_acc: 0.4418\n",
      "Epoch 263/300\n",
      " - 2s - loss: 8.4461 - acc: 0.3828 - val_loss: 8.5455 - val_acc: 0.4224\n",
      "Epoch 264/300\n",
      " - 2s - loss: 8.4424 - acc: 0.3762 - val_loss: 8.5556 - val_acc: 0.4013\n",
      "Epoch 265/300\n",
      " - 2s - loss: 8.5074 - acc: 0.3555 - val_loss: 8.7599 - val_acc: 0.2442\n",
      "Epoch 266/300\n",
      " - 2s - loss: 8.6763 - acc: 0.3621 - val_loss: 8.5844 - val_acc: 0.3703\n",
      "Epoch 267/300\n",
      " - 2s - loss: 8.5000 - acc: 0.3674 - val_loss: 8.4906 - val_acc: 0.4407\n",
      "Epoch 268/300\n",
      " - 2s - loss: 8.5183 - acc: 0.3596 - val_loss: 8.5003 - val_acc: 0.4271\n",
      "Epoch 269/300\n",
      " - 2s - loss: 8.5818 - acc: 0.3461 - val_loss: 8.9743 - val_acc: 0.2437\n",
      "Epoch 270/300\n",
      " - 2s - loss: 8.5624 - acc: 0.3391 - val_loss: 8.8561 - val_acc: 0.3466\n",
      "Epoch 271/300\n",
      " - 2s - loss: 8.4854 - acc: 0.3563 - val_loss: 8.5017 - val_acc: 0.4242\n",
      "Epoch 272/300\n",
      " - 2s - loss: 8.4849 - acc: 0.3592 - val_loss: 8.5908 - val_acc: 0.3977\n",
      "Epoch 273/300\n",
      " - 2s - loss: 8.4780 - acc: 0.3664 - val_loss: 8.5215 - val_acc: 0.4335\n",
      "Epoch 274/300\n",
      " - 2s - loss: 8.4684 - acc: 0.3615 - val_loss: 8.5158 - val_acc: 0.4386\n",
      "Epoch 275/300\n",
      " - 2s - loss: 8.4786 - acc: 0.3643 - val_loss: 8.5119 - val_acc: 0.4364\n",
      "Epoch 276/300\n",
      " - 2s - loss: 8.5305 - acc: 0.3621 - val_loss: 8.5209 - val_acc: 0.4124\n",
      "Epoch 277/300\n",
      " - 2s - loss: 8.4481 - acc: 0.3711 - val_loss: 8.6814 - val_acc: 0.3457\n",
      "Epoch 278/300\n",
      " - 2s - loss: 8.4613 - acc: 0.3725 - val_loss: 8.6595 - val_acc: 0.3596\n",
      "Epoch 279/300\n",
      " - 2s - loss: 8.4937 - acc: 0.3717 - val_loss: 8.4909 - val_acc: 0.4380\n",
      "Epoch 280/300\n",
      " - 2s - loss: 8.4153 - acc: 0.3826 - val_loss: 8.5212 - val_acc: 0.4401\n",
      "Epoch 281/300\n",
      " - 2s - loss: 8.4007 - acc: 0.3875 - val_loss: 8.4981 - val_acc: 0.4391\n",
      "Epoch 282/300\n",
      " - 2s - loss: 8.4138 - acc: 0.3867 - val_loss: 8.5018 - val_acc: 0.4391\n",
      "Epoch 283/300\n",
      " - 3s - loss: 8.4010 - acc: 0.3893 - val_loss: 8.5428 - val_acc: 0.3450\n",
      "Epoch 284/300\n",
      " - 3s - loss: 8.3982 - acc: 0.3885 - val_loss: 8.5352 - val_acc: 0.3443\n",
      "Epoch 285/300\n",
      " - 3s - loss: 8.4432 - acc: 0.3714 - val_loss: 8.5816 - val_acc: 0.4386\n",
      "Epoch 286/300\n",
      " - 3s - loss: 8.4998 - acc: 0.3671 - val_loss: 8.5009 - val_acc: 0.4261\n",
      "Epoch 287/300\n",
      " - 2s - loss: 8.4722 - acc: 0.3694 - val_loss: 8.5291 - val_acc: 0.4404\n",
      "Epoch 288/300\n",
      " - 2s - loss: 8.4181 - acc: 0.3779 - val_loss: 8.5221 - val_acc: 0.4139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/300\n",
      " - 2s - loss: 8.5261 - acc: 0.3642 - val_loss: 8.5309 - val_acc: 0.4376\n",
      "Epoch 290/300\n",
      " - 2s - loss: 8.4856 - acc: 0.3768 - val_loss: 8.5032 - val_acc: 0.4357\n",
      "Epoch 291/300\n",
      " - 2s - loss: 8.4606 - acc: 0.3774 - val_loss: 8.5208 - val_acc: 0.4402\n",
      "Epoch 292/300\n",
      " - 2s - loss: 8.4626 - acc: 0.3810 - val_loss: 8.5656 - val_acc: 0.3466\n",
      "Epoch 293/300\n",
      " - 2s - loss: 8.4813 - acc: 0.3834 - val_loss: 8.5561 - val_acc: 0.4359\n",
      "Epoch 294/300\n",
      " - 2s - loss: 8.4608 - acc: 0.3801 - val_loss: 8.5789 - val_acc: 0.3420\n",
      "Epoch 295/300\n",
      " - 2s - loss: 8.5007 - acc: 0.3718 - val_loss: 8.5321 - val_acc: 0.4301\n",
      "Epoch 296/300\n",
      " - 2s - loss: 8.4594 - acc: 0.3785 - val_loss: 8.7542 - val_acc: 0.3467\n",
      "Epoch 297/300\n",
      " - 2s - loss: 8.4649 - acc: 0.3794 - val_loss: 8.6316 - val_acc: 0.3437\n",
      "Epoch 298/300\n",
      " - 2s - loss: 8.6172 - acc: 0.3637 - val_loss: 8.5287 - val_acc: 0.4359\n",
      "Epoch 299/300\n",
      " - 2s - loss: 8.4558 - acc: 0.3744 - val_loss: 8.6285 - val_acc: 0.3470\n",
      "Epoch 300/300\n",
      " - 2s - loss: 8.4832 - acc: 0.3732 - val_loss: 8.4887 - val_acc: 0.4388\n"
     ]
    }
   ],
   "source": [
    "net9 = model4.fit(X, y, epochs=300, batch_size=512, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.865926019002892\n"
     ]
    }
   ],
   "source": [
    "#checking the best validation loss after decreasing the learning rate\n",
    "valid_loss9 = min(net9.history[\"val_loss\"])\n",
    "print(valid_loss9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increased the number of neurons and with the tanh function, we see that the validation loss for our model has severely suffered and it is not working on our data set, note that our data set contains only 10% of the entire Otto data set, this is another reason for the high validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 178.35 556.00\" width=\"178pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-552 174.349,-552 174.349,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 5205386016 -->\n",
       "<g class=\"node\" id=\"node1\"><title>5205386016</title>\n",
       "<polygon fill=\"none\" points=\"0,-511.5 0,-547.5 170.349,-547.5 170.349,-511.5 0,-511.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-525.3\">Dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 5205385680 -->\n",
       "<g class=\"node\" id=\"node2\"><title>5205385680</title>\n",
       "<polygon fill=\"none\" points=\"31.4932,-438.5 31.4932,-474.5 138.855,-474.5 138.855,-438.5 31.4932,-438.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-452.3\">Dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 5205386016&#45;&gt;5205385680 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>5205386016-&gt;5205385680</title>\n",
       "<path d=\"M85.1743,-511.313C85.1743,-503.289 85.1743,-493.547 85.1743,-484.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-484.529 85.1743,-474.529 81.6744,-484.529 88.6744,-484.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5408505080 -->\n",
       "<g class=\"node\" id=\"node3\"><title>5408505080</title>\n",
       "<polygon fill=\"none\" points=\"19.8174,-365.5 19.8174,-401.5 150.531,-401.5 150.531,-365.5 19.8174,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-379.3\">Dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 5205385680&#45;&gt;5408505080 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>5205385680-&gt;5408505080</title>\n",
       "<path d=\"M85.1743,-438.313C85.1743,-430.289 85.1743,-420.547 85.1743,-411.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-411.529 85.1743,-401.529 81.6744,-411.529 88.6744,-411.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5408685584 -->\n",
       "<g class=\"node\" id=\"node4\"><title>5408685584</title>\n",
       "<polygon fill=\"none\" points=\"31.4932,-292.5 31.4932,-328.5 138.855,-328.5 138.855,-292.5 31.4932,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-306.3\">Dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 5408505080&#45;&gt;5408685584 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>5408505080-&gt;5408685584</title>\n",
       "<path d=\"M85.1743,-365.313C85.1743,-357.289 85.1743,-347.547 85.1743,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-338.529 85.1743,-328.529 81.6744,-338.529 88.6744,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5408682616 -->\n",
       "<g class=\"node\" id=\"node5\"><title>5408682616</title>\n",
       "<polygon fill=\"none\" points=\"19.8174,-219.5 19.8174,-255.5 150.531,-255.5 150.531,-219.5 19.8174,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-233.3\">Dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 5408685584&#45;&gt;5408682616 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>5408685584-&gt;5408682616</title>\n",
       "<path d=\"M85.1743,-292.313C85.1743,-284.289 85.1743,-274.547 85.1743,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-265.529 85.1743,-255.529 81.6744,-265.529 88.6744,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5408685136 -->\n",
       "<g class=\"node\" id=\"node6\"><title>5408685136</title>\n",
       "<polygon fill=\"none\" points=\"31.4932,-146.5 31.4932,-182.5 138.855,-182.5 138.855,-146.5 31.4932,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-160.3\">Dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 5408682616&#45;&gt;5408685136 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>5408682616-&gt;5408685136</title>\n",
       "<path d=\"M85.1743,-219.313C85.1743,-211.289 85.1743,-201.547 85.1743,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-192.529 85.1743,-182.529 81.6744,-192.529 88.6744,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5205540816 -->\n",
       "<g class=\"node\" id=\"node7\"><title>5205540816</title>\n",
       "<polygon fill=\"none\" points=\"19.8174,-73.5 19.8174,-109.5 150.531,-109.5 150.531,-73.5 19.8174,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-87.3\">Dropout_3: Dropout</text>\n",
       "</g>\n",
       "<!-- 5408685136&#45;&gt;5205540816 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>5408685136-&gt;5205540816</title>\n",
       "<path d=\"M85.1743,-146.313C85.1743,-138.289 85.1743,-128.547 85.1743,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-119.529 85.1743,-109.529 81.6744,-119.529 88.6744,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5408930952 -->\n",
       "<g class=\"node\" id=\"node8\"><title>5408930952</title>\n",
       "<polygon fill=\"none\" points=\"36.5415,-0.5 36.5415,-36.5 133.807,-36.5 133.807,-0.5 36.5415,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85.1743\" y=\"-14.3\">Output: Dense</text>\n",
       "</g>\n",
       "<!-- 5205540816&#45;&gt;5408930952 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>5205540816-&gt;5408930952</title>\n",
       "<path d=\"M85.1743,-73.3129C85.1743,-65.2895 85.1743,-55.5475 85.1743,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"88.6744,-46.5288 85.1743,-36.5288 81.6744,-46.5289 88.6744,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing with the winning solution\n",
    "#incrreasing the number of layers in the nn\n",
    "def getModel4(dropout=0.4, neurons1=2500, neurons2=1300,neurons3=40,\n",
    "             learningRate=0.08):\n",
    "    np.random.seed(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, activation='relu', input_dim=num_features,\n",
    "                    name='Dense_1'))\n",
    "    model.add(Dropout(dropout,name='Dropout_1'))\n",
    "    model.add(Dense(neurons2, activation='relu',name='Dense_2'))\n",
    "    model.add(Dropout(dropout,name='Dropout_2'))\n",
    "    model.add(Dense(neurons3, activation='relu', name='Dense_3'))\n",
    "    model.add(Dropout(dropout, name='Dropout_3'))\n",
    "    model.add(Dense(num_classes, activation='softmax',name='Output'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=learningRate), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model4 = getModel4()\n",
    "\n",
    "SVG(model_to_dot(model4).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32995 samples, validate on 8249 samples\n",
      "Epoch 1/600\n",
      " - 14s - loss: 11.6625 - acc: 0.2548 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 2/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 3/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 4/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 5/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 6/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 7/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 8/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 9/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 10/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 11/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 12/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 13/600\n",
      " - 11s - loss: 11.9663 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 14/600\n",
      " - 11s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 15/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 16/600\n",
      " - 11s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 17/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 18/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 19/600\n",
      " - 10s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 20/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 21/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 22/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 23/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 24/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 25/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 26/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 27/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 28/600\n",
      " - 11s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 29/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 30/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 31/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 32/600\n",
      " - 11s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 33/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 34/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 35/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 36/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 37/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 38/600\n",
      " - 11s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 39/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 40/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 41/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 42/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 43/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 44/600\n",
      " - 10s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 45/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 46/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 47/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 48/600\n",
      " - 10s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 49/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 50/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 51/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 52/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 53/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 54/600\n",
      " - 2164s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 55/600\n",
      " - 20s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 56/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 57/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 58/600\n",
      " - 10s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 59/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 60/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 61/600\n",
      " - 684s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 62/600\n",
      " - 19s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 63/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 64/600\n",
      " - 11s - loss: 11.9663 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 65/600\n",
      " - 5784s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 66/600\n",
      " - 18s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 67/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 68/600\n",
      " - 10s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 69/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 70/600\n",
      " - 900s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 71/600\n",
      " - 20s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 72/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 73/600\n",
      " - 10s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 74/600\n",
      " - 4733s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 75/600\n",
      " - 21s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 76/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 77/600\n",
      " - 10s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 78/600\n",
      " - 1258s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 79/600\n",
      " - 53s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 80/600\n",
      " - 14s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 81/600\n",
      " - 14s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 82/600\n",
      " - 14s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 83/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 84/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 85/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 86/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 87/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 88/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 89/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 90/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 91/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 92/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 93/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 95/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 96/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 97/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 98/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 99/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 100/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 101/600\n",
      " - 13s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 102/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 103/600\n",
      " - 11s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 104/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 105/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 106/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 107/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 108/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 109/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 110/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 111/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 112/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 113/600\n",
      " - 11s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 114/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 115/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 116/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 117/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 118/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 119/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 120/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 121/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 122/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 123/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 124/600\n",
      " - 14s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 125/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 126/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 127/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 128/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 129/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 130/600\n",
      " - 11s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 131/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 132/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 133/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 134/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 135/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 136/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 137/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 138/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 139/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 140/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 141/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 142/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 143/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 144/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 145/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 146/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 147/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 148/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 149/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 150/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 151/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 152/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 153/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 154/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 155/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 156/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 157/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 158/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 159/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 160/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 161/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 162/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 163/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 164/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 165/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 166/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 167/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 168/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 169/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 170/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 171/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 172/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 173/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 174/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 175/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 176/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 177/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 178/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 179/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 180/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 181/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 182/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 183/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 184/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 185/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 186/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 187/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 188/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 189/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 190/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 191/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 192/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 193/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 194/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 195/600\n",
      " - 14s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 196/600\n",
      " - 15s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 197/600\n",
      " - 17s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 198/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 199/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 200/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 201/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 202/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 203/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 204/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 205/600\n",
      " - 14s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 206/600\n",
      " - 15s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 207/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 208/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 209/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 210/600\n",
      " - 14s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 211/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 212/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 213/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 214/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 215/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 216/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 217/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 218/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 219/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 220/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 221/600\n",
      " - 12s - loss: 11.9663 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 222/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 223/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 224/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 225/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 226/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 227/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 228/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 229/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 230/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 231/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 232/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 233/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 234/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 235/600\n",
      " - 329s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 236/600\n",
      " - 24s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 237/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 238/600\n",
      " - 60s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 239/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 240/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 241/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 242/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 243/600\n",
      " - 18s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 244/600\n",
      " - 16s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 245/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 246/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 247/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 248/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 249/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 250/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 251/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 252/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 253/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 254/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 255/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 256/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 257/600\n",
      " - 11s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 258/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 259/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 260/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 261/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 262/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 263/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 264/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 265/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 266/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 267/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 268/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 269/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 270/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 271/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 272/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 273/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 274/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 275/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 276/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 277/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 278/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 280/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 281/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 282/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 283/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 284/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 285/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 286/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 287/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 288/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 289/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 290/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 291/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 292/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 293/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 294/600\n",
      " - 16s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 295/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 296/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 297/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 298/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 299/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 300/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 301/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 302/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 303/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 304/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 305/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 306/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 307/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 308/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 309/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 310/600\n",
      " - 11s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 311/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 312/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 313/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 314/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 315/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 316/600\n",
      " - 11s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 317/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 318/600\n",
      " - 12s - loss: 11.9692 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 319/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 320/600\n",
      " - 11s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 321/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 322/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 323/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 324/600\n",
      " - 14s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 325/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 326/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 327/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 328/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 329/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 330/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 331/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 332/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 333/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 334/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 335/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 336/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 337/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 338/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 339/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 340/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 341/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 342/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 343/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 344/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 345/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 346/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 347/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 348/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 349/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 350/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 351/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 352/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 353/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 354/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 355/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 356/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 357/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 358/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 359/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 360/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 361/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 362/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 363/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 364/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 365/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 366/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 367/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 368/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 369/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 370/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 371/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 372/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 373/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 374/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 375/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 376/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 377/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 378/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 379/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 380/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 381/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 382/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 383/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 384/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 385/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 386/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 387/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 388/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 389/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 390/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 391/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 392/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 393/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 394/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 395/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 396/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 397/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 398/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 399/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 400/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 401/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 402/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 403/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 404/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 405/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 406/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 407/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 408/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 409/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 410/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 411/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 412/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 413/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 414/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 415/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 416/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 417/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 418/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 419/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 420/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 421/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 422/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 423/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 424/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 425/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 426/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 427/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 428/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 429/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 430/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 431/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 432/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 433/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 434/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 435/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 436/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 437/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 438/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 439/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 440/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 441/600\n",
      " - 13s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 442/600\n",
      " - 15s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 443/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 444/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 445/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 446/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 447/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 448/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 449/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 450/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 451/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 452/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 453/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 454/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 455/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 456/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 457/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 458/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 459/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 460/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 461/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 462/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 463/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 464/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 465/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 466/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 467/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 468/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 469/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 470/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 471/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 472/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 473/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 474/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 475/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 476/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 477/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 478/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 479/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 480/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 481/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 482/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 483/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 484/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 485/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 486/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 487/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 488/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 489/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 490/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 491/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 492/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 493/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 494/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 495/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 496/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 497/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 498/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 499/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 500/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 501/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 502/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 503/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 504/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 505/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 506/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 507/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 508/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 509/600\n",
      " - 13s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 510/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 511/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 512/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 513/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 514/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 515/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 516/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 517/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 518/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 519/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 520/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 521/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 522/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 523/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 524/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 525/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 526/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 527/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 528/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 529/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 530/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 531/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 532/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 533/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 534/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 535/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 536/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 537/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 538/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 539/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 540/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 541/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 542/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 543/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 544/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 545/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 546/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 547/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 548/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 549/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 550/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 551/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 552/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 553/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 554/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 555/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 556/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 14s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 557/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 558/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 559/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 560/600\n",
      " - 12s - loss: 11.9688 - acc: 0.2574 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 561/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 562/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 563/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 564/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 565/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 566/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 567/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 568/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 569/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 570/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 571/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 572/600\n",
      " - 12s - loss: 11.9668 - acc: 0.2576 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 573/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 574/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 575/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 576/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 577/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 578/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 579/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 580/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 581/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 582/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 583/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 584/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 585/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 586/600\n",
      " - 13s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 587/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 588/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 589/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 590/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 591/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 592/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 593/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 594/600\n",
      " - 12s - loss: 11.9673 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 595/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 596/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 597/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 598/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 599/600\n",
      " - 12s - loss: 11.9683 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n",
      "Epoch 600/600\n",
      " - 12s - loss: 11.9678 - acc: 0.2575 - val_loss: 11.8116 - val_acc: 0.2672\n"
     ]
    }
   ],
   "source": [
    "net9 = model4.fit(X, y, epochs=600, batch_size=1024, verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.811590574830788\n"
     ]
    }
   ],
   "source": [
    "#checking the best validation loss after decreasing the learning rate\n",
    "valid_loss9 = min(net9.history[\"val_loss\"])\n",
    "print(valid_loss9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
