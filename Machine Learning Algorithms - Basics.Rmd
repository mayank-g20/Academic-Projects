---
title: "ML Course Project Part 1"
author: "Mayank"
date: "6/1/2018"
output: pdf_document
---

# About the data set:
* The data contain 384 features extracted from 53,500 CT images from 74 patients
* Each record characterizes a slice of an image
* Each CT slice is described by two histograms in polar space. 
* The first histogram describes the location of bone structures in the image, 
* the second the location of air inclusions inside of the body.
* The class output variable is numeric and denotes the relative location of the CT slice on 
* the axial axis of the human body.

## Data Description: 
1. PatientId: Each ID identifies a different patient - each patient has multiple entries
2. From column number 2 - 241: Histogram describing bone structures
3. From column number 242 - 385: Histogram describing air inclusions
4. Column number 386: Reference: Relative location of the image on the axial axis (class value). Values are in the range [0; 180] where 0 denotes the top of the head and 180 the soles of the feet.

## Problem objective: It is important to predict location of the slice using the features because when 2 or more scans are compared or in general it is necessary to navigate to a certain part of the body, the whole scan (about 1 Gb) needs to be loaded over clinical network and then the required slice is usually identified manually.


## Compiling all the libraries
```{r results='hide', message=FALSE, warning=FALSE}
library(data.table)
library(pls)
library(factoextra)
#install.packages("relaimpo")
library(relaimpo)
suppressWarnings(library(relaimpo))
library(glmnet)
library(rpart)
library(rpart.plot)
```



## Reading the data

```{r }
datapath<-"/Users/mayank/Documents/College Documents/Q3 Courses/Machine Learning/Course Project/"

dat<-read.csv(file=paste(datapath,"slice_localization_data.csv",sep="/"))
head(colnames(dat))
tail(colnames(dat))

```

## Conducting Exploratory Data Analysis

```{r}
pred<-dat[,c(-1,-386)]
Y<-dat[,386]
head(colnames(pred))

```

## Checking the distribution of the dependent variable

```{r}
plot(Y)

```

## Checking the summary of Y

```{r}
summary(Y)

```

## Checking the histogram of the independent variable

```{r}
hist(Y)

```


### The histogram in itself is representing that the distribution of the dependent variable is not gaussian, however, we check the q-q plot to ascertain it

## Checking the q-q plot if the dependent variable follows gaussian distribution

```{r}
qqnorm(Y)
qqline(Y)

```

### The q-q plot ascertains our initial hypothesis of the dependent variable not following a normal distribution

## As a first step for prediction we will try to fit a Linear Model. As a part of the assignment, we fit linear model with all the predictors.

```{r}
#setting up the option to run the code on my system

lm_model<-lm(Y~., data=data.frame(Y,pred))

#creating a dataframe which has the values of all the beta coeffs and their respective p-vals

lm_beta<-data.frame(summary(lm_model)$coefficients[,1], summary(lm_model)$coefficients[,4])
lm_beta[,3]<-rownames(lm_beta)

colnames(lm_beta)<-c("coeff", "p.val", "predictor_var")

#removing the intercept value

lm_beta<-lm_beta[-1,]
lm_beta_2<-as.data.frame(lm_beta)
```

### Eliminating the betas for which the p-value is greater than 5

```{r}

significant_betas<-subset(lm_beta_2, lm_beta_2$p.val<0.05)

#subsetting the predictor list to fit the reduced model

pred_lm_reduced<-pred[,c(significant_betas[,3])]

#fitting linear regression to the reduced predictor list

reduced_lm_model<-glm(Y~., data.frame(Y, pred_lm_reduced), family = "gaussian")

#Creating vector of characteristics of the fit: AIC, R2, MSE, number of predictors in the model

R2<-1-(reduced_lm_model$deviance/reduced_lm_model$null.deviance)
MSE<-mean(reduced_lm_model$residuals^2)
(lm_fit_characteristics<-c(reduced_lm_model$aic, R2, MSE, ncol(pred_lm_reduced)))

```


## As the next steps we will be applying PCA-regression to the data as it was explained in our lecture

```{r}
#running pcr

#install.packages("pls")
pcr.fit<-pcr(Y~., data=data.frame(Y,pred))
#summary(pcr.fit)

#we see from the fit summary that still only 86% of the variance can be explained 
#by the data set
#runnign PCA on the data set

pca<-prcomp(pred)

#installing and using the below mentioned library for visualizations 
#of factors and their explained variances

#install.packages("factoextra")
fviz_eig(pca)
```

### Relative Importance
Relative importance code wont work for all the factors as the amount of computation power required for it is more than that of my local machine, so we will subset the number of factors and then use the number of predictors from that list. For this we will check the initial analysis and on the basis of that decide what numnber of factors we should be proceeding forward with. Those factors need to be removed to prevent singular system error in linear model fitting process. 

```{r}
#lets check the importance of last 50 factors

summary(pca)$importance[,334:384]

#segregating the first 334 factors for our analysis

num_factors<-334

#in case of prcomp the rotation is the factors loadings, i.e. they are the eigen vectors
#in case of prcomp the variable "x" contains the factor scores values, i.e. the 
#coordinates of the individuals (observations) on the principal components.

factor_loading<-as.matrix(pca$rotation[,1:num_factors])
factor_scores<-as.matrix(pred)%*%factor_loading

#creating a dataframe with the above segregated number of factors

pca_filtered_factors<-data.frame(Y,factor_scores)
```

### We see that how gradually the value of explained variance takes a toll. It is almost an exponential trend using all the factors as created below does not yield any results due inverse matrix calculation error

```{r}
new_vars<-as.data.frame(pca$scores)

#running linear regression on pc comps
```


```{r }
lm.pca<-lm(Y~., data=pca_filtered_factors)


#on the basis of the results depicted above I will go with 334 factors only

rel.imp2<-calc.relimp(lm.pca, type = "first", rela=TRUE)
plot(rel.imp2)
```

### Relative importance measure "first" makes perfect decomposition of $R^2$. It is also mentioned in the graph above. Now, I will reorder the PCA factors according to their relative importance as predictors explaining the output and calculate the $R^2$ sequence.    

```{r}
ordered.PC<-rel.imp2@first.rank
ordered.PC
```

### We will try to use this ordering and then as per it we will re-run out regression analysis as per the ordering segregating the loadings and factor scores as we did that earlier, but this time as per the ordering of the components as per their importance.
```{r}
ranked_factors<-factor_scores[,order(ordered.PC)]
ranked_loadings<-factor_loading[,order(ordered.PC)]
head(colnames(ranked_factors))

tail(colnames(ranked_factors))

#creating a chart to check how the explained variance behaves when we use the 
#ordered principal components

ranked_pca<-sapply(2:334,function(z) 
  summary(lm(Y~.,data=data.frame(Y=dat[,386],ranked_factors[,1:z])))$r.squared)
plot(2:334,ranked_pca,type="l")
```

### We see that the curve kind of diminishes or does not add any sufficient information after the first 150 principal components, we will now try to understand and echouate the optimal cur-off point for our analysis.

```{r}
data_filter_2<-rbind(R2=c(0,ranked_pca),ranked_loadings)
head(data_filter_2[,1:10])

#Now we will look for the maximum level of accuracy we can reach for with the given set of ordered 
#principal components

max(data_filter_2[1,])
```

### We see that the maximum accuracy level which we can achieve is 86%, thus now we will try to find the number of principal components requried to achieve that accuracy
```{r}
accuracy_level<-0.86
level_86_perc<-(data_filter_2[1,]>=accuracy_level)*(1:length(data_filter_2[1,]))
(num_factors_2<-min(level_86_perc[level_86_perc>0]))

#thus we will now use only 196 ranked principal components to run our 
#analysis and linear regression model again

final_pca_data<-cbind(Y,ranked_factors)
regression_data_pca<-as.data.frame(final_pca_data[,1:(num_factors_2+1)])
pca_regression<-lm(Y~.,data=regression_data_pca)

#Create vector of fit characteristics.

(pca_fit<-c(AIC=AIC(pca_regression),
            R2=summary(pca_regression)$r.squared,
            MSE=mean(pca_regression$residuals^2),
            nSlopes=pca_regression$rank-1))


#Checking the predictions and the actual values 

matplot(1:length(final_pca_data[,1]),cbind(final_pca_data[,1],pca_regression$fitted.values),type="l",
        lwd=2,lty=1,ylab="Output",xlab="Index")

#checking the residuals

plot(pca_regression$residuals)

#checking the histogram of the residuals

hist(pca_regression$residuals)


#checking the Q-Q plots for residuals

qqnorm(pca_regression$residuals)
qqline(pca_regression$residuals)



```



## Next we will be applying the lasso regression method on the data set

```{r}

set.seed(1)

#running cross validation in-order to obtain the best lambda value

cv_out=cv.glmnet(x=as.matrix(dat[,c(-1,-386)]),y=as.matrix(dat[,386]),alpha=1)
plot(cv_out)

#the best lambda

(best_lam =cv_out$lambda.min)

#running the lasso regression model with the best value of lambda

lasso_model<-glmnet(x=as.matrix(dat[,c(-1,-386)]),y=as.matrix(dat[,386]), alpha = 1, 
                    lambda = best_lam, standardize = F)

#Extracting all the betas in a data frame

lasso_beta<-as.data.frame(lasso_model$beta[,1])
colnames(lasso_beta)<-c("coeff_val")
lasso_beta$variable<-rownames(lasso_beta)


eliminated.preds<-subset(lasso_beta, lasso_beta$coeff_val==0)

# we see from the above analysis that 62 obs have been eliminated from our analysis

```

```{r}
#checking the plot of all the used variables in our table
#currently this plotis not working, check why

plot(lasso_model, xvar="norm")
abline(h=0)
```

Each curve corresponds to a variable. It shows the path of its coefficient against the ℓ1-norm of the whole coefficient vector at as λ varies. The axis above indicates the number of nonzero coefficients at the current λ, which is the effective degrees of freedom (df) for the lasso. Users may also wish to annotate the curves; this can be done by setting label = TRUE in the plot command

The plot here is not working right now, it is something specific to my system, but it should run without a glitch on other systems


```{r}
# checking the plot of the predicted vs original outputs and how well or bad our model performed

lasso_predicted<-predict(lasso_model,type="response",newx=as.matrix(pred),s=best_lam)
matplot(1:length(dat[,386]),cbind(dat[,386],lasso_predicted),type="l",lty=1)
```

```{r}
#checking the behavior of the residuals now

(lasso_residuals<-lasso_predicted-dat[,386])
plot(lasso_residuals)
```

```{r}
#finally running a linear regression model using the variables which 
#havent been eliminated through lasso

lasso_pred_set<-pred[,!names(pred) %in% (eliminated.preds$variable)]
lasso_lm<-lm(Y~.,data=data.frame(cbind(Y=Y,as.matrix(lasso_pred_set))))

#Create vector of fit characteristics   

(lasso_fit<-c(AIC=AIC(lasso_lm),
              R2=summary(lasso_lm)$r.squared,
              MSE=mean(lasso_lm$residuals^2),
              nSlopes=lasso_lm$rank-1))

```

## Next we will use the regression tree method on the above data set to estimate its predictive quality using 10 fold cross validation

```{r results="hide", message="hide"}

#estimate its predictive quality using 10 fold cross validation
#rpart uses CV of 10 by default thus we dont need to set up that option anywhere
#create a vector of characteristics r.square, MSE, number of predictors in the model

```

### Method is used to pass the splitting rule for each node. In the code below we are using method="anova" because we want to do linear regression. The other options are 
+ method= "Class", it is used for categorical data
+ method="Poisson", it is used for counts or processes
+ method= "Exp", it is used for survival modeling

```{r}
set.seed(1)

FullTree <- rpart(Y ~ ., data=data.frame(Y,pred), method="anova", control = rpart.control(cp = 0))

#visualizing the tree

plotcp(FullTree)
```

### Check out the cp table
A rule of thumb is that tree needs to be pruned at a minimum level where rel error + xstd < xerror

```{r eval=FALSE}
#I am not showing it here as the table is pageS long
FullTree$cptable
```

```{r }
#store the optimal value of the complexity parameter, so that we can use it 
#to prune the tree later on
#also, not showing it here as it is pageS long

cpbest <- FullTree$cptable[6,1]

```

```{r}
#fit the model with this best cp parameter for pruning

set.seed(1)
PrunedTree <- rpart(Y ~ ., data=data.frame(Y,pred), method="anova", 
                    control = rpart.control(cp = cpbest))

#visualizing the tree

#install.packages("rpart.plot")
prp(PrunedTree,extra=101, # display the number of observations that fall in the node
    branch=.5, # change angle of branch lines
    shadow.col="gray", # shadows under the leaves
    branch.lty=3, # draw branches using dotted lines
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5,
    nn=TRUE) # display the node numbers, default is FALSE
```

### Analyze the tree. Which factors are the most important for size of compensation? Are there any questionable results of the tree model? Tree with too small terminals is unreliable and misleading. This tree is too deep. Analyze the depth of the tree. How many nodes are necessary to keep the tree stable and still accurate? Decision about appropriate depth of the tree is made based on parameter CP and the error columns.

```{r}

#calculating the parameter values of this model
#calculating the rmse of this model

rmse <- function(x) sqrt(mean(x^2))
tree_Rmse <- rmse(resid(PrunedTree))
tree_Rmse
#calculating the MSE of this model

RSS<-sum(resid(PrunedTree)^2)
MSE<-RSS/nrow(pred)
R2<-1-(RSS/(sum(Y-mean(Y)^2)))

#Look at the residuals from this model, just as with a regular linear regression fit

#plot(predict(PrunedTree), jitter(resid(PrunedTree)))
#temp <- PrunedTree$frame[PrunedTree$frame$var == '<leaf>',]
#axis(3, at = temp$yval, as.character(row.names(temp)))
#mtext('leaf number', side = 3, line = 3)
#abline(h = 0, lty = 2)

#check out if there are any outliers in any of these trees and how many trees have outliers

#final parameters list
(tree_fit<-c(AIC=RSS, R2=R2, MSE=RSS/nrow(pred), nSlopes=NaN))

```



#FINALLY COMPARE THE FIT OF ALL THE METHODS USED ABOVE

```{r}

#create a vector of characteristics r.square, MSE, number of predictors in the model
rbind(LM=lm_fit_characteristics,PCA=pca_fit,LASSO=lasso_fit,Tree=tree_fit)
```
